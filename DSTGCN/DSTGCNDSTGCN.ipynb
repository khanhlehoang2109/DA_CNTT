{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13060620,"sourceType":"datasetVersion","datasetId":8270673},{"sourceId":262120305,"sourceType":"kernelVersion"}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":20518.612627,"end_time":"2025-09-16T14:11:07.658621","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-09-16T08:29:09.045994","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DSTGCN Model for Accident Prediction\n\nĐây là notebook gộp toàn bộ mã nguồn từ các file Python để huấn luyện mô hình DSTGCN. Notebook được chia thành các phần chính:\n1. **Cài đặt & Imports**: Cài đặt các thư viện cần thiết và import chúng.\n2. **Cấu hình (Configuration)**: Tạo và tải file cấu hình.\n3. **Hàm tiện ích (Utilities)**: Bao gồm các hàm tính loss, đánh giá, chuyển đổi tọa độ, và các hàm hỗ trợ khác.\n4. **Kiến trúc Model (Model Architecture)**: Định nghĩa các lớp mạng neural, bao gồm các layer và mô hình DSTGCN chính.\n5. **Tải dữ liệu (Data Loading)**: Định nghĩa Dataset và DataLoader để nạp và xử lý dữ liệu.\n6. **Hàm Huấn luyện (Training Function)**: Chứa logic cho vòng lặp huấn luyện, validation và test.\n7. **Thực thi chính (Main Execution)**: Chạy toàn bộ quy trình huấn luyện và lưu kết quả.","metadata":{"papermill":{"duration":0.004878,"end_time":"2025-09-16T08:29:13.029146","exception":false,"start_time":"2025-09-16T08:29:13.024268","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### 1. Cài đặt & Imports","metadata":{"papermill":{"duration":0.003345,"end_time":"2025-09-16T08:29:13.036336","exception":false,"start_time":"2025-09-16T08:29:13.032991","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!pip install torchdata==0.7.1 --quiet\n!pip install dgl -f https://data.dgl.ai/wheels/torch-2.1/cu121/repo.html --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T09:51:19.519486Z","iopub.execute_input":"2025-12-20T09:51:19.519889Z","iopub.status.idle":"2025-12-20T09:55:21.259136Z","shell.execute_reply.started":"2025-12-20T09:51:19.519864Z","shell.execute_reply":"2025-12-20T09:55:21.258110Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install tensorboardX pandas numpy networkx tqdm scikit-learn scipy --quiet","metadata":{"execution":{"iopub.status.busy":"2025-12-20T09:55:21.260839Z","iopub.execute_input":"2025-12-20T09:55:21.261091Z","iopub.status.idle":"2025-12-20T09:55:25.131708Z","shell.execute_reply.started":"2025-12-20T09:55:21.261065Z","shell.execute_reply":"2025-12-20T09:55:25.130863Z"},"papermill":{"duration":3.931397,"end_time":"2025-09-16T08:32:28.945008","exception":false,"start_time":"2025-09-16T08:32:25.013611","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"CUDA version: {torch.version.cuda}\")","metadata":{"execution":{"iopub.status.busy":"2025-12-20T09:55:25.132844Z","iopub.execute_input":"2025-12-20T09:55:25.133136Z","iopub.status.idle":"2025-12-20T09:55:27.307293Z","shell.execute_reply.started":"2025-12-20T09:55:25.133102Z","shell.execute_reply":"2025-12-20T09:55:27.306394Z"},"papermill":{"duration":1.735716,"end_time":"2025-09-16T08:32:30.728256","exception":false,"start_time":"2025-09-16T08:32:28.992540","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport dgl\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom tensorboardX import SummaryWriter\nimport pandas as pd\nimport numpy as np\nimport networkx as nx\nfrom tqdm import tqdm\nimport json\nimport os\nimport shutil\nimport copy\nimport time\nimport datetime\nimport math\nimport sys\nimport warnings\nfrom typing import List, Dict, Tuple\nfrom dgl import init as g_init\nfrom dgl.nn.pytorch import GraphConv\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.stats import pearsonr\nfrom sklearn import metrics\n\n# Bỏ qua các cảnh báo không cần thiết\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2025-12-20T09:55:27.309114Z","iopub.execute_input":"2025-12-20T09:55:27.309446Z","iopub.status.idle":"2025-12-20T09:55:49.938092Z","shell.execute_reply.started":"2025-12-20T09:55:27.309426Z","shell.execute_reply":"2025-12-20T09:55:49.937446Z"},"papermill":{"duration":19.146848,"end_time":"2025-09-16T08:32:49.922463","exception":false,"start_time":"2025-09-16T08:32:30.775615","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. Cấu hình (Configuration)","metadata":{"papermill":{"duration":0.047005,"end_time":"2025-09-16T08:32:50.017423","exception":false,"start_time":"2025-09-16T08:32:49.970418","status":"completed"},"tags":[]}},{"cell_type":"code","source":"config_content = '''{\n  \"model_name\": \"DSTGCN\",\n  \"train_repeat_times\": 1,\n  \"epochs\": 100,\n  \"cuda\": 0,\n  \"temporal_features_number\": 1,\n  \"weather_features_number\": 38,\n  \"external_temporal_features_number\": 5,\n  \"poi_features_number\": 22,\n  \"batch_size\": 64,\n  \"learning_rate\": 1e-3,\n  \"loss_function\": \"bce_loss\",\n  \"optim\": \"Adam\",\n  \"weight_decay\": 1e-3,\n  \"K_hop\": 10,\n  \"spatial_features_mean\": [\n    6.40254292e+00,\n    4.81553347e+00,\n    2.64811501e+01,\n    5.54260681e+00,\n    7.84915710e+00,\n    9.01424670e+00,\n    1.05751666e+00,\n    2.83014290e+01,\n    1.77408007e+01,\n    3.24747843e+01,\n    1.05794121e+00,\n    3.47343126e+00,\n    6.26610390e+00,\n    3.39390529e+00,\n    1.28002351e+01,\n    5.71796174e-02,\n    2.36710361e-02,\n    5.17406913e-02,\n    8.18431583e-01,\n    4.57574675e-01,\n    1.61370302e+02,\n    2.00511484e+00\n  ],\n  \"spatial_features_std\": [\n    8.25421040e+00,\n    6.98772605e+00,\n    3.12181863e+01,\n    7.78804699e+00,\n    1.12482891e+01,\n    8.86203753e+00,\n    4.88544842e+00,\n    4.65677369e+01,\n    1.88312515e+01,\n    6.70090744e+01,\n    2.95457633e+00,\n    4.29173854e+00,\n    9.78727429e+00,\n    3.76843985e+00,\n    1.19405955e+01,\n    2.71230076e-01,\n    1.88342431e-01,\n    2.82777835e-01,\n    2.18729099e+00,\n    1.55007715e+00,\n    2.04879612e+02,\n    7.87830458e-02\n  ],\n  \"temporal_features_mean\": [\n    18.54884516\n  ],\n  \"temporal_features_std\": [\n    19.87195773\n  ],\n  \"external_features_mean\": [\n    7.02588369e+01,\n    5.28922463e+01,\n    5.95480331e+01,\n    2.98048133e+01,\n    6.23360889e+00,\n    7.17909065e+01,\n    1.31128848e-02,\n    8.48346636e-01,\n    4.86031927e-02,\n    1.65336374e-02,\n    1.71037628e-03,\n    1.71037628e-02,\n    1.51083238e-02,\n    7.41163056e-03,\n    2.28050171e-03,\n    1.58209806e-02,\n    1.25427594e-02,\n    0.00000000e+00,\n    1.42531357e-04,\n    1.28278221e-03,\n    2.10946408e-02,\n    5.23090080e-02,\n    2.53705815e-02,\n    4.53249715e-02,\n    1.62058153e-01,\n    1.96693273e-02,\n    4.53249715e-02,\n    6.01482326e-02,\n    6.14310148e-02,\n    5.60148233e-02,\n    6.07183580e-02,\n    5.91505131e-02,\n    1.58209806e-02,\n    2.28050171e-03,\n    2.29618016e-01,\n    2.87913341e-02,\n    5.11687571e-02,\n    3.70581528e-03,\n    8.90992018e+00,\n    1.67015393e+01,\n    2.72548461e+00,\n    1.22075257e+01,\n    1.92702395e-01\n  ],\n  \"external_features_std\": [\n    1.45939773e+01,\n    1.86117750e+01,\n    2.39727858e+01,\n    2.15191858e-01,\n    5.02840584e+00,\n    1.71451636e+01,\n    1.05974340e-01,\n    3.50647069e-01,\n    2.05374727e-01,\n    1.19435494e-01,\n    2.91935396e-02,\n    1.22304197e-01,\n    1.10326636e-01,\n    6.60570950e-02,\n    3.95302952e-02,\n    1.18032057e-01,\n    9.83734863e-02,\n    1.00000000e+00,\n    8.44069685e-03,\n    2.52931923e-02,\n    1.17508093e-01,\n    1.82744266e-01,\n    1.15431393e-01,\n    1.69085448e-01,\n    3.23940225e-01,\n    9.79301412e-02,\n    1.64816793e-01,\n    1.96033008e-01,\n    2.05755011e-01,\n    1.93423139e-01,\n    2.05099602e-01,\n    1.97963022e-01,\n    1.01824764e-01,\n    3.76843851e-02,\n    3.46547020e-01,\n    1.50627796e-01,\n    1.96944453e-01,\n    4.76105938e-02,\n    8.32550240e-01,\n    8.70795832e+00,\n    1.86828552e+00,\n    5.96074486e+00,\n    3.94421325e-01\n  ]\n}'''  # (cut for brevity — insert your full JSON here)\n\nwith open(\"config.json\", \"w\") as f:\n    f.write(config_content)","metadata":{"execution":{"iopub.status.busy":"2025-12-20T09:55:49.939027Z","iopub.execute_input":"2025-12-20T09:55:49.939637Z","iopub.status.idle":"2025-12-20T09:55:49.945341Z","shell.execute_reply.started":"2025-12-20T09:55:49.939616Z","shell.execute_reply":"2025-12-20T09:55:49.944647Z"},"papermill":{"duration":0.055258,"end_time":"2025-09-16T08:32:50.120182","exception":false,"start_time":"2025-09-16T08:32:50.064924","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tải cấu hình từ file\nwith open(\"config.json\") as file:\n    config = json.load(file)\n\ndef get_attribute(name, defaultValue=None):\n    try:\n        return config[name]\n    except KeyError:\n        return defaultValue\n\nconfig['device'] = f'cuda:{get_attribute(\"cuda\")}' if torch.cuda.is_available() and get_attribute(\"cuda\") >= 0 else 'cpu'\nprint(f\"Using device: {config['device']}\")","metadata":{"execution":{"iopub.status.busy":"2025-12-20T09:55:49.945996Z","iopub.execute_input":"2025-12-20T09:55:49.946187Z","iopub.status.idle":"2025-12-20T09:55:49.968750Z","shell.execute_reply.started":"2025-12-20T09:55:49.946171Z","shell.execute_reply":"2025-12-20T09:55:49.968059Z"},"papermill":{"duration":0.056258,"end_time":"2025-09-16T08:32:50.223760","exception":false,"start_time":"2025-09-16T08:32:50.167502","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3. Hàm tiện ích (Utilities)","metadata":{"papermill":{"duration":0.046958,"end_time":"2025-09-16T08:32:50.318205","exception":false,"start_time":"2025-09-16T08:32:50.271247","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Module: coordTransform_utils.py & coord_converter.py\n\nx_pi = 3.14159265358979324 * 3000.0 / 180.0\npi = 3.1415926535897932384626  # π\na = 6378245.0  # 長半軸\nee = 0.00669342162296594323  # 偏心率平方\n\ndef gcj02_to_bd09(lng, lat):\n    z = math.sqrt(lng * lng + lat * lat) + 0.00002 * math.sin(lat * x_pi)\n    theta = math.atan2(lat, lng) + 0.000003 * math.cos(lng * x_pi)\n    bd_lng = z * math.cos(theta) + 0.0065\n    bd_lat = z * math.sin(theta) + 0.006\n    return [bd_lng, bd_lat]\n\ndef bd09_to_gcj02(bd_lon, bd_lat):\n    x = bd_lon - 0.0065\n    y = bd_lat - 0.006\n    z = math.sqrt(x * x + y * y) - 0.00002 * math.sin(y * x_pi)\n    theta = math.atan2(y, x) - 0.000003 * math.cos(x * x_pi)\n    gg_lng = z * math.cos(theta)\n    gg_lat = z * math.sin(theta)\n    return [gg_lng, gg_lat]\n\ndef wgs84_to_gcj02(lng, lat):\n    if out_of_china(lng, lat):\n        return [lng, lat]\n    dlat = _transformlat(lng - 105.0, lat - 35.0)\n    dlng = _transformlng(lng - 105.0, lat - 35.0)\n    radlat = lat / 180.0 * pi\n    magic = math.sin(radlat)\n    magic = 1 - ee * magic * magic\n    sqrtmagic = math.sqrt(magic)\n    dlat = (dlat * 180.0) / ((a * (1 - ee)) / (magic * sqrtmagic) * pi)\n    dlng = (dlng * 180.0) / (a / sqrtmagic * math.cos(radlat) * pi)\n    mglat = lat + dlat\n    mglng = lng + dlng\n    return [mglng, mglat]\n\ndef gcj02_to_wgs84(lng, lat):\n    if out_of_china(lng, lat):\n        return [lng, lat]\n    dlat = _transformlat(lng - 105.0, lat - 35.0)\n    dlng = _transformlng(lng - 105.0, lat - 35.0)\n    radlat = lat / 180.0 * pi\n    magic = math.sin(radlat)\n    magic = 1 - ee * magic * magic\n    sqrtmagic = math.sqrt(magic)\n    dlat = (dlat * 180.0) / ((a * (1 - ee)) / (magic * sqrtmagic) * pi)\n    dlng = (dlng * 180.0) / (a / sqrtmagic * math.cos(radlat) * pi)\n    mglat = lat + dlat\n    mglng = lng + dlng\n    return [lng * 2 - mglng, lat * 2 - mglat]\n\ndef bd09_to_wgs84(bd_lon, bd_lat):\n    lon, lat = bd09_to_gcj02(bd_lon, bd_lat)\n    return gcj02_to_wgs84(lon, lat)\n\ndef wgs84_to_bd09(lon, lat):\n    lon, lat = wgs84_to_gcj02(lon, lat)\n    return gcj02_to_bd09(lon, lat)\n\ndef _transformlat(lng, lat):\n    ret = -100.0 + 2.0 * lng + 3.0 * lat + 0.2 * lat * lat + \\\n          0.1 * lng * lat + 0.2 * math.sqrt(math.fabs(lng))\n    ret += (20.0 * math.sin(6.0 * lng * pi) + 20.0 *\n            math.sin(2.0 * lng * pi)) * 2.0 / 3.0\n    ret += (20.0 * math.sin(lat * pi) + 40.0 *\n            math.sin(lat / 3.0 * pi)) * 2.0 / 3.0\n    ret += (160.0 * math.sin(lat / 12.0 * pi) + 320 *\n            math.sin(lat * pi / 30.0)) * 2.0 / 3.0\n    return ret\n\ndef _transformlng(lng, lat):\n    ret = 300.0 + lng + 2.0 * lat + 0.1 * lng * lng + \\\n          0.1 * lng * lat + 0.1 * math.sqrt(math.fabs(lng))\n    ret += (20.0 * math.sin(6.0 * lng * pi) + 20.0 *\n            math.sin(2.0 * lng * pi)) * 2.0 / 3.0\n    ret += (20.0 * math.sin(lng * pi) + 40.0 *\n            math.sin(lng / 3.0 * pi)) * 2.0 / 3.0\n    ret += (150.0 * math.sin(lng / 12.0 * pi) + 300.0 *\n            math.sin(lng / 30.0 * pi)) * 2.0 / 3.0\n    return ret\n\ndef out_of_china(lng, lat):\n    return not (lng > 73.66 and lng < 135.05 and lat > 3.86 and lat < 53.55)\n\ndef convert_by_type(lng, lat, type):\n    if type == 'g2b':\n        return gcj02_to_bd09(lng, lat)\n    elif type == 'b2g':\n        return bd09_to_gcj02(lng, lat)\n    elif type == 'w2g':\n        return wgs84_to_gcj02(lng, lat)\n    elif type == 'g2w':\n        return gcj02_to_wgs84(lng, lat)\n    elif type == 'b2w':\n        return bd09_to_wgs84(lng, lat)\n    elif type == 'w2b':\n        return wgs84_to_bd09(lng, lat)\n    else:\n        print('Usage: type must be in one of g2b, b2g, w2g, g2w, b2w, w2b')\n        sys.exit()","metadata":{"execution":{"iopub.status.busy":"2025-12-20T09:55:49.969712Z","iopub.execute_input":"2025-12-20T09:55:49.970074Z","iopub.status.idle":"2025-12-20T09:55:49.988193Z","shell.execute_reply.started":"2025-12-20T09:55:49.970048Z","shell.execute_reply":"2025-12-20T09:55:49.987690Z"},"papermill":{"duration":0.063224,"end_time":"2025-09-16T08:32:50.429369","exception":false,"start_time":"2025-09-16T08:32:50.366145","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Module: loss.py\nclass MSELoss(nn.Module):\n    def __init__(self):\n        super(MSELoss, self).__init__()\n        self.mse_loss = nn.MSELoss(reduction='sum')\n\n    def forward(self, truth, predict):\n        loss = self.mse_loss(predict, truth)\n        return loss\n\nclass BCELoss(nn.Module):\n    def __init__(self):\n        super(BCELoss, self).__init__()\n        self.bce_loss = nn.BCELoss()\n\n    def forward(self, truth, predict):\n        loss = self.bce_loss(predict, truth)\n        return loss\n\n# Module: metric.py\ndef evaluate(y_predictions: np.ndarray, y_targets: np.ndarray, threshold: float = 0.5):\n    assert y_predictions.shape == y_targets.shape, \\\n        f'Predictions of shape {y_predictions.shape} while targets of shape {y_predictions.shape}.'\n    rmse = mean_squared_error(y_targets, y_predictions) ** 0.5\n    pcc, _ = pearsonr(y_predictions, y_targets)\n\n    y_predictions_class = y_predictions >= threshold\n    y_targets_class = y_targets == 1\n\n    tp = ((y_predictions_class == 1) & (y_targets_class == 1)).sum()\n    fp = ((y_predictions_class == 1) & (y_targets_class == 0)).sum()\n    fn = ((y_predictions_class == 0) & (y_targets_class == 1)).sum()\n\n    if tp + fp != 0:\n        precision = tp / (tp + fp)\n    else:\n        precision = 0.0\n    if tp + fn != 0:\n        recall = tp / (tp + fn)\n    else:\n        recall = 0.0\n    if precision + recall != 0:\n        f1_score = 2 * (precision * recall) / (precision + recall)\n    else:\n        f1_score = 0.0\n\n    # Use original probabilities for AUC\n    auc = metrics.roc_auc_score(y_targets_class, y_predictions)\n\n    # Create a dictionary of metrics\n    results = {\n        'RMSE': rmse,\n        'PCC': pcc,\n        'PRECISION': precision,\n        'RECALL': recall,\n        'F1-SCORE': f1_score,\n        'AUC': auc\n    }\n\n    return results\n\n# Module: util.py\ndef convert_to_gpu(data):\n    return data.to(get_attribute('device'))\n\ndef convert_train_truth_to_gpu(train_data, truth_data):\n    train_data = [convert_to_gpu(data) for data in train_data]\n    truth_data = convert_to_gpu(truth_data)\n    return train_data, truth_data\n\ndef load_model(model, modelFilePath):\n    model.load_state_dict(torch.load(modelFilePath))\n    return model\n\ndef save_model(path: str, **save_dict):\n    os.makedirs(os.path.split(path)[0], exist_ok=True)\n    torch.save(save_dict, path)","metadata":{"execution":{"iopub.status.busy":"2025-12-20T09:55:49.988949Z","iopub.execute_input":"2025-12-20T09:55:49.989188Z","iopub.status.idle":"2025-12-20T09:55:50.012743Z","shell.execute_reply.started":"2025-12-20T09:55:49.989166Z","shell.execute_reply":"2025-12-20T09:55:50.012141Z"},"papermill":{"duration":0.059267,"end_time":"2025-09-16T08:32:50.536242","exception":false,"start_time":"2025-09-16T08:32:50.476975","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4. Kiến trúc Model (Model Architecture)","metadata":{"papermill":{"duration":0.047145,"end_time":"2025-09-16T08:32:50.633421","exception":false,"start_time":"2025-09-16T08:32:50.586276","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Module: fully_connected.py\nclass fully_connected_layer(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(fully_connected_layer, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        fcList = []\n        reluList = []\n        \n        current_size = input_size\n        for index in range(len(self.hidden_size)):\n            fc = nn.Linear(current_size, self.hidden_size[index])\n            setattr(self, f'fc{index}', fc)\n            fcList.append(fc)\n            relu = nn.ReLU()\n            setattr(self, f'relu{index}', relu)\n            reluList.append(relu)\n            current_size = self.hidden_size[index]\n            \n        self.last_fc = nn.Linear(self.hidden_size[-1], self.output_size)\n\n        self.fcList = nn.ModuleList(fcList)\n        self.reluList = nn.ModuleList(reluList)\n\n    def forward(self, input_tensor):\n        for idx in range(len(self.fcList)):\n            out = self.fcList[idx](input_tensor)\n            out = self.reluList[idx](out)\n            input_tensor = out\n        output_tensor = self.last_fc(input_tensor)\n        return output_tensor","metadata":{"execution":{"iopub.status.busy":"2025-12-20T09:55:50.013512Z","iopub.execute_input":"2025-12-20T09:55:50.013741Z","iopub.status.idle":"2025-12-20T09:55:50.033417Z","shell.execute_reply.started":"2025-12-20T09:55:50.013726Z","shell.execute_reply":"2025-12-20T09:55:50.032888Z"},"papermill":{"duration":0.054568,"end_time":"2025-09-16T08:32:50.735405","exception":false,"start_time":"2025-09-16T08:32:50.680837","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Module: spatial_layer.py\nclass GCN(nn.Module):\n    def __init__(self, in_features: int, hidden_sizes: List[int], out_features: int):\n        super(GCN, self).__init__()\n        gcns, relus, bns = nn.ModuleList(), nn.ModuleList(), nn.ModuleList()\n        for idx, hidden_size in enumerate(hidden_sizes):\n            if idx == 0:\n                gcns.append(GraphConv(in_features, hidden_size))\n                relus.append(nn.ReLU())\n                bns.append(nn.BatchNorm1d(hidden_size))\n            else:\n                gcns.append(GraphConv(hidden_sizes[idx - 1], hidden_size))\n                relus.append(nn.ReLU())\n                bns.append(nn.BatchNorm1d(hidden_size))\n        relus.append(nn.ReLU())\n        bns.append(nn.BatchNorm1d(out_features))\n        gcns.append(GraphConv(hidden_sizes[-1], out_features))\n        self.gcns, self.relus, self.bns = gcns, relus, bns\n\n    def forward(self, g: dgl.DGLGraph, node_features: torch.Tensor):\n        g = g.local_var() # Make a local copy of the graph\n        h = node_features\n        for i in range(len(self.gcns)):\n            h = self.gcns[i](g, h)\n            if len(h.shape) > 2:\n                h = self.bns[i](h.transpose(1, -1)).transpose(1, -1)\n            else:\n                h = self.bns[i](h)\n            h = self.relus[i](h)\n        return h\n\nclass StackedSBlocks(nn.ModuleList):\n    def __init__(self, *args, **kwargs):\n        super(StackedSBlocks, self).__init__(*args, **kwargs)\n\n    def forward(self, *input):\n        g, h = input\n        for module in self[:-1]:\n            h = h + module(g, h)\n        h = self[-1](g, h)\n        return h","metadata":{"execution":{"iopub.status.busy":"2025-12-20T09:55:50.035373Z","iopub.execute_input":"2025-12-20T09:55:50.035984Z","iopub.status.idle":"2025-12-20T09:55:50.055224Z","shell.execute_reply.started":"2025-12-20T09:55:50.035966Z","shell.execute_reply":"2025-12-20T09:55:50.054642Z"},"papermill":{"duration":0.057482,"end_time":"2025-09-16T08:32:50.840084","exception":false,"start_time":"2025-09-16T08:32:50.782602","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Module: spatial_temporal_layer.py\nclass STBlock(nn.Module):\n    def __init__(self, f_in: int, f_out: int):\n        super(STBlock, self).__init__()\n        self.spatial_embedding = GCN(f_in, [(f_in * (4 - i) + f_out * i) // 4 for i in (1, 4)], f_out)\n        self.temporal_embedding = nn.Conv1d(f_out, f_out, 3, padding=1)\n\n    def forward(self, g: dgl.DGLGraph, temporal_features: torch.Tensor):\n        # Transpose to (node_num, t_in, f_in) for GCN\n        spatial_input = temporal_features.transpose(-2, -1)\n        spatial_output = self.spatial_embedding(g, spatial_input)\n        # Transpose back to (node_num, f_out, t_in) for Conv1d\n        temporal_input = spatial_output.transpose(-2, -1)\n        temporal_output = self.temporal_embedding(temporal_input)\n        return temporal_output\n\nclass StackedSTBlocks(nn.ModuleList):\n    def __init__(self, *args, **kwargs):\n        super(StackedSTBlocks, self).__init__(*args, **kwargs)\n\n    def forward(self, *input):\n        g, h = input\n        for module in self:\n            processed_h = module(g, h)\n            # Concatenate along the feature dimension (dim=1)\n            h = torch.cat((h, processed_h), dim=1)\n        return h","metadata":{"execution":{"iopub.status.busy":"2025-12-20T09:55:50.055840Z","iopub.execute_input":"2025-12-20T09:55:50.055994Z","iopub.status.idle":"2025-12-20T09:55:50.074972Z","shell.execute_reply.started":"2025-12-20T09:55:50.055981Z","shell.execute_reply":"2025-12-20T09:55:50.074447Z"},"papermill":{"duration":0.055503,"end_time":"2025-09-16T08:32:50.943984","exception":false,"start_time":"2025-09-16T08:32:50.888481","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Module: DSTGCN.py\nclass DSTGCN(nn.Module):\n    def __init__(self, f_1: int, f_2: int, f_3: int):\n        super(DSTGCN, self).__init__()\n\n        self.spatial_embedding = fully_connected_layer(f_1, [20], 15)\n        self.spatial_gcn = StackedSBlocks([GCN(15, [15, 15, 15], 15),\n                                           GCN(15, [15, 15, 15], 15),\n                                           GCN(15, [14, 13, 12, 11], 10)])\n        # Correct final dimension after concatenation in StackedSTBlocks\n        # Initial: f_2 -> After STBlock(f_2, 4): f_2 + 4 -> After STBlock(f_2+4, 5): f_2+4+5 -> After STBlock(f_2+4+5, 10): f_2+4+5+10\n        # Given f_2=1: 1 -> 1+4=5 -> 5+5=10 -> 10+10=20\n        self.temporal_embedding = StackedSTBlocks([STBlock(f_2, 4), STBlock(f_2 + 4, 5), STBlock(f_2 + 4 + 5, 10)])\n\n        # The input channel to AvgPool1d is the final feature dimension from temporal_embedding\n        final_temporal_dim = f_2 + 4 + 5 + 10\n        self.temporal_agg = nn.Sequential(\n            nn.Linear(final_temporal_dim, 20), # Aggregate features\n            nn.ReLU()\n        )\n        \n        self.external_embedding = fully_connected_layer(f_3, [(f_3 * (4 - i) + 10 * i) // 4 for i in (1, 4)], 10)\n\n        # Input to final layer: 10 (spatial) + 20 (temporal) + 10 (external)\n        self.output_layer = nn.Sequential(nn.Linear(10 + 20 + 10, 1),\n                                          nn.Sigmoid())\n\n    def forward(self,\n                bg: dgl.DGLGraph,\n                spatial_features: torch.Tensor,\n                temporal_features: torch.Tensor,\n                external_features: torch.Tensor):\n\n        # Spatial stream\n        s_emb = self.spatial_embedding(spatial_features)\n        s_out = self.spatial_gcn(bg, s_emb)\n\n        # Temporal stream\n        temporal_embeddings = self.temporal_embedding(bg, temporal_features)\n        # temporal_embeddings shape: [node_num, final_temporal_dim, T]\n        \n        # Aggregate across time dimension (T=24)\n        t_agg_time = torch.mean(temporal_embeddings, dim=2) # Shape: [node_num, final_temporal_dim]\n        t_out = self.temporal_agg(t_agg_time) # Shape: [node_num, 20]\n\n        # External stream\n        e_out = self.external_embedding(external_features)\n\n        # Aggregate node features for each graph in the batch\n        # We use the features of the first node (the target node) for prediction\n        nums_nodes = bg.batch_num_nodes().tolist()\n        s_features_list, t_features_list = [], []\n        \n        node_idx_offset = 0\n        for num_nodes in nums_nodes:\n            # The first node of each subgraph is the target node\n            s_features_list.append(s_out[node_idx_offset])\n            t_features_list.append(t_out[node_idx_offset])\n            node_idx_offset += num_nodes\n\n        s_features = torch.stack(s_features_list) # Shape: [batch_size, 10]\n        t_features = torch.stack(t_features_list) # Shape: [batch_size, 20]\n        \n        # Concatenate features from all streams\n        output_features = torch.cat((s_features, t_features, e_out), dim=-1)\n\n        return self.output_layer(output_features)","metadata":{"execution":{"iopub.status.busy":"2025-12-20T09:55:50.075642Z","iopub.execute_input":"2025-12-20T09:55:50.075875Z","iopub.status.idle":"2025-12-20T09:55:50.095993Z","shell.execute_reply.started":"2025-12-20T09:55:50.075853Z","shell.execute_reply":"2025-12-20T09:55:50.095483Z"},"papermill":{"duration":0.057086,"end_time":"2025-09-16T08:32:51.048182","exception":false,"start_time":"2025-09-16T08:32:50.991096","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5. Tải dữ liệu (Data Loading)\n\n**Lưu ý quan trọng:** Trước khi chạy cell bên dưới, bạn cần tải dữ liệu của mình lên Colab. Hãy tạo một thư mục tên là `data` trong cây thư mục của Colab (ngang hàng với `sample_data`) và tải các file sau vào đó:\n- `beijing_roadnet.gpickle`\n- `edges_data.h5`\n- `accident.h5`\n- `weather.h5`\n- `all_grids_speed.h5`\n\nNếu bạn dùng Google Drive, hãy mount Drive và thay đổi đường dẫn cho phù hợp.","metadata":{"papermill":{"duration":0.047203,"end_time":"2025-09-16T08:32:51.143371","exception":false,"start_time":"2025-09-16T08:32:51.096168","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Module: data_container.py\n\nlongitudeMin = 116.09608\nlongitudeMax = 116.71040\nlatitudeMin = 39.69086\nlatitudeMax = 40.17647\n\nlongitudeMin, latitudeMin = convert_by_type(lng=longitudeMin, lat=latitudeMin, type=\"g2w\")\nlongitudeMax, latitudeMax = convert_by_type(lng=longitudeMax, lat=latitudeMax, type=\"g2w\")\n\ndivideBound = 5\n\nwidthSingle = 0.01 / math.cos(latitudeMin / 180 * math.pi) / divideBound\nwidth = math.floor((longitudeMax - longitudeMin) / widthSingle)\nheightSingle = 0.01 / divideBound\nheight = math.floor((latitudeMax - latitudeMin) / heightSingle)\n\ndef collate_fn(batch):\n    ret = list()\n    for idx, item in enumerate(zip(*batch)):\n        if isinstance(item[0], torch.Tensor):\n            if idx < 3:  # spatial and temporal features\n                ret.append(torch.cat(item))\n            else:  # overall features and y\n                ret.append(torch.stack(item))\n        elif isinstance(item[0], dgl.DGLGraph):\n            ret.append(dgl.batch(item))\n        else:\n            raise ValueError(f'batch must contain tensors or graphs; found {type(item[0])}')\n    return tuple(ret)\n\ndef fill_speed(speed_data):\n    date_range = pd.date_range(start=\"2018-08-01\", end=\"2018-11-01\", freq=\"1H\")[:-1]\n    speed_data = speed_data.resample(rule=\"1H\").mean()\n    assert date_range[0] in speed_data.index and date_range[-1] in speed_data.index\n    \n    one_week, two_week = datetime.timedelta(days=7), datetime.timedelta(days=14)\n    \n    # Tối ưu hóa - tránh việc kiểm tra từng ngày\n    missing_dates = []\n    for date in tqdm(date_range, 'Finding missing dates'):\n        if any(speed_data.loc[date].isna()):\n            missing_dates.append(date)\n    \n    print(f\"Found {len(missing_dates)} dates with missing data\")\n    \n    # Xử lý chỉ những ngày missing\n    for date in tqdm(missing_dates, 'Fill speed'):\n        for idx in [date - one_week, date + one_week, date - two_week, date + two_week]:\n            if idx in speed_data.index and all(speed_data.loc[idx].notna()):\n                speed_data.loc[date] = speed_data.loc[idx]\n                break\n        else:\n            print(f\"Warning: Cannot find replacement for {date}\")\n            # Thay vì raise error, có thể fill với giá trị trung bình\n            speed_data.loc[date] = speed_data.mean()\n    \n    return speed_data\n\nclass AccidentDataset(Dataset):\n    def __init__(self,\n                 k_order: int,\n                 network: nx.Graph,\n                 node_attr: pd.DataFrame,\n                 accident: pd.DataFrame,\n                 weather: pd.DataFrame,\n                 speed: pd.DataFrame,\n                 sf_scaler: Tuple[np.ndarray, np.ndarray] = None,\n                 tf_scaler: Tuple[np.ndarray, np.ndarray] = None,\n                 ef_scaler: Tuple[np.ndarray, np.ndarray] = None):\n        self.k_order = k_order\n        self.network = network\n        self.nodes = node_attr\n        self.accident = accident\n        self.weather = weather\n        self.speed = speed\n        self.sf_scaler = sf_scaler\n        self.tf_scaler = tf_scaler\n        self.ef_scaler = ef_scaler\n\n    def __getitem__(self, sample_id: int):\n        _, _, accident_time, node_id, target = self.accident.iloc[sample_id]\n        neighbors = nx.single_source_shortest_path_length(self.network, node_id, cutoff=self.k_order)\n        neighbors.pop(node_id, None)\n        neighbors = [node_id] + sorted(neighbors.keys())\n        \n        sub_graph_nx = nx.subgraph(self.network, neighbors)\n        # Relabel nodes to be contiguous integers from 0\n        relabel_map = {old_label: new_label for new_label, old_label in enumerate(neighbors)}\n        sub_graph_nx = nx.relabel_nodes(sub_graph_nx, relabel_map)\n        sub_graph_nx.add_edges_from([(v, v) for v in sub_graph_nx.nodes])\n        g = dgl.from_networkx(sub_graph_nx)\n\n        date_range = pd.date_range(end=accident_time.strftime(\"%Y%m%d %H\"), freq=\"1H\", periods=24)\n        selected_time = self.speed.loc[date_range]\n        selected_nodes = self.nodes.loc[neighbors]\n        spatial_features = selected_nodes['spatial_features'].tolist()\n\n        x_ids = np.floor((selected_nodes['XCoord'].values - longitudeMin) / widthSingle).astype(np.int64)\n        y_ids = np.floor((selected_nodes['YCoord'].values - latitudeMin) / heightSingle).astype(np.int64)\n        \n        temporal_features = selected_time[map(lambda ids: f'{ids[0]},{ids[1]}', zip(y_ids, x_ids))].values.transpose()\n\n        weather_data = self.weather.loc[date_range[-1]].tolist()\n        external_features = weather_data + [accident_time.month, accident_time.day, accident_time.dayofweek,\n                                       accident_time.hour, int(accident_time.dayofweek >= 5)]\n\n        if self.sf_scaler is not None:\n            mean, std = self.sf_scaler\n            spatial_features = (np.array(spatial_features) - mean) / std\n        if self.tf_scaler is not None:\n            mean, std = self.tf_scaler\n            temporal_features = (np.array(temporal_features) - mean) / std\n        if self.ef_scaler is not None:\n            mean, std = self.ef_scaler\n            external_features = (np.array(external_features) - mean) / std\n\n        spatial_features = torch.tensor(spatial_features).float()\n        temporal_features = torch.tensor(temporal_features).unsqueeze(1).float()\n        external_features = torch.tensor(external_features).float()\n        target = torch.tensor(target).float()\n\n        return g, spatial_features, temporal_features, external_features, target\n\n    def __len__(self):\n        return len(self.accident)\n\ndef get_data_loaders(k_order, batch_size):\n    # Thay đổi đường dẫn tương đối\n    network_path = r'/kaggle/input/dstgcn-dataset/beijing_roadnet.gpickle'\n    node_attr_path = r'/kaggle/input/dstgcn-dataset/edges_data.h5'\n    accident_path = r'/kaggle/input/dstgcn-dataset/accident.h5'\n    weather_path = \"/kaggle/input/dstgcn-dataset/weather.h5\"\n    speed_path = \"/kaggle/input/dstgcn-dataset/all_grids_speed.h5\"\n\n    sf_mean, sf_std = np.array(get_attribute('spatial_features_mean')), np.array(get_attribute('spatial_features_std'))\n    tf_mean, tf_std = np.array(get_attribute('temporal_features_mean')), np.array(get_attribute('temporal_features_std'))\n    ef_mean, ef_std = np.array(get_attribute('external_features_mean')), np.array(get_attribute('external_features_std'))\n\n    import pickle\n    with open(network_path, 'rb') as f:\n        network = pickle.load(f)\n    nodes = pd.read_hdf(node_attr_path)\n    weather = pd.read_hdf(weather_path)\n    speed = fill_speed(pd.read_hdf(speed_path))\n\n    dls = dict()\n    for key in ['train', 'validate', 'test']:\n        accident = pd.read_hdf(accident_path, key=key)\n        dataset = AccidentDataset(k_order, network, nodes, accident, weather, speed,\n                                  sf_scaler=(sf_mean, sf_std),\n                                  tf_scaler=(tf_mean, tf_std),\n                                  ef_scaler=(ef_mean, ef_std))\n        # Giảm num_workers cho Colab để tránh lỗi\n        dls[key] = DataLoader(dataset=dataset,\n                              batch_size=batch_size,\n                              shuffle=True,\n                              drop_last=False,\n                              collate_fn=collate_fn,\n                              num_workers=2) \n    return dls","metadata":{"execution":{"iopub.status.busy":"2025-12-20T09:55:50.096830Z","iopub.execute_input":"2025-12-20T09:55:50.097089Z","iopub.status.idle":"2025-12-20T09:55:50.123752Z","shell.execute_reply.started":"2025-12-20T09:55:50.097068Z","shell.execute_reply":"2025-12-20T09:55:50.123141Z"},"papermill":{"duration":0.067232,"end_time":"2025-09-16T08:32:51.256923","exception":false,"start_time":"2025-09-16T08:32:51.189691","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 6. Hàm Huấn luyện (Training Function)","metadata":{"papermill":{"duration":0.046315,"end_time":"2025-09-16T08:32:51.349237","exception":false,"start_time":"2025-09-16T08:32:51.302922","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Module: train_model.py\ndef train_model(model: nn.Module,\n                data_loaders: Dict[str, DataLoader],\n                loss_func: callable,\n                optimizer,\n                model_folder: str,\n                tensorboard_folder: str):\n    phases = ['train', 'validate', 'test']\n    writer = SummaryWriter(tensorboard_folder)\n    num_epochs = get_attribute('epochs')\n\n    since = time.time()\n\n    model = convert_to_gpu(model)\n    loss_func = convert_to_gpu(loss_func)\n\n    save_dict, best_f1_score = {'model_state_dict': copy.deepcopy(model.state_dict()), 'epoch': 0}, 0\n\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=.5, patience=2, threshold=1e-3, min_lr=1e-6)\n    test_metric = None\n\n    try:\n        for epoch in range(num_epochs):\n            print(f'Epoch {epoch}/{num_epochs - 1}')\n            print('-' * 10)\n            running_loss, running_metrics = {phase: 0.0 for phase in phases}, {phase: dict() for phase in phases}\n            save_validate_this_epoch = False\n\n            for phase in phases:\n                if phase == 'train':\n                    model.train()\n                else:\n                    model.eval()\n\n                steps, predictions, targets = 0, list(), list()\n                tqdm_loader = tqdm(data_loaders[phase], ncols=120)\n                for g, spatial_features, temporal_features, external_features, truth_data in tqdm_loader:\n                    features, truth_data = convert_train_truth_to_gpu(\n                        [spatial_features, temporal_features, external_features], truth_data)\n                    g = convert_to_gpu(g)\n\n                    with torch.set_grad_enabled(phase == 'train'):\n                        outputs = model(g, *features)\n                        outputs = torch.squeeze(outputs)\n\n                        loss = loss_func(truth_data, outputs)\n\n                        if phase == 'train':\n                            optimizer.zero_grad()\n                            loss.backward()\n                            optimizer.step()\n\n                    targets.append(truth_data.cpu().numpy())\n                    with torch.no_grad():\n                        predictions.append(outputs.cpu().numpy())\n\n                    running_loss[phase] += loss.item() * truth_data.size(0)\n                    steps += truth_data.size(0)\n\n                    tqdm_loader.set_description(\n                        f'{phase:8} epoch: {epoch:3}, {phase:8} loss: {running_loss[phase] / steps:3.6}')\n\n                    torch.cuda.empty_cache()\n\n                print(f'{phase} metric ...')\n                scores = evaluate(np.concatenate(predictions), np.concatenate(targets))\n                running_metrics[phase] = scores\n                print(scores)\n\n                if phase == 'validate' and scores['F1-SCORE'] > best_f1_score:\n                    best_f1_score = scores['F1-SCORE']\n                    save_validate_this_epoch = True\n                    save_dict.update(model_state_dict=copy.deepcopy(model.state_dict()),\n                                     epoch=epoch,\n                                     optimizer_state_dict=copy.deepcopy(optimizer.state_dict()))\n                    print(f\"save model as {model_folder}/model_{epoch}.pkl\")\n                    save_model(f\"{model_folder}/model_{epoch}.pkl\", **save_dict)\n\n            scheduler.step(running_loss['train'])\n\n            if save_validate_this_epoch:\n                test_metric = running_metrics[\"test\"].copy()\n\n            for metric in running_metrics['train'].keys():\n                writer.add_scalars(metric, {\n                    f'{phase} {metric}': running_metrics[phase][metric] for phase in phases},\n                                   global_step=epoch)\n            writer.add_scalars('Loss', {\n                f'{phase} loss': running_loss[phase] / len(data_loaders[phase].dataset) for phase in phases},\n                               global_step=epoch)\n    finally:\n        time_elapsed = time.time() - since\n        print(f\"cost {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n        save_model(f\"{model_folder}/best_model.pkl\", **save_dict)\n\n    return test_metric","metadata":{"execution":{"iopub.status.busy":"2025-12-20T09:55:50.124598Z","iopub.execute_input":"2025-12-20T09:55:50.124837Z","iopub.status.idle":"2025-12-20T09:55:50.145223Z","shell.execute_reply.started":"2025-12-20T09:55:50.124816Z","shell.execute_reply":"2025-12-20T09:55:50.144725Z"},"papermill":{"duration":0.0604,"end_time":"2025-09-16T08:32:51.456681","exception":false,"start_time":"2025-09-16T08:32:51.396281","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Module: train_main.py (phần thực thi chính)\n\ndef create_model() -> nn.Module:\n    return DSTGCN(get_attribute('poi_features_number'), get_attribute('temporal_features_number'),\n                  get_attribute('weather_features_number') + get_attribute('external_temporal_features_number'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T09:56:56.683058Z","iopub.execute_input":"2025-12-20T09:56:56.683378Z","iopub.status.idle":"2025-12-20T09:56:56.687917Z","shell.execute_reply.started":"2025-12-20T09:56:56.683357Z","shell.execute_reply":"2025-12-20T09:56:56.687101Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport os\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom scipy.stats import pearsonr\nfrom tqdm import tqdm\n\n# --- 1. Thiết lập lại thiết bị & Load cấu hình ---\n# Đảm bảo hàm get_attribute và biến config đã tồn tại (chạy cell Config trước)\ndevice = torch.device(get_attribute('device'))\nprint(f\"Using device: {device}\")\n\n# --- 2. Hàm đánh giá chi tiết (MAE, RMSE, PCC) ---\ndef evaluate_metrics(model, data_loader, device):\n    model.eval()  # Chế độ đánh giá\n    predictions = []\n    targets = []\n    \n    print(\"Đang chạy đánh giá trên tập Test...\")\n    with torch.no_grad():\n        for g, spatial_features, temporal_features, external_features, truth_data in tqdm(data_loader, ncols=100):\n            # Chuyển dữ liệu sang GPU/CPU\n            # Sử dụng lại các hàm tiện ích trong notebook gốc nếu có, hoặc dùng .to(device) trực tiếp\n            spatial_features = spatial_features.to(device)\n            temporal_features = temporal_features.to(device)\n            external_features = external_features.to(device)\n            truth_data = truth_data.to(device)\n            g = g.to(device)\n\n            # Dự đoán\n            outputs = model(g, spatial_features, temporal_features, external_features)\n            outputs = torch.squeeze(outputs)\n\n            # Lưu kết quả (chuyển về CPU để tính toán sklearn)\n            predictions.extend(outputs.cpu().numpy())\n            targets.extend(truth_data.cpu().numpy())\n\n    # Chuyển list sang numpy array\n    y_pred = np.array(predictions)\n    y_true = np.array(targets)\n\n    # Tính toán các chỉ số\n    mae = mean_absolute_error(y_true, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    pcc, _ = pearsonr(y_pred.flatten(), y_true.flatten())\n\n    return mae, rmse, pcc\n\n# --- 3. Khởi tạo Model & Load Weights ---\nprint(\"Khởi tạo model...\")\nmodel = create_model() # Hàm này phải được định nghĩa trong notebook\nmodel = model.to(device)\n\n# Đường dẫn file model đã lưu (từ code train cũ)\nsave_path = f\"/kaggle/input/dstgcn/saves/DSTGCN/best_model.pkl\"\n\nif os.path.exists(save_path):\n    print(f\"Đang tải trọng số từ: {save_path}\")\n    checkpoint = torch.load(save_path, map_location=device)\n    \n    # Kiểm tra cấu trúc file save (dict hay state_dict trực tiếp)\n    if 'model_state_dict' in checkpoint:\n        model.load_state_dict(checkpoint['model_state_dict'])\n    else:\n        model.load_state_dict(checkpoint)\n    print(\"Load model thành công!\")\nelse:\n    print(f\"CẢNH BÁO: Không tìm thấy file {save_path}. Hãy đảm bảo bạn đã train xong và file được lưu.\")\n    # Nếu không có file save, code sẽ chạy với trọng số ngẫu nhiên (không đúng)\n\n# --- 4. Load Dữ liệu Test ---\nprint(\"Đang load dữ liệu test...\")\n# Gọi lại hàm lấy dataloader\nloaders = get_data_loaders(get_attribute('K_hop'), get_attribute('batch_size'))\ntest_loader = loaders['test']\n\n# --- 5. Thực thi tính toán ---\nif os.path.exists(save_path):\n    mae, rmse, pcc = evaluate_metrics(model, test_loader, device)\n    \n    print(\"\\n\" + \"=\"*30)\n    print(\"KẾT QUẢ ĐÁNH GIÁ (TEST SET)\")\n    print(\"=\"*30)\n    print(f\"MAE  : {mae:.6f}\")\n    print(f\"RMSE : {rmse:.6f}\")\n    print(f\"PCC  : {pcc:.6f}\")\n    print(\"=\"*30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T09:57:00.840114Z","iopub.execute_input":"2025-12-20T09:57:00.840843Z","execution_failed":"2025-12-20T10:02:47.756Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 7. Thực thi chính (Main Execution)","metadata":{"papermill":{"duration":0.047583,"end_time":"2025-09-16T08:32:51.550296","exception":false,"start_time":"2025-09-16T08:32:51.502713","status":"completed"},"tags":[]}}]}