{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13060620,"sourceType":"datasetVersion","datasetId":8270673}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchdata==0.7.1 --quiet\n!pip install dgl -f https://data.dgl.ai/wheels/torch-2.1/cu121/repo.html --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-20T03:45:45.797798Z","iopub.execute_input":"2025-12-20T03:45:45.798399Z","iopub.status.idle":"2025-12-20T03:48:40.066178Z","shell.execute_reply.started":"2025-12-20T03:45:45.798374Z","shell.execute_reply":"2025-12-20T03:48:40.065480Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.4/184.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchtune 0.6.1 requires torchdata==0.11.0, but you have torchdata 0.7.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m483.2/483.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchtune 0.6.1 requires torchdata==0.11.0, but you have torchdata 0.7.1 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ntorchvision 0.23.0+cu126 requires torch==2.8.0, but you have torch 2.4.0 which is incompatible.\ntorchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install tensorboardX pandas numpy networkx tqdm scikit-learn scipy --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T03:48:40.067442Z","iopub.execute_input":"2025-12-20T03:48:40.067723Z","iopub.status.idle":"2025-12-20T03:48:43.720123Z","shell.execute_reply.started":"2025-12-20T03:48:40.067692Z","shell.execute_reply":"2025-12-20T03:48:43.719440Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nimport dgl\nimport math\nimport os\nimport sys\nfrom tqdm import tqdm\nfrom scipy.sparse.linalg import eigs\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom scipy.stats import pearsonr\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n# --- Thiết lập Device ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# --- Utils cho GCN (Bổ sung phần thiếu) ---\ndef scaled_Laplacian(W):\n    '''\n    Tính toán Scaled Laplacian matrix cho ChebNet\n    '''\n    assert W.shape[0] == W.shape[1]\n    D = torch.diag(torch.sum(W, axis=1))\n    L = D - W\n    lambda_max = 2.0  # Giả định, hoặc tính bằng eigs(L, k=1, which='LM')[0].real\n    if isinstance(W, torch.Tensor):\n        return (2 * L) / lambda_max - torch.eye(W.shape[0]).to(W.device)\n    else:\n        return (2 * L) / lambda_max - np.identity(W.shape[0])\n\ndef cheb_polynomial(L_tilde, K):\n    '''\n    Tính đa thức Chebyshev\n    '''\n    N = L_tilde.shape[0]\n    cheb_polynomials = [torch.eye(N).to(L_tilde.device), L_tilde.clone()]\n    for i in range(2, K):\n        cheb_polynomials.append(2 * L_tilde * cheb_polynomials[i - 1] - cheb_polynomials[i - 2])\n    return cheb_polynomials","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T03:48:43.722053Z","iopub.execute_input":"2025-12-20T03:48:43.722296Z","iopub.status.idle":"2025-12-20T03:48:48.030053Z","shell.execute_reply.started":"2025-12-20T03:48:43.722271Z","shell.execute_reply":"2025-12-20T03:48:48.029437Z"}},"outputs":[{"name":"stderr","text":"DGL backend not selected or invalid.  Assuming PyTorch for now.\n","output_type":"stream"},{"name":"stdout","text":"Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from torch.nn.parameter import Parameter\nfrom torch.nn.modules.module import Module\n\n# --- Paste toàn bộ các class từ file bạn cung cấp vào đây ---\n# Tôi sẽ rút gọn lại để tập trung vào Encoder_GRU_SubMAGCN vì chúng ta dùng nó làm encoder chính.\n\nclass GCN(nn.Module):\n    def __init__(self, dim_in, dim_out, order_K, device, in_drop=0.0, gcn_drop=0.0, residual=False):\n        super(GCN, self).__init__()\n        self.DEVICE = device\n        self.order_K = order_K\n        self.dim_in = dim_in\n        self.dim_out = dim_out\n        self.Theta = nn.ParameterList([nn.Parameter(torch.FloatTensor(dim_in, dim_out)) for _ in range(order_K)])\n        self.weights = nn.Parameter(torch.FloatTensor(size=(dim_out, dim_out)))\n        self.biases = nn.Parameter(torch.FloatTensor(size=(dim_out,)))\n        self._in_drop = in_drop\n        self._gcn_drop = gcn_drop\n        self._residual = residual\n        self.linear = nn.Linear(dim_in, dim_out)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        for param in self.parameters():\n            if len(param.shape) > 1:\n                nn.init.xavier_uniform_(param)\n            else:\n                nn.init.zeros_(param)\n\n    def forward(self, x, L_tilde):\n        # x: (batch, nodes, features)\n        batch_size, num_of_vertices, in_channels = x.shape\n        cheb_polynomials = cheb_polynomial(L_tilde, self.order_K)\n        \n        output = torch.zeros(batch_size, num_of_vertices, self.dim_out).to(self.DEVICE)\n        \n        x0 = x\n        if self._in_drop != 0:\n            x = torch.dropout(x, 1.0 - self._in_drop, train=self.training)\n            \n        for k in range(self.order_K):\n            # Cheb poly: (nodes, nodes)\n            # x: (batch, nodes, feat) -> permute(0,2,1) -> (batch, feat, nodes)\n            # Support = x * T_k\n            support = torch.matmul(x.permute(0, 2, 1), cheb_polynomials[k]).permute(0, 2, 1)\n            output = output + torch.matmul(support, self.Theta[k])\n            \n        output = torch.matmul(output, self.weights) + self.biases\n        res = F.relu(output)\n        \n        if self._gcn_drop != 0.0:\n            res = torch.dropout(res, 1.0 - self._gcn_drop, train=self.training)\n        if self._residual:\n            x0 = self.linear(x0)\n            res = res + x0\n        return res\n\nclass MRA_GCN_Simple(nn.Module):\n    # Simplified version of MRA for the adapter\n    def __init__(self, dim_in, dim_out, range_K, device):\n        super(MRA_GCN_Simple, self).__init__()\n        self.DEVICE = device\n        self.GCN = GCN(dim_in, dim_out, range_K, device)\n        self.W_a = nn.Parameter(torch.FloatTensor(dim_out, dim_out))\n        self.U = nn.Parameter(torch.FloatTensor(dim_out))\n        nn.init.xavier_uniform_(self.W_a)\n        nn.init.uniform_(self.U)\n\n    def forward(self, X, L_tilde):\n        # X: (batch, nodes, feat)\n        gcn_out = self.GCN(X, L_tilde) # (batch, nodes, dim_out)\n        \n        # Attention Mechanism (Simplified for node-level)\n        # e = gcn_out * Wa * U\n        score = torch.matmul(torch.matmul(gcn_out, self.W_a), self.U) # (batch, nodes)\n        alpha = F.softmax(score, dim=1).unsqueeze(-1) # (batch, nodes, 1)\n        \n        # Weighted context\n        # context = torch.sum(gcn_out * alpha, dim=1, keepdim=True) # (batch, 1, dim_out)\n        # return gcn_out + context # Residual connection style\n        return gcn_out\n\nclass Encoder_GRU_Custom(nn.Module):\n    def __init__(self, dim_in, dim_out, range_K, device):\n        super(Encoder_GRU_Custom, self).__init__()\n        self.DEVICE = device\n        self.dim_in = dim_in\n        self.dim_out = dim_out\n        \n        # Gates using GCN\n        self.gate_conv = MRA_GCN_Simple(dim_in + dim_out, dim_out * 2, range_K, device)\n        self.update_conv = MRA_GCN_Simple(dim_in + dim_out, dim_out, range_K, device)\n\n    def forward(self, x, hx, L_tilde):\n        # x: (batch, nodes, dim_in)\n        # hx: (batch, nodes, dim_out)\n        combined = torch.cat((x, hx), dim=-1)\n        \n        gates = self.gate_conv(combined, L_tilde)\n        reset_gate, update_gate = torch.split(gates, self.dim_out, dim=-1)\n        reset_gate = torch.sigmoid(reset_gate)\n        update_gate = torch.sigmoid(update_gate)\n        \n        combined_r = torch.cat((x, reset_gate * hx), dim=-1)\n        candidate = torch.tanh(self.update_conv(combined_r, L_tilde))\n        \n        hy = update_gate * hx + (1 - update_gate) * candidate\n        return hy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T03:48:48.030935Z","iopub.execute_input":"2025-12-20T03:48:48.031390Z","iopub.status.idle":"2025-12-20T03:48:48.046302Z","shell.execute_reply.started":"2025-12-20T03:48:48.031368Z","shell.execute_reply":"2025-12-20T03:48:48.045689Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class MADGCN(nn.Module):\n    def __init__(self, \n                 poi_feat_dim, \n                 temp_feat_dim, \n                 ext_feat_dim, \n                 hidden_dim=32, \n                 K_hop=3):\n        super(MADGCN, self).__init__()\n        \n        self.hidden_dim = hidden_dim\n        self.device = device\n        \n        # Feature embeddings\n        self.spatial_linear = nn.Linear(poi_feat_dim, hidden_dim)\n        self.temporal_linear = nn.Linear(temp_feat_dim, hidden_dim) \n        \n        # Encoder (Time series processing)\n        self.encoder = Encoder_GRU_Custom(dim_in=hidden_dim, dim_out=hidden_dim, range_K=K_hop, device=device)\n        \n        # External features processing\n        self.ext_linear = nn.Linear(ext_feat_dim, 16)\n        \n        # Final prediction\n        self.output_layer = nn.Sequential(\n            nn.Linear(hidden_dim + 16, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1),\n            nn.Sigmoid() \n        )\n\n    def forward(self, g, spatial_features, temporal_features, external_features):\n        \"\"\"\n        Sửa lỗi FileNotFoundError bằng cách tính Adjacency Matrix thủ công\n        \"\"\"\n        # --- BẮT ĐẦU ĐOẠN SỬA LỖI ---\n        # Thay vì gọi g.adjacency_matrix() gây lỗi thư viện C++\n        # Chúng ta lấy danh sách cạnh (src, dst) và tự tạo ma trận\n        src, dst = g.edges()\n        N = g.num_nodes()\n        \n        # Tạo ma trận kề dense (N x N)\n        adj = torch.zeros((N, N), device=self.device)\n        adj[src, dst] = 1 # Gán trọng số cạnh là 1\n        \n        # Tính Laplacian từ ma trận tự tạo này\n        L_tilde = scaled_Laplacian(adj) \n        # --- KẾT THÚC ĐOẠN SỬA LỖI ---\n        \n        # 2. Xử lý đầu vào (Input Processing)\n        # Chuyển temporal features về dạng (Time, Nodes, Feat)\n        ts_data = temporal_features.permute(2, 0, 1) \n        \n        # Khởi tạo hidden state\n        hx = torch.zeros(1, N, self.hidden_dim).to(self.device) \n        \n        # 3. Encoder Loop (GRU)\n        for t in range(ts_data.shape[0]):\n            x_t = ts_data[t].unsqueeze(0) # (1, Nodes, 1)\n            x_emb = self.temporal_linear(x_t) # (1, Nodes, Hidden)\n            hx = self.encoder(x_emb, hx, L_tilde)\n        \n        # hx shape: (1, Nodes, Hidden) -> squeeze -> (Nodes, Hidden)\n        node_features = hx.squeeze(0) \n        \n        # Lấy feature của các node trung tâm (Target Nodes)\n        # Trong DGL batch, các graph được nối tiếp nhau. Cần lấy node đầu tiên của mỗi subgraph.\n        batch_num_nodes_list = g.batch_num_nodes().tolist()\n        target_indices = []\n        current_idx = 0\n        for n_nodes in batch_num_nodes_list:\n            target_indices.append(current_idx)\n            current_idx += n_nodes\n            \n        target_node_feats = node_features[target_indices] # (Batch_Size, Hidden)\n        \n        # 4. Kết hợp External Features\n        ext_emb = self.ext_linear(external_features) # (Batch_Size, 16)\n        \n        final_emb = torch.cat((target_node_feats, ext_emb), dim=1)\n        \n        output = self.output_layer(final_emb)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T03:48:48.047308Z","iopub.execute_input":"2025-12-20T03:48:48.047607Z","iopub.status.idle":"2025-12-20T03:48:51.005533Z","shell.execute_reply.started":"2025-12-20T03:48:48.047580Z","shell.execute_reply":"2025-12-20T03:48:51.004689Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# --- Cấu hình ---\n# Giả lập config.json\nconfig = {\n    \"K_hop\": 2, # DSTGCN dùng subgraph, K nhỏ\n    \"batch_size\": 32,\n    \"spatial_features_mean\": [0]*22, # Placeholder, cần load từ file thật\n    \"spatial_features_std\": [1]*22,\n    \"temporal_features_mean\": [0],\n    \"temporal_features_std\": [1],\n    \"external_features_mean\": [0]*43,\n    \"external_features_std\": [1]*43,\n    \"poi_features_number\": 22,\n    \"temporal_features_number\": 1,\n    \"external_features_number\": 43, # 38 weather + 5 time\n    \"epochs\": 50,\n    \"learning_rate\": 0.001\n}\n\n# --- Helper Functions cho Data Loading ---\nlongitudeMin, longitudeMax = 116.09608, 116.71040\nlatitudeMin, latitudeMax = 39.69086, 40.17647\nwidthSingle = 0.01 / math.cos(latitudeMin / 180 * math.pi) / 5\nheightSingle = 0.01 / 5\n\ndef collate_fn(batch):\n    ret = list()\n    for idx, item in enumerate(zip(*batch)):\n        if isinstance(item[0], torch.Tensor):\n            if idx < 3: \n                ret.append(torch.cat(item))\n            else: \n                ret.append(torch.stack(item))\n        elif isinstance(item[0], dgl.DGLGraph):\n            ret.append(dgl.batch(item))\n    return tuple(ret)\n\ndef fill_speed(speed_data):\n    # (Giữ nguyên logic fill missing data của DSTGCN)\n    speed_data = speed_data.resample(rule=\"1H\").mean()\n    speed_data = speed_data.fillna(method='ffill').fillna(method='bfill')\n    return speed_data\n\nclass AccidentDataset(Dataset):\n    def __init__(self, k_order, network, node_attr, accident, weather, speed, sf_scaler=None, tf_scaler=None, ef_scaler=None):\n        self.k_order = k_order\n        self.network = network\n        self.nodes = node_attr\n        self.accident = accident\n        self.weather = weather\n        self.speed = speed\n        self.sf_scaler = sf_scaler\n        self.tf_scaler = tf_scaler\n        self.ef_scaler = ef_scaler\n\n    def __getitem__(self, sample_id):\n        # Trích xuất dữ liệu cho 1 mẫu tai nạn\n        _, _, accident_time, node_id, target = self.accident.iloc[sample_id]\n        \n        # Tạo Subgraph\n        neighbors = nx.single_source_shortest_path_length(self.network, node_id, cutoff=self.k_order)\n        neighbors_list = [node_id] + sorted([n for n in neighbors.keys() if n != node_id])\n        \n        sub_graph_nx = nx.subgraph(self.network, neighbors_list)\n        relabel_map = {old: new for new, old in enumerate(neighbors_list)}\n        sub_graph_nx = nx.relabel_nodes(sub_graph_nx, relabel_map)\n        sub_graph_nx.add_edges_from([(v, v) for v in sub_graph_nx.nodes]) # Self-loop\n        g = dgl.from_networkx(sub_graph_nx)\n\n        # Lấy Features\n        date_range = pd.date_range(end=accident_time.strftime(\"%Y%m%d %H\"), freq=\"1H\", periods=24)\n        \n        # Spatial\n        selected_nodes = self.nodes.loc[neighbors_list]\n        spatial_features = np.array(selected_nodes['spatial_features'].tolist())\n\n        # Temporal (Speed) - Mapping Grid ID\n        x_ids = np.floor((selected_nodes['XCoord'].values - longitudeMin) / widthSingle).astype(np.int64)\n        y_ids = np.floor((selected_nodes['YCoord'].values - latitudeMin) / heightSingle).astype(np.int64)\n        grid_keys = [f'{y},{x}' for y, x in zip(y_ids, x_ids)]\n        \n        # Handle missing grids safely\n        temp_list = []\n        for key in grid_keys:\n            if key in self.speed.columns:\n                temp_list.append(self.speed.loc[date_range, key].values)\n            else:\n                temp_list.append(np.zeros(24)) # Default 0 if missing\n        temporal_features = np.array(temp_list)\n\n        # External\n        weather_data = self.weather.loc[date_range[-1]].tolist() if date_range[-1] in self.weather.index else [0]*38\n        external_features = weather_data + [accident_time.month, accident_time.day, accident_time.dayofweek, accident_time.hour, int(accident_time.dayofweek >= 5)]\n\n        # Scaling\n        if self.sf_scaler: spatial_features = (spatial_features - self.sf_scaler[0]) / self.sf_scaler[1]\n        if self.tf_scaler: temporal_features = (temporal_features - self.tf_scaler[0]) / self.tf_scaler[1]\n        if self.ef_scaler: external_features = (external_features - self.ef_scaler[0]) / self.ef_scaler[1]\n\n        # To Tensor\n        # Spatial: (Nodes, Feat)\n        # Temporal: (Nodes, 1, Time)\n        spatial_features = torch.tensor(spatial_features).float()\n        temporal_features = torch.tensor(temporal_features).unsqueeze(1).float()\n        external_features = torch.tensor(external_features).float()\n        target = torch.tensor(target).float()\n\n        return g, spatial_features, temporal_features, external_features, target\n\n    def __len__(self):\n        return len(self.accident)\n\ndef get_data_loaders(config):\n    # Giả định đường dẫn file, bạn cần thay đổi cho đúng môi trường Colab/Kaggle\n    path_prefix = \"/kaggle/input/dstgcn-dataset/\" # Hoặc \"./data/\"\n    try:\n        import pickle\n        with open(path_prefix + 'beijing_roadnet.gpickle', 'rb') as f:\n            network = pickle.load(f)\n        nodes = pd.read_hdf(path_prefix + 'edges_data.h5')\n        weather = pd.read_hdf(path_prefix + 'weather.h5')\n        speed = fill_speed(pd.read_hdf(path_prefix + 'all_grids_speed.h5'))\n        accident_path = path_prefix + 'accident.h5'\n    except Exception as e:\n        print(f\"Error loading data: {e}\")\n        return None\n\n    dls = {}\n    for key in ['train', 'validate', 'test']:\n        accident = pd.read_hdf(accident_path, key=key)\n        # Sử dụng 1 phần dữ liệu để test code nếu cần (vd: accident.iloc[:100])\n        ds = AccidentDataset(\n            config['K_hop'], network, nodes, accident, weather, speed,\n            sf_scaler=(np.array(config['spatial_features_mean']), np.array(config['spatial_features_std'])),\n            tf_scaler=(np.array(config['temporal_features_mean']), np.array(config['temporal_features_std'])),\n            ef_scaler=(np.array(config['external_features_mean']), np.array(config['external_features_std']))\n        )\n        dls[key] = DataLoader(ds, batch_size=config['batch_size'], shuffle=(key=='train'), collate_fn=collate_fn, num_workers=0)\n    return dls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T03:48:51.006516Z","iopub.execute_input":"2025-12-20T03:48:51.006810Z","iopub.status.idle":"2025-12-20T03:48:51.035554Z","shell.execute_reply.started":"2025-12-20T03:48:51.006787Z","shell.execute_reply":"2025-12-20T03:48:51.034809Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def train_madgcn_model(model, data_loaders, optimizer, criterion, epochs):\n    model.to(device)\n    best_rmse = float('inf')\n    \n    for epoch in range(epochs):\n        print(f\"Epoch {epoch+1}/{epochs}\")\n        \n        for phase in ['train', 'validate', 'test']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n                \n            running_loss = 0.0\n            predictions = []\n            targets = []\n            \n            # Sử dụng tqdm nếu chạy interactive\n            # loader = tqdm(data_loaders[phase], desc=phase)\n            loader = data_loaders[phase]\n            \n            for g, s_feat, t_feat, e_feat, y_true in loader:\n                g = g.to(device)\n                s_feat = s_feat.to(device)\n                t_feat = t_feat.to(device)\n                e_feat = e_feat.to(device)\n                y_true = y_true.to(device)\n                \n                optimizer.zero_grad()\n                \n                with torch.set_grad_enabled(phase == 'train'):\n                    output = model(g, s_feat, t_feat, e_feat)\n                    output = output.squeeze()\n                    \n                    loss = criterion(output, y_true)\n                    \n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                \n                running_loss += loss.item() * y_true.size(0)\n                \n                predictions.extend(output.detach().cpu().numpy())\n                targets.extend(y_true.detach().cpu().numpy())\n            \n            epoch_loss = running_loss / len(data_loaders[phase].dataset)\n            predictions = np.array(predictions)\n            targets = np.array(targets)\n            \n            # --- Tính Metrics ---\n            mae = mean_absolute_error(targets, predictions)\n            rmse = np.sqrt(mean_squared_error(targets, predictions))\n            pcc, _ = pearsonr(predictions.flatten(), targets.flatten())\n            \n            print(f\"{phase.upper()} | Loss: {epoch_loss:.4f} | MAE: {mae:.4f} | RMSE: {rmse:.4f} | PCC: {pcc:.4f}\")\n            \n            if phase == 'test' and rmse < best_rmse:\n                best_rmse = rmse\n                # torch.save(model.state_dict(), 'best_madgcn.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T03:48:51.036491Z","iopub.execute_input":"2025-12-20T03:48:51.036728Z","iopub.status.idle":"2025-12-20T03:48:51.045853Z","shell.execute_reply.started":"2025-12-20T03:48:51.036696Z","shell.execute_reply":"2025-12-20T03:48:51.045262Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# 1. Load Data\nprint(\"Loading data...\")\nloaders = get_data_loaders(config)\n\nif loaders:\n    # 2. Khởi tạo Model\n    print(\"Initializing MADGCN Model...\")\n    model = MADGCN(\n        poi_feat_dim=config['poi_features_number'],\n        temp_feat_dim=config['temporal_features_number'], # Usually 1\n        ext_feat_dim=config['external_features_number'],\n        hidden_dim=32,\n        K_hop=config['K_hop']\n    )\n    \n    # 3. Setup Optimizer & Loss\n    # Bài toán dự đoán rủi ro (0-1), có thể dùng MSE hoặc BCELoss\n    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n    criterion = nn.MSELoss() \n    \n    # 4. Train\n    print(\"Start Training...\")\n    train_madgcn_model(model, loaders, optimizer, criterion, epochs=config['epochs'])\nelse:\n    print(\"Cannot create dataloaders. Check data paths.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T03:48:51.046727Z","iopub.execute_input":"2025-12-20T03:48:51.046981Z","iopub.status.idle":"2025-12-20T05:04:26.874261Z","shell.execute_reply.started":"2025-12-20T03:48:51.046962Z","shell.execute_reply":"2025-12-20T05:04:26.873475Z"}},"outputs":[{"name":"stdout","text":"Loading data...\nInitializing MADGCN Model...\nStart Training...\nEpoch 1/50\nTRAIN | Loss: 0.2082 | MAE: 0.4231 | RMSE: 0.4563 | PCC: 0.4107\nVALIDATE | Loss: 0.1984 | MAE: 0.4067 | RMSE: 0.4454 | PCC: 0.4820\nTEST | Loss: 0.1974 | MAE: 0.4076 | RMSE: 0.4443 | PCC: 0.4922\nEpoch 2/50\nTRAIN | Loss: 0.1800 | MAE: 0.3707 | RMSE: 0.4243 | PCC: 0.5308\nVALIDATE | Loss: 0.1951 | MAE: 0.3775 | RMSE: 0.4418 | PCC: 0.5185\nTEST | Loss: 0.1918 | MAE: 0.3745 | RMSE: 0.4379 | PCC: 0.5351\nEpoch 3/50\nTRAIN | Loss: 0.1662 | MAE: 0.3439 | RMSE: 0.4077 | PCC: 0.5807\nVALIDATE | Loss: 0.1823 | MAE: 0.3490 | RMSE: 0.4270 | PCC: 0.5437\nTEST | Loss: 0.1797 | MAE: 0.3460 | RMSE: 0.4239 | PCC: 0.5538\nEpoch 4/50\nTRAIN | Loss: 0.1588 | MAE: 0.3249 | RMSE: 0.3985 | PCC: 0.6045\nVALIDATE | Loss: 0.1543 | MAE: 0.3219 | RMSE: 0.3929 | PCC: 0.6241\nTEST | Loss: 0.1582 | MAE: 0.3267 | RMSE: 0.3977 | PCC: 0.6120\nEpoch 5/50\nTRAIN | Loss: 0.1514 | MAE: 0.3069 | RMSE: 0.3892 | PCC: 0.6282\nVALIDATE | Loss: 0.1627 | MAE: 0.3272 | RMSE: 0.4033 | PCC: 0.6246\nTEST | Loss: 0.1654 | MAE: 0.3303 | RMSE: 0.4067 | PCC: 0.6163\nEpoch 6/50\nTRAIN | Loss: 0.1496 | MAE: 0.2997 | RMSE: 0.3868 | PCC: 0.6337\nVALIDATE | Loss: 0.1438 | MAE: 0.3005 | RMSE: 0.3792 | PCC: 0.6556\nTEST | Loss: 0.1482 | MAE: 0.3055 | RMSE: 0.3849 | PCC: 0.6418\nEpoch 7/50\nTRAIN | Loss: 0.1443 | MAE: 0.2930 | RMSE: 0.3799 | PCC: 0.6504\nVALIDATE | Loss: 0.1473 | MAE: 0.2952 | RMSE: 0.3838 | PCC: 0.6411\nTEST | Loss: 0.1484 | MAE: 0.2968 | RMSE: 0.3853 | PCC: 0.6375\nEpoch 8/50\nTRAIN | Loss: 0.1425 | MAE: 0.2879 | RMSE: 0.3774 | PCC: 0.6560\nVALIDATE | Loss: 0.1483 | MAE: 0.2871 | RMSE: 0.3851 | PCC: 0.6385\nTEST | Loss: 0.1472 | MAE: 0.2864 | RMSE: 0.3836 | PCC: 0.6419\nEpoch 9/50\nTRAIN | Loss: 0.1401 | MAE: 0.2782 | RMSE: 0.3743 | PCC: 0.6631\nVALIDATE | Loss: 0.1392 | MAE: 0.2737 | RMSE: 0.3730 | PCC: 0.6664\nTEST | Loss: 0.1402 | MAE: 0.2764 | RMSE: 0.3744 | PCC: 0.6631\nEpoch 10/50\nTRAIN | Loss: 0.1351 | MAE: 0.2735 | RMSE: 0.3675 | PCC: 0.6781\nVALIDATE | Loss: 0.1345 | MAE: 0.2642 | RMSE: 0.3667 | PCC: 0.6860\nTEST | Loss: 0.1396 | MAE: 0.2730 | RMSE: 0.3736 | PCC: 0.6702\nEpoch 11/50\nTRAIN | Loss: 0.1311 | MAE: 0.2631 | RMSE: 0.3621 | PCC: 0.6897\nVALIDATE | Loss: 0.1375 | MAE: 0.2473 | RMSE: 0.3708 | PCC: 0.6969\nTEST | Loss: 0.1450 | MAE: 0.2565 | RMSE: 0.3807 | PCC: 0.6760\nEpoch 12/50\nTRAIN | Loss: 0.1311 | MAE: 0.2578 | RMSE: 0.3620 | PCC: 0.6898\nVALIDATE | Loss: 0.1428 | MAE: 0.2672 | RMSE: 0.3779 | PCC: 0.6678\nTEST | Loss: 0.1467 | MAE: 0.2735 | RMSE: 0.3830 | PCC: 0.6558\nEpoch 13/50\nTRAIN | Loss: 0.1310 | MAE: 0.2608 | RMSE: 0.3619 | PCC: 0.6900\nVALIDATE | Loss: 0.1392 | MAE: 0.2559 | RMSE: 0.3730 | PCC: 0.6824\nTEST | Loss: 0.1471 | MAE: 0.2647 | RMSE: 0.3836 | PCC: 0.6636\nEpoch 14/50\nTRAIN | Loss: 0.1350 | MAE: 0.2653 | RMSE: 0.3674 | PCC: 0.6784\nVALIDATE | Loss: 0.1326 | MAE: 0.2657 | RMSE: 0.3642 | PCC: 0.7030\nTEST | Loss: 0.1396 | MAE: 0.2728 | RMSE: 0.3736 | PCC: 0.6857\nEpoch 15/50\nTRAIN | Loss: 0.1277 | MAE: 0.2542 | RMSE: 0.3574 | PCC: 0.6993\nVALIDATE | Loss: 0.1267 | MAE: 0.2620 | RMSE: 0.3560 | PCC: 0.7153\nTEST | Loss: 0.1327 | MAE: 0.2687 | RMSE: 0.3643 | PCC: 0.6981\nEpoch 16/50\nTRAIN | Loss: 0.1232 | MAE: 0.2491 | RMSE: 0.3510 | PCC: 0.7122\nVALIDATE | Loss: 0.1202 | MAE: 0.2484 | RMSE: 0.3467 | PCC: 0.7310\nTEST | Loss: 0.1263 | MAE: 0.2547 | RMSE: 0.3554 | PCC: 0.7136\nEpoch 17/50\nTRAIN | Loss: 0.1221 | MAE: 0.2446 | RMSE: 0.3494 | PCC: 0.7154\nVALIDATE | Loss: 0.1154 | MAE: 0.2391 | RMSE: 0.3397 | PCC: 0.7343\nTEST | Loss: 0.1235 | MAE: 0.2490 | RMSE: 0.3514 | PCC: 0.7117\nEpoch 18/50\nTRAIN | Loss: 0.1201 | MAE: 0.2404 | RMSE: 0.3466 | PCC: 0.7208\nVALIDATE | Loss: 0.1171 | MAE: 0.2294 | RMSE: 0.3423 | PCC: 0.7315\nTEST | Loss: 0.1210 | MAE: 0.2346 | RMSE: 0.3479 | PCC: 0.7211\nEpoch 19/50\nTRAIN | Loss: 0.1189 | MAE: 0.2393 | RMSE: 0.3449 | PCC: 0.7241\nVALIDATE | Loss: 0.1070 | MAE: 0.2394 | RMSE: 0.3271 | PCC: 0.7618\nTEST | Loss: 0.1163 | MAE: 0.2514 | RMSE: 0.3410 | PCC: 0.7345\nEpoch 20/50\nTRAIN | Loss: 0.1191 | MAE: 0.2429 | RMSE: 0.3451 | PCC: 0.7237\nVALIDATE | Loss: 0.1309 | MAE: 0.2330 | RMSE: 0.3618 | PCC: 0.7148\nTEST | Loss: 0.1384 | MAE: 0.2432 | RMSE: 0.3720 | PCC: 0.6949\nEpoch 21/50\nTRAIN | Loss: 0.1165 | MAE: 0.2318 | RMSE: 0.3414 | PCC: 0.7307\nVALIDATE | Loss: 0.1118 | MAE: 0.2140 | RMSE: 0.3344 | PCC: 0.7558\nTEST | Loss: 0.1202 | MAE: 0.2251 | RMSE: 0.3468 | PCC: 0.7339\nEpoch 22/50\nTRAIN | Loss: 0.1114 | MAE: 0.2261 | RMSE: 0.3338 | PCC: 0.7446\nVALIDATE | Loss: 0.1042 | MAE: 0.2191 | RMSE: 0.3227 | PCC: 0.7644\nTEST | Loss: 0.1118 | MAE: 0.2269 | RMSE: 0.3343 | PCC: 0.7437\nEpoch 23/50\nTRAIN | Loss: 0.1102 | MAE: 0.2217 | RMSE: 0.3319 | PCC: 0.7479\nVALIDATE | Loss: 0.1096 | MAE: 0.2214 | RMSE: 0.3310 | PCC: 0.7514\nTEST | Loss: 0.1141 | MAE: 0.2246 | RMSE: 0.3377 | PCC: 0.7390\nEpoch 24/50\nTRAIN | Loss: 0.1089 | MAE: 0.2172 | RMSE: 0.3300 | PCC: 0.7513\nVALIDATE | Loss: 0.1040 | MAE: 0.2245 | RMSE: 0.3225 | PCC: 0.7697\nTEST | Loss: 0.1133 | MAE: 0.2351 | RMSE: 0.3366 | PCC: 0.7461\nEpoch 25/50\nTRAIN | Loss: 0.1088 | MAE: 0.2146 | RMSE: 0.3299 | PCC: 0.7515\nVALIDATE | Loss: 0.1090 | MAE: 0.2057 | RMSE: 0.3302 | PCC: 0.7624\nTEST | Loss: 0.1187 | MAE: 0.2184 | RMSE: 0.3446 | PCC: 0.7373\nEpoch 26/50\nTRAIN | Loss: 0.1099 | MAE: 0.2228 | RMSE: 0.3314 | PCC: 0.7488\nVALIDATE | Loss: 0.1210 | MAE: 0.2142 | RMSE: 0.3479 | PCC: 0.7369\nTEST | Loss: 0.1263 | MAE: 0.2211 | RMSE: 0.3553 | PCC: 0.7227\nEpoch 27/50\nTRAIN | Loss: 0.1077 | MAE: 0.2142 | RMSE: 0.3282 | PCC: 0.7544\nVALIDATE | Loss: 0.1113 | MAE: 0.2354 | RMSE: 0.3336 | PCC: 0.7480\nTEST | Loss: 0.1178 | MAE: 0.2415 | RMSE: 0.3432 | PCC: 0.7292\nEpoch 28/50\nTRAIN | Loss: 0.1065 | MAE: 0.2178 | RMSE: 0.3263 | PCC: 0.7579\nVALIDATE | Loss: 0.0993 | MAE: 0.1979 | RMSE: 0.3151 | PCC: 0.7776\nTEST | Loss: 0.1045 | MAE: 0.2035 | RMSE: 0.3233 | PCC: 0.7636\nEpoch 29/50\nTRAIN | Loss: 0.1037 | MAE: 0.2051 | RMSE: 0.3220 | PCC: 0.7650\nVALIDATE | Loss: 0.0895 | MAE: 0.1956 | RMSE: 0.2992 | PCC: 0.8025\nTEST | Loss: 0.1010 | MAE: 0.2084 | RMSE: 0.3178 | PCC: 0.7726\nEpoch 30/50\nTRAIN | Loss: 0.1029 | MAE: 0.2027 | RMSE: 0.3208 | PCC: 0.7670\nVALIDATE | Loss: 0.1010 | MAE: 0.2076 | RMSE: 0.3178 | PCC: 0.7862\nTEST | Loss: 0.1085 | MAE: 0.2152 | RMSE: 0.3293 | PCC: 0.7635\nEpoch 31/50\nTRAIN | Loss: 0.1062 | MAE: 0.2110 | RMSE: 0.3259 | PCC: 0.7584\nVALIDATE | Loss: 0.0902 | MAE: 0.1919 | RMSE: 0.3004 | PCC: 0.8001\nTEST | Loss: 0.1018 | MAE: 0.2028 | RMSE: 0.3190 | PCC: 0.7701\nEpoch 32/50\nTRAIN | Loss: 0.1030 | MAE: 0.2035 | RMSE: 0.3210 | PCC: 0.7668\nVALIDATE | Loss: 0.0889 | MAE: 0.1882 | RMSE: 0.2982 | PCC: 0.8053\nTEST | Loss: 0.0952 | MAE: 0.1944 | RMSE: 0.3085 | PCC: 0.7883\nEpoch 33/50\nTRAIN | Loss: 0.1024 | MAE: 0.2012 | RMSE: 0.3200 | PCC: 0.7685\nVALIDATE | Loss: 0.1135 | MAE: 0.2332 | RMSE: 0.3369 | PCC: 0.7602\nTEST | Loss: 0.1153 | MAE: 0.2370 | RMSE: 0.3395 | PCC: 0.7534\nEpoch 34/50\nTRAIN | Loss: 0.1004 | MAE: 0.2010 | RMSE: 0.3168 | PCC: 0.7736\nVALIDATE | Loss: 0.1058 | MAE: 0.1961 | RMSE: 0.3252 | PCC: 0.7638\nTEST | Loss: 0.1118 | MAE: 0.2054 | RMSE: 0.3343 | PCC: 0.7482\nEpoch 35/50\nTRAIN | Loss: 0.1047 | MAE: 0.2010 | RMSE: 0.3235 | PCC: 0.7628\nVALIDATE | Loss: 0.1030 | MAE: 0.2004 | RMSE: 0.3210 | PCC: 0.7686\nTEST | Loss: 0.1143 | MAE: 0.2148 | RMSE: 0.3381 | PCC: 0.7393\nEpoch 36/50\nTRAIN | Loss: 0.1031 | MAE: 0.2025 | RMSE: 0.3211 | PCC: 0.7666\nVALIDATE | Loss: 0.0965 | MAE: 0.2008 | RMSE: 0.3107 | PCC: 0.7864\nTEST | Loss: 0.1051 | MAE: 0.2121 | RMSE: 0.3242 | PCC: 0.7629\nEpoch 37/50\nTRAIN | Loss: 0.0961 | MAE: 0.1934 | RMSE: 0.3099 | PCC: 0.7847\nVALIDATE | Loss: 0.0883 | MAE: 0.1828 | RMSE: 0.2972 | PCC: 0.8043\nTEST | Loss: 0.0990 | MAE: 0.1953 | RMSE: 0.3147 | PCC: 0.7774\nEpoch 38/50\nTRAIN | Loss: 0.1001 | MAE: 0.1945 | RMSE: 0.3164 | PCC: 0.7744\nVALIDATE | Loss: 0.1084 | MAE: 0.2093 | RMSE: 0.3293 | PCC: 0.7528\nTEST | Loss: 0.1172 | MAE: 0.2184 | RMSE: 0.3424 | PCC: 0.7299\nEpoch 39/50\nTRAIN | Loss: 0.0965 | MAE: 0.1946 | RMSE: 0.3106 | PCC: 0.7838\nVALIDATE | Loss: 0.0906 | MAE: 0.2066 | RMSE: 0.3010 | PCC: 0.8040\nTEST | Loss: 0.1020 | MAE: 0.2201 | RMSE: 0.3193 | PCC: 0.7717\nEpoch 40/50\nTRAIN | Loss: 0.0986 | MAE: 0.1946 | RMSE: 0.3140 | PCC: 0.7782\nVALIDATE | Loss: 0.0927 | MAE: 0.1941 | RMSE: 0.3045 | PCC: 0.7938\nTEST | Loss: 0.1035 | MAE: 0.2084 | RMSE: 0.3217 | PCC: 0.7658\nEpoch 41/50\nTRAIN | Loss: 0.1006 | MAE: 0.1960 | RMSE: 0.3173 | PCC: 0.7730\nVALIDATE | Loss: 0.1053 | MAE: 0.2159 | RMSE: 0.3245 | PCC: 0.7689\nTEST | Loss: 0.1146 | MAE: 0.2273 | RMSE: 0.3385 | PCC: 0.7439\nEpoch 42/50\nTRAIN | Loss: 0.1010 | MAE: 0.2028 | RMSE: 0.3178 | PCC: 0.7720\nVALIDATE | Loss: 0.0881 | MAE: 0.1808 | RMSE: 0.2967 | PCC: 0.8058\nTEST | Loss: 0.1035 | MAE: 0.2001 | RMSE: 0.3217 | PCC: 0.7673\nEpoch 43/50\nTRAIN | Loss: 0.0999 | MAE: 0.1977 | RMSE: 0.3160 | PCC: 0.7749\nVALIDATE | Loss: 0.0872 | MAE: 0.1866 | RMSE: 0.2953 | PCC: 0.8084\nTEST | Loss: 0.0970 | MAE: 0.1979 | RMSE: 0.3114 | PCC: 0.7841\nEpoch 44/50\nTRAIN | Loss: 0.0966 | MAE: 0.1942 | RMSE: 0.3109 | PCC: 0.7832\nVALIDATE | Loss: 0.0849 | MAE: 0.1882 | RMSE: 0.2914 | PCC: 0.8160\nTEST | Loss: 0.0947 | MAE: 0.2004 | RMSE: 0.3078 | PCC: 0.7898\nEpoch 45/50\nTRAIN | Loss: 0.0923 | MAE: 0.1907 | RMSE: 0.3039 | PCC: 0.7943\nVALIDATE | Loss: 0.0928 | MAE: 0.1819 | RMSE: 0.3047 | PCC: 0.7953\nTEST | Loss: 0.1046 | MAE: 0.1955 | RMSE: 0.3234 | PCC: 0.7656\nEpoch 46/50\nTRAIN | Loss: 0.0957 | MAE: 0.1921 | RMSE: 0.3093 | PCC: 0.7857\nVALIDATE | Loss: 0.0939 | MAE: 0.1766 | RMSE: 0.3064 | PCC: 0.7931\nTEST | Loss: 0.1055 | MAE: 0.1905 | RMSE: 0.3247 | PCC: 0.7642\nEpoch 47/50\nTRAIN | Loss: 0.0950 | MAE: 0.1826 | RMSE: 0.3083 | PCC: 0.7875\nVALIDATE | Loss: 0.0947 | MAE: 0.1770 | RMSE: 0.3077 | PCC: 0.7924\nTEST | Loss: 0.1073 | MAE: 0.1888 | RMSE: 0.3276 | PCC: 0.7622\nEpoch 48/50\nTRAIN | Loss: 0.0939 | MAE: 0.1837 | RMSE: 0.3064 | PCC: 0.7903\nVALIDATE | Loss: 0.0896 | MAE: 0.1722 | RMSE: 0.2993 | PCC: 0.8012\nTEST | Loss: 0.1011 | MAE: 0.1831 | RMSE: 0.3180 | PCC: 0.7731\nEpoch 49/50\nTRAIN | Loss: 0.0986 | MAE: 0.1911 | RMSE: 0.3141 | PCC: 0.7783\nVALIDATE | Loss: 0.1037 | MAE: 0.2059 | RMSE: 0.3221 | PCC: 0.7685\nTEST | Loss: 0.1100 | MAE: 0.2122 | RMSE: 0.3317 | PCC: 0.7515\nEpoch 50/50\nTRAIN | Loss: 0.0989 | MAE: 0.1963 | RMSE: 0.3145 | PCC: 0.7774\nVALIDATE | Loss: 0.0995 | MAE: 0.1961 | RMSE: 0.3155 | PCC: 0.7785\nTEST | Loss: 0.1173 | MAE: 0.2123 | RMSE: 0.3425 | PCC: 0.7342\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom scipy.stats import pearsonr\nimport torch\n\ndef evaluate_final_metrics(model, test_loader, device):\n    print(\"--- Bắt đầu đánh giá trên toàn bộ tập Test ---\")\n    \n    model.eval()  # Chuyển model sang chế độ đánh giá (không dropout/grad)\n    predictions = []\n    targets = []\n    \n    # Tắt tính toán gradient để tiết kiệm bộ nhớ và tăng tốc\n    with torch.no_grad():\n        for g, s_feat, t_feat, e_feat, y_true in test_loader:\n            # Chuyển dữ liệu sang thiết bị (GPU/CPU)\n            g = g.to(device)\n            s_feat = s_feat.to(device)\n            t_feat = t_feat.to(device)\n            e_feat = e_feat.to(device)\n            y_true = y_true.to(device)\n            \n            # Dự đoán\n            output = model(g, s_feat, t_feat, e_feat)\n            output = output.squeeze()  # Chuyển shape (B, 1) -> (B,)\n            \n            # Lưu lại kết quả (chuyển về CPU và numpy)\n            predictions.extend(output.cpu().numpy())\n            targets.extend(y_true.cpu().numpy())\n            \n    # Chuyển list thành numpy array\n    predictions = np.array(predictions)\n    targets = np.array(targets)\n    \n    # Tính toán các metrics\n    mae = mean_absolute_error(targets, predictions)\n    rmse = np.sqrt(mean_squared_error(targets, predictions))\n    pcc, p_value = pearsonr(predictions.flatten(), targets.flatten())\n    \n    # Hiển thị kết quả đẹp\n    print(\"\\n\" + \"=\"*40)\n    print(f\"{'KẾT QUẢ ĐÁNH GIÁ TỔNG HỢP (TEST SET)':^40}\")\n    print(\"=\"*40)\n    print(f\"{'Metric':<10} | {'Giá trị':<20}\")\n    print(\"-\" * 40)\n    print(f\"{'MAE':<10} | {mae:.6f}\")\n    print(f\"{'RMSE':<10} | {rmse:.6f}\")\n    print(f\"{'PCC':<10} | {pcc:.6f}\")\n    print(\"=\"*40)\n    \n    return {'mae': mae, 'rmse': rmse, 'pcc': pcc}\n\n# --- THỰC THI ---\n# Kiểm tra xem loaders và model đã tồn tại chưa\nif 'loaders' in globals() and 'model' in globals():\n    final_results = evaluate_final_metrics(model, loaders['test'], device)\nelse:\n    print(\"Lỗi: Bạn cần chạy các cell phía trên để khởi tạo 'loaders' và 'model' trước.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T05:04:46.912989Z","iopub.execute_input":"2025-12-20T05:04:46.913579Z","iopub.status.idle":"2025-12-20T05:05:03.122961Z","shell.execute_reply.started":"2025-12-20T05:04:46.913549Z","shell.execute_reply":"2025-12-20T05:05:03.121539Z"}},"outputs":[{"name":"stdout","text":"--- Bắt đầu đánh giá trên toàn bộ tập Test ---\n\n========================================\n  KẾT QUẢ ĐÁNH GIÁ TỔNG HỢP (TEST SET)  \n========================================\nMetric     | Giá trị             \n----------------------------------------\nMAE        | 0.212302\nRMSE       | 0.342545\nPCC        | 0.734217\n========================================\n","output_type":"stream"}],"execution_count":9}]}