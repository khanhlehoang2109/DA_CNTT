{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14203902,"sourceType":"datasetVersion","datasetId":9059075}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport json\n\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef data_loader(data_path, city, year, level='district', length=12, n_steps=12, is_scale=False, temporal_copy=False, is_realtime=False, train_ratio=0.8):\n    \n    def normalize(train, test):\n        if is_scale:\n            scaler = MinMaxScaler()\n            train_shape, test_shape = train.shape, test.shape\n            train = scaler.fit_transform(train.reshape(-1, train_shape[-1]))\n            test = scaler.transform(test.reshape(-1, test_shape[-1]))\n            return train.reshape(train_shape), test.reshape(test_shape), scaler\n        else:\n            return train, test, None\n\n    risk_data = pd.read_csv(f'{data_path}/risk_scores/{city}-{year}-{level}-hour-risk.csv')\n    selected_areas = risk_data.drop(columns=['date', 'time']).columns\n    n_districts = len(selected_areas) # number of districts\n    n_outputs = len(selected_areas)\n    train_length = int(30 * train_ratio)\n\n    risk_train, y_train = [], []\n    risk_test, y_test = [], []\n    for i in range(length, 721-n_steps):\n        if i <= (train_length * 24): # before date 25th\n            y_train.append(risk_data.drop(columns=['date', 'time']).iloc[i:i+n_steps, :n_outputs].to_numpy())\n            risk_train.append(risk_data.drop(columns=['date', 'time']).iloc[i-length:i, :n_districts].to_numpy())\n        else:\n            y_test.append(risk_data.drop(columns=['date', 'time']).iloc[i:i+n_steps, :n_outputs].to_numpy())\n            risk_test.append(risk_data.drop(columns=['date', 'time']).iloc[i-length:i, :n_districts].to_numpy())\n        \n    risk_train, risk_test, risk_scaler = normalize(np.array(risk_train), np.array(risk_test))\n    y_train, y_test = np.array(y_train), np.array(y_test)\n    y_train_scaled, y_test_scaled, y_scaler = normalize(y_train, y_test)\n\n    # Weather & Air Quality  \n    weather_data = pd.read_csv(f'{data_path}/weather/{city}-{year}-count.csv').fillna(0)\n    if level == 'district':\n        weather_data['location'] = weather_data['location'].apply(lambda x: x.split('|')[0])\n        weather_data = weather_data.groupby(by=['date','time','location'], as_index=False).mean()                \n    weather_train, weather_test = [], []\n\n    location_weather = []\n    for location in selected_areas:\n        location_weather.append(weather_data[weather_data['location'] == location].iloc[:, 3:].to_numpy())\n\n    location_weather = np.concatenate(location_weather, axis=1)\n\n    for i in range(length, 721-n_steps):\n        if i <= (train_length * 24): # before date 25th\n            weather_train.append(location_weather[i-length:i])\n        else:\n            weather_test.append(location_weather[i-length:i])\n    \n    weather_train, weather_test, _ = normalize(np.array(weather_train).reshape(len(weather_train), length, n_districts, -1), np.array(weather_test).reshape(len(weather_test), length, n_districts, -1))\n\n\n    # Dangerous Driving Behavior\n    dtg_data = pd.read_csv(f'{data_path}/dangerous_cases/{city}-{year}-date-hour-{level}-new.csv')\n    dtg_train, dtg_test = [], []\n\n    location_dtg = []\n    for location in selected_areas:\n        if level == 'district':\n            district = location.split('|')[0]\n            location_dtg.append(dtg_data[dtg_data['district'] == district].iloc[:, 3:].to_numpy())\n        else:\n            district, subdistrict = location.split('|')[0], location.split('|')[1]\n            location_dtg.append(dtg_data[(dtg_data['district'] == district) & (dtg_data['subdistrict'] == subdistrict)].iloc[:, 3:].to_numpy())\n\n    location_dtg = np.concatenate(location_dtg, axis=1)\n\n    for i in range(length, 721-n_steps):\n        if i <= (train_length * 24): # before date 25th\n            dtg_train.append(location_dtg[i-length:i])\n        else:\n            dtg_test.append(location_dtg[i-length:i])\n\n    dtg_train, dtg_test, _ = normalize(np.array(dtg_train).reshape(len(dtg_train), length, n_districts, -1), np.array(dtg_test).reshape(len(dtg_test), length, n_districts, -1))\n\n\n    # Road data\n    road_data = pd.read_csv(f'{data_path}/roads/{city}-{year}-{level}-road-count.csv').drop(columns=['attribute'])\n    road_train, road_test = [], []\n\n    location_road = []\n    for location in selected_areas:\n        location_road.append(road_data[location].to_numpy())\n\n    for i in range(length, 721-n_steps):\n        if i <= (train_length * 24): # before date 25th\n            road_train.append(np.array([location_road]*length)) if temporal_copy else road_train.append(np.array(location_road))\n        else:\n            road_test.append(np.array([location_road]*length)) if temporal_copy else road_test.append(np.array(location_road))\n            \n    road_train, road_test, _ = normalize(np.array(road_train), np.array(road_test))\n\n\n    # demographics data\n    demo_data = pd.read_csv(f'{data_path}/demographic/{city}-{year}-{level}.csv').drop(columns=['index'])\n    demo_train, demo_test = [], []\n\n    location_demo = []\n    for location in selected_areas:\n        location_demo.append(demo_data[location].to_numpy())\n\n    for i in range(length, 721-n_steps):\n        if i <= (train_length * 24): # before date 25th\n            demo_train.append(np.array([location_demo]*length)) if temporal_copy else demo_train.append(np.array(location_demo))\n        else:\n            demo_test.append(np.array([location_demo]*length)) if temporal_copy else demo_test.append(np.array(location_demo))\n            \n    demo_train, demo_test, _ = normalize(np.array(demo_train), np.array(demo_test))\n\n\n    # POI data\n    poi_data = pd.read_csv(f'{data_path}/poi/{city}-{year}-{level}.csv').drop(columns=['location'])\n    poi_train, poi_test = [], []\n\n    location_poi = []\n    for location in selected_areas:\n        location_poi.append(poi_data[location].to_numpy())\n\n    for i in range(length, 721-n_steps):\n        if i <= (train_length * 24): # before date 25th\n            poi_train.append(np.array([location_poi]*length)) if temporal_copy else poi_train.append(np.array(location_poi))\n        else:\n            poi_test.append(np.array([location_poi]*length)) if temporal_copy else poi_test.append(np.array(location_poi))\n            \n    poi_train, poi_test, _ = normalize(np.array(poi_train), np.array(poi_test))\n\n\n    # traffic volumes\n    volume_data = pd.read_csv(f'{data_path}/traffic_volume/{city}-{year}.csv').drop(columns=['date', 'hour'])\n    volume_train, volume_test = [], []\n\n    for i in range(length, 721-n_steps):\n        if i <= (train_length * 24): # before date 25th\n            volume_train.append(volume_data.iloc[i-length:i, :n_districts].to_numpy())\n        else:\n            volume_test.append(volume_data.iloc[i-length:i, :n_districts].to_numpy())\n\n    volume_train, volume_test, _ = normalize(np.array(volume_train), np.array(volume_test))\n    \n\n    # traffic speed\n    speed_data = pd.read_csv(f'{data_path}/traffic_speed/{city}-{year}.csv').drop(columns=['date', 'hour'])\n    speed_train, speed_test = [], []\n\n    for i in range(length, 721-n_steps):\n        if i <= (train_length * 24): # before date 25th\n            speed_train.append(speed_data.iloc[i-length:i, :n_districts].to_numpy())\n        else:\n            speed_test.append(speed_data.iloc[i-length:i, :n_districts].to_numpy())\n\n    speed_train, speed_test, _ = normalize(np.array(speed_train), np.array(speed_test))\n    \n\n    # calendar\n    calendar_data = pd.read_csv(f'{data_path}/calendar/calendar-{city}-{year}-{level}.csv')\n    calendar_train, calendar_test = [], []\n    \n    location_calendar = []\n    for location in selected_areas:\n        location_calendar.append(calendar_data[calendar_data['location'] == location].iloc[:, 1:].to_numpy())\n\n    location_calendar = np.concatenate(location_calendar, axis=1)\n\n    for i in range(length, 721-n_steps):\n        if i <= (train_length * 24): # before date 25th\n            calendar_train.append(location_calendar[i:i+n_steps]) if is_realtime else calendar_train.append(location_calendar[i-length:i])\n        else:\n            calendar_test.append(location_calendar[i:i+n_steps]) if is_realtime else calendar_test.append(location_calendar[i-length:i])\n    calendar_train, calendar_test = np.array(calendar_train), np.array(calendar_test)        \n    calendar_train, calendar_test, _ = normalize(calendar_train.reshape(calendar_train.shape[0], calendar_train.shape[1], n_districts, -1), calendar_test.reshape(calendar_test.shape[0], calendar_test.shape[1], n_districts, -1))\n    \n    # Match Shape\n    risk_train = risk_train[:,:,:,None]\n    risk_test = risk_test[:,:,:,None]\n    volume_train = volume_train[:,:,:,None]\n    volume_test = volume_test[:,:,:,None]\n    speed_train = speed_train[:,:,:,None]\n    speed_test = speed_test[:,:,:,None]\n\n    return {\n        'risk': [risk_train, risk_test],\n        'road': [road_train, road_test],\n        'poi': [poi_train, poi_test],\n        'demo': [demo_train, demo_test],\n        'weather': [weather_train, weather_test],\n        'calendar': [calendar_train, calendar_test],\n        'volume': [volume_train, volume_test],\n        'speed': [speed_train, speed_test],\n        'dtg': [dtg_train, dtg_test],\n        'y': [y_train, y_test],\n        'y_scaled': [y_train_scaled, y_test_scaled],\n        'selected_areas': selected_areas,\n        'scaler': risk_scaler\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T05:00:11.150659Z","iopub.execute_input":"2025-12-19T05:00:11.151328Z","iopub.status.idle":"2025-12-19T05:00:12.072220Z","shell.execute_reply.started":"2025-12-19T05:00:11.151290Z","shell.execute_reply":"2025-12-19T05:00:12.071430Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport os\n\n\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.metrics import mean_absolute_percentage_error as mape\nfrom sklearn.metrics import top_k_accuracy_score as top_accuracy\n\ndef compute_error(y_true, y_pred):\n    top_y_true, top_y_pred = y_true[np.nonzero(y_true > 1)], y_pred[np.nonzero(y_true > 1)]\n    unique_idx = tuple(np.unique((np.nonzero(y_true > 0)[0], np.nonzero(y_true > 0)[1]), axis=1))  \n    \n    n_districts = y_true.shape[-1]\n\n    mse_score = mse(np.ravel(y_true), np.ravel(y_pred))\n    mae_score = mae(np.ravel(y_true), np.ravel(y_pred))\n    mape_score = mape(top_y_true, top_y_pred)\n    topk_acc = top_accuracy(np.argmax(y_true[unique_idx]/n_districts, axis=1), y_pred[unique_idx]/n_districts, k=round(n_districts*0.20), labels=[l for l in range(n_districts)])\n    return { 'MAE': mae_score, 'MSE': mse_score, 'MAPE': mape_score, 'ACC': topk_acc }\n\n\ndef stepwise_error(y_true, y_pred, n_steps):    \n    mae_scores, mse_scores, mape_scores, topk_accs = [], [], [], np.zeros(y_true.shape[:-1], dtype=int)\n    topk_recalls = np.where(topk_accs != 0, topk_accs, np.nan)\n        \n    n_districts = y_true.shape[-1]\n    \n    for t in range(n_steps):\n        y_true_t, y_pred_t = y_true[:,t,:], y_pred[:,t,:]\n        top_y_true, top_y_pred = y_true_t[np.nonzero(y_true_t > 1)], y_pred_t[np.nonzero(y_true_t > 1)]\n    \n        mse_scores.append(mse(np.ravel(y_true_t), np.ravel(y_pred_t)))\n        mae_scores.append(mae(np.ravel(y_true_t), np.ravel(y_pred_t)))\n        mape_scores.append(mape(top_y_true, top_y_pred))\n    \n    for i in range(y_true.shape[0]):\n        for t in range(n_steps):\n\n            with tf.device('/cpu:0'):\n                t_true = tf.math.top_k(y_true[i,t,:], k=round(n_districts * 0.20))\n                t_true = set(t_true.indices.numpy()[[di for di, val in enumerate(t_true.values.numpy()) if val > 0]])\n                t_pred = set(tf.math.top_k(y_pred[i,t,:], k=round(n_districts * 0.20)).indices.numpy())\n            \n                if len(t_true) > 0:\n                    topk_recalls[i, t] = len(t_true.intersection(t_pred)) / len(t_true)\n    \n    return { 'MAE': mae_scores, 'MSE': mse_scores, 'MAPE': mape_scores, 'ACC': list(np.nanmean(topk_recalls, axis=0)), 'TOP_ACC': topk_recalls }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T05:00:12.073586Z","iopub.execute_input":"2025-12-19T05:00:12.074082Z","iopub.status.idle":"2025-12-19T05:00:25.680912Z","shell.execute_reply.started":"2025-12-19T05:00:12.074043Z","shell.execute_reply":"2025-12-19T05:00:25.680140Z"}},"outputs":[{"name":"stderr","text":"2025-12-19 05:00:13.754679: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766120413.952334      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766120414.007038      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766120414.487883      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766120414.487918      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766120414.487921      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766120414.487924      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install spektral==1.0.6","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T05:00:25.681830Z","iopub.execute_input":"2025-12-19T05:00:25.682303Z","iopub.status.idle":"2025-12-19T05:00:30.093251Z","shell.execute_reply.started":"2025-12-19T05:00:25.682277Z","shell.execute_reply":"2025-12-19T05:00:30.092328Z"}},"outputs":[{"name":"stdout","text":"Collecting spektral==1.0.6\n  Downloading spektral-1.0.6-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from spektral==1.0.6) (1.5.3)\nRequirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from spektral==1.0.6) (5.4.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from spektral==1.0.6) (3.5)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from spektral==1.0.6) (2.0.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from spektral==1.0.6) (2.2.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from spektral==1.0.6) (2.32.5)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from spektral==1.0.6) (1.6.1)\nRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from spektral==1.0.6) (1.15.3)\nRequirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from spektral==1.0.6) (2.19.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from spektral==1.0.6) (4.67.1)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.1.0->spektral==1.0.6) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.1.0->spektral==1.0.6) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.1.0->spektral==1.0.6) (25.9.23)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.1.0->spektral==1.0.6) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.1.0->spektral==1.0.6) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.1.0->spektral==1.0.6) (18.1.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.1.0->spektral==1.0.6) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.1.0->spektral==1.0.6) (25.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.1.0->spektral==1.0.6) (5.29.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.1.0->spektral==1.0.6) (75.2.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.1.0->spektral==1.0.6) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.1.0->spektral==1.0.6) (3.1.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.1.0->spektral==1.0.6) (4.15.0)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.1.0->spektral==1.0.6) (2.0.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.1.0->spektral==1.0.6) (1.75.1)\nRequirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.1.0->spektral==1.0.6) (2.19.0)\nRequirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.1.0->spektral==1.0.6) (3.10.0)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.1.0->spektral==1.0.6) (3.15.1)\nRequirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.1.0->spektral==1.0.6) (0.5.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->spektral==1.0.6) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->spektral==1.0.6) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->spektral==1.0.6) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->spektral==1.0.6) (2025.11.12)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->spektral==1.0.6) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->spektral==1.0.6) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->spektral==1.0.6) (2025.3)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->spektral==1.0.6) (3.6.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow>=2.1.0->spektral==1.0.6) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=2.1.0->spektral==1.0.6) (14.2.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=2.1.0->spektral==1.0.6) (0.1.0)\nRequirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=2.1.0->spektral==1.0.6) (0.17.0)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.1.0->spektral==1.0.6) (3.9)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.1.0->spektral==1.0.6) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.1.0->spektral==1.0.6) (3.1.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow>=2.1.0->spektral==1.0.6) (3.0.3)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.1.0->spektral==1.0.6) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.1.0->spektral==1.0.6) (2.19.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=2.1.0->spektral==1.0.6) (0.1.2)\nDownloading spektral-1.0.6-py3-none-any.whl (114 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.4/114.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: spektral\nSuccessfully installed spektral-1.0.6\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import tensorflow as tf\n\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\n\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Input, Dense, Activation, MultiHeadAttention, Dropout, RepeatVector, TimeDistributed\nfrom tensorflow.keras.layers import Concatenate, Lambda, Reshape, GRU, BatchNormalization, Dot, Add, Bidirectional\n\nfrom spektral.layers import GCNConv, GlobalAvgPool\n\nes = tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True)\nlr = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001, decay_steps=10000, decay_rate=0.9)\noptimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n\n\nclass TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n    \ndef InterviewAttention(V, H):\n    V_attn = Dense(V.shape[1], activation='relu')(V)\n    V_attn = Dense(V_attn.shape[1], activation='sigmoid')(V_attn)\n    return Dot(axes=-1)([H, V_attn])\n\ndef TemporalAttention(h_units, H, length):\n    H = TransformerBlock(H.shape[-1], length, 2048)(H, training=True)\n    H = Reshape(target_shape=[length, -1])(H)\n    return GRU(h_units)(H)\n\ndef MG_TAR(x_train, y_train, x_val, y_val, configs, length=12, n_steps=6):\n    tf.keras.backend.clear_session()\n    \n    _, _, _, _, _, node_features = x_train\n    _, _, n_districts, n_features = node_features.shape\n    gru_h, gcn_f, fc_h, n_layers, bn, d = configs\n\n    A_S = Input(shape=[n_districts, n_districts]) # spatial closeness\n    A_P = Input(shape=[n_districts, n_districts]) # functional similarity (POI)\n    A_R = Input(shape=[n_districts, n_districts]) # road similarity\n    A_D = Input(shape=[n_districts, n_districts]) # demographic similarity\n    A_T = Input(shape=[length, n_districts, n_districts]) # traffic patterns\n\n    F = Input(shape=[length, n_districts, n_features]) # node features\n    \n    H = [] # H_1 to H_T\n\n    for t in range(length):\n        # slice for each time step t\n        Ft = Lambda(lambda f: f[:,t,:,:])(F)\n        A_Tt = Lambda(lambda a: a[:,t,:,:])(A_T)\n\n        X_S, X_P, X_R, X_D, X_Tt = Ft, Ft, Ft, Ft, Ft # input H_t0 time t layer 0\n        H_S, H_P, H_R, H_D, H_Tt = X_S, X_P, X_R, X_D, X_Tt\n\n        for i in range(n_layers): # using aggregation for each layer as in ST-MGCN ?\n            H_S = GCNConv(gcn_f)([H_S, A_S]) # GCN for Adjacency Matrix\n            if (i + 1) % 2 == 0:\n                H_S = BatchNormalization()(H_S) if bn else H_S\n            H_S = Activation('relu')(H_S)\n\n            H_P = GCNConv(gcn_f)([H_P, A_P]) # GCN for POI Graph\n            if (i + 1) % 2 == 0:\n                H_P = BatchNormalization()(H_P) if bn else H_P\n            H_P = Activation('relu')(H_P)\n\n            H_D = GCNConv(gcn_f)([H_D, A_D])  # GCN for Demographic Graph\n            if (i + 1) % 2 == 0:\n                H_D = BatchNormalization()(H_D) if bn else H_D\n            H_D = Activation('relu')(H_D)\n\n            H_R = GCNConv(gcn_f)([H_R, A_R]) # GCN for Road Graph\n            if (i + 1) % 2 == 0:\n                H_R = BatchNormalization()(H_R) if bn else H_R\n            H_R = Activation('relu')(H_R)   \n\n            H_Tt = GCNConv(gcn_f)([H_Tt, A_Tt]) # GCN for Traffic Patterns\n            if (i + 1) % 2 == 0:\n                H_Tt = BatchNormalization()(H_Tt) if bn else H_Tt\n            H_Tt = Activation('relu')(H_Tt) \n\n            \n        H_S = GCNConv(1, activation='relu')([H_S, A_S]) \n        H_P = GCNConv(1, activation='relu')([H_P, A_P])\n        H_D = GCNConv(1, activation='relu')([H_D, A_D])\n        H_R = GCNConv(1, activation='relu')([H_R, A_R])\n        H_Tt = GCNConv(1, activation='relu')([H_Tt, A_Tt])\n        \n        # summarize each channel (i.e., view) into a scalar \n        z = Concatenate()([GlobalAvgPool()(H_S), GlobalAvgPool()(H_P), GlobalAvgPool()(H_D), GlobalAvgPool()(H_R), GlobalAvgPool()(H_Tt)]) # concatenate it into vector z\n        Ht = Concatenate()([H_S, H_P, H_D, H_R, H_Tt]) # concatenate each view i to Ht\n        H.append(InterviewAttention(z, Ht)) # get scaled Ht\n        \n    H = Concatenate()(H)\n    H = Reshape(target_shape=[length, n_districts, 1])(H)\n    H = Concatenate()([H, F])\n    H = TemporalAttention(gru_h, H, length)\n    \n    H = Dense(fc_h, activation='relu')(H)\n    H = Dropout(0.1)(H)\n    H = Dense(fc_h, activation='relu')(H)\n    H = Dropout(0.1)(H)\n        \n    y = Dense(n_steps * n_districts)(H)\n    y = Reshape([n_steps, n_districts])(y)\n\n    # A_train, A_poi_train, A_demo_train, A_road_train, A_traffic_train, node_features_train\n    model = Model(inputs=[A_S, A_P, A_D, A_R, A_T, F], outputs=y)\n    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(delta=d))\n    model.fit(x_train, y_train, epochs=100, batch_size=32, validation_data=(x_val, y_val), callbacks=[es], verbose=0)\n    return model\n\ndef MG_TAR_V(x_train, y_train, x_val, y_val, configs, length=12, n_steps=6, view='All'):\n    tf.keras.backend.clear_session()\n    \n    _, _, _, _, _, node_features = x_train\n    _, _, n_districts, n_features = node_features.shape\n    gru_h, gcn_f, fc_h, n_layers, bn, d = configs\n\n    A_S = Input(shape=[n_districts, n_districts]) # spatial closeness\n    A_P = Input(shape=[n_districts, n_districts]) # functional similarity (POI)\n    A_R = Input(shape=[n_districts, n_districts]) # road similarity\n    A_D = Input(shape=[n_districts, n_districts]) # demographic similarity\n    A_T = Input(shape=[length, n_districts, n_districts]) # traffic patterns\n\n    F = Input(shape=[length, n_districts, n_features]) # node features\n    \n    H = [] # H_1 to H_T\n\n    for t in range(length):\n        # slice for each time step t\n        Ft = Lambda(lambda f: f[:,t,:,:])(F)\n        A_Tt = Lambda(lambda a: a[:,t,:,:])(A_T)\n\n        X_S, X_P, X_R, X_D, X_Tt = Ft, Ft, Ft, Ft, Ft # input H_t0 time t layer 0\n        H_S, H_P, H_R, H_D, H_Tt = X_S, X_P, X_R, X_D, X_Tt\n\n        for i in range(n_layers): # using aggregation for each layer as in ST-MGCN ?\n            if view == 'All' or view == 'OS':\n                H_S = GCNConv(gcn_f)([H_S, A_S]) # GCN for Adjacency Matrix\n                if (i + 1) % 2 == 0:\n                    H_S = BatchNormalization()(H_S) if bn else H_S\n                H_S = Activation('relu')(H_S)\n                \n            if view == 'All' or (view != 'P' and view != 'OS'):\n                H_P = GCNConv(gcn_f)([H_P, A_P]) # GCN for POI Graph\n                if (i + 1) % 2 == 0:\n                    H_P = BatchNormalization()(H_P) if bn else H_P\n                H_P = Activation('relu')(H_P)\n                \n            if view == 'All' or (view != 'D' and view != 'OS'):\n                H_D = GCNConv(gcn_f)([H_D, A_D])  # GCN for Demographic Graph\n                if (i + 1) % 2 == 0:\n                    H_D = BatchNormalization()(H_D) if bn else H_D\n                H_D = Activation('relu')(H_D)\n            \n            if view == 'All' or (view != 'R' and view != 'OS'):\n                H_R = GCNConv(gcn_f)([H_R, A_R]) # GCN for Road Graph\n                if (i + 1) % 2 == 0:\n                    H_R = BatchNormalization()(H_R) if bn else H_R\n                H_R = Activation('relu')(H_R)   \n                \n            if view == 'All' or (view != 'T' and view != 'OS'):\n                H_Tt = GCNConv(gcn_f)([H_Tt, A_Tt]) # GCN for Traffic Patterns\n                if (i + 1) % 2 == 0:\n                    H_Tt = BatchNormalization()(H_Tt) if bn else H_Tt\n                H_Tt = Activation('relu')(H_Tt) \n\n        \n        if view == 'All':\n            H_S = GCNConv(1, activation='relu')([H_S, A_S]) \n            H_P = GCNConv(1, activation='relu')([H_P, A_P])\n            H_D = GCNConv(1, activation='relu')([H_D, A_D])\n            H_R = GCNConv(1, activation='relu')([H_R, A_R])\n            H_Tt = GCNConv(1, activation='relu')([H_Tt, A_Tt])\n\n            # summarize each channel (i.e., view) into a scalar \n            z = Concatenate()([GlobalAvgPool()(H_S), GlobalAvgPool()(H_P), GlobalAvgPool()(H_D), GlobalAvgPool()(H_R), GlobalAvgPool()(H_Tt)]) # concatenate it into vector z\n            Ht = Concatenate()([H_S, H_P, H_D, H_R, H_Tt]) # concatenate each view i to Ht\n            H.append(InterviewAttention(z, Ht)) # get scaled Ht\n\n        elif view == 'OS':\n            H_S = GCNConv(1, activation='relu')([H_S, A_S]) \n\n            # summarize each channel (i.e., view) into a scalar \n            z = GlobalAvgPool()(H_S) # concatenate it into vector z\n            Ht = H_S # concatenate each view i to Ht\n            H.append(InterviewAttention(z, Ht)) # get scaled Ht\n\n        elif view == 'S':\n            H_P = GCNConv(1, activation='relu')([H_P, A_P])\n            H_D = GCNConv(1, activation='relu')([H_D, A_D])\n            H_R = GCNConv(1, activation='relu')([H_R, A_R])\n            H_Tt = GCNConv(1, activation='relu')([H_Tt, A_Tt])\n\n            # summarize each channel (i.e., view) into a scalar \n            z = Concatenate()([GlobalAvgPool()(H_P), GlobalAvgPool()(H_D), GlobalAvgPool()(H_R), GlobalAvgPool()(H_Tt)]) # concatenate it into vector z\n            Ht = Concatenate()([H_P, H_D, H_R, H_Tt]) # concatenate each view i to Ht\n            H.append(InterviewAttention(z, Ht)) # get scaled Ht\n\n        elif view == 'P':\n            H_S = GCNConv(1, activation='relu')([H_S, A_S]) \n            H_D = GCNConv(1, activation='relu')([H_D, A_D])\n            H_R = GCNConv(1, activation='relu')([H_R, A_R])\n            H_Tt = GCNConv(1, activation='relu')([H_Tt, A_Tt])\n\n            # summarize each channel (i.e., view) into a scalar \n            z = Concatenate()([GlobalAvgPool()(H_S), GlobalAvgPool()(H_D), GlobalAvgPool()(H_R), GlobalAvgPool()(H_Tt)]) # concatenate it into vector z\n            Ht = Concatenate()([H_S, H_D, H_R, H_Tt]) # concatenate each view i to Ht\n            H.append(InterviewAttention(z, Ht)) # get scaled Ht\n\n        elif view == 'R':\n            H_S = GCNConv(1, activation='relu')([H_S, A_S]) \n            H_P = GCNConv(1, activation='relu')([H_P, A_P])\n            H_D = GCNConv(1, activation='relu')([H_D, A_D])\n            H_Tt = GCNConv(1, activation='relu')([H_Tt, A_Tt])\n\n            # summarize each channel (i.e., view) into a scalar \n            z = Concatenate()([GlobalAvgPool()(H_S), GlobalAvgPool()(H_P), GlobalAvgPool()(H_D), GlobalAvgPool()(H_Tt)]) # concatenate it into vector z\n            Ht = Concatenate()([H_S, H_P, H_D, H_Tt]) # concatenate each view i to Ht\n            H.append(InterviewAttention(z, Ht)) # get scaled Ht\n\n        elif view == 'D':\n            H_S = GCNConv(1, activation='relu')([H_S, A_S]) \n            H_P = GCNConv(1, activation='relu')([H_P, A_P])\n            H_R = GCNConv(1, activation='relu')([H_R, A_R])\n            H_Tt = GCNConv(1, activation='relu')([H_Tt, A_Tt])\n\n            # summarize each channel (i.e., view) into a scalar \n            z = Concatenate()([GlobalAvgPool()(H_S), GlobalAvgPool()(H_P), GlobalAvgPool()(H_R), GlobalAvgPool()(H_Tt)]) # concatenate it into vector z\n            Ht = Concatenate()([H_S, H_P, H_R, H_Tt]) # concatenate each view i to Ht\n            H.append(InterviewAttention(z, Ht)) # get scaled Ht\n\n        elif view == 'T':\n            H_S = GCNConv(1, activation='relu')([H_S, A_S]) \n            H_P = GCNConv(1, activation='relu')([H_P, A_P])\n            H_D = GCNConv(1, activation='relu')([H_D, A_D])\n            H_R = GCNConv(1, activation='relu')([H_R, A_R])\n\n            # summarize each channel (i.e., view) into a scalar \n            z = Concatenate()([GlobalAvgPool()(H_S), GlobalAvgPool()(H_P), GlobalAvgPool()(H_D), GlobalAvgPool()(H_R)]) # concatenate it into vector z\n            Ht = Concatenate()([H_S, H_P, H_D, H_R]) # concatenate each view i to Ht\n            H.append(InterviewAttention(z, Ht)) # get scaled Ht\n\n        \n    H = Concatenate()(H)\n    H = Reshape(target_shape=[length, n_districts, 1])(H)\n    H = Concatenate()([H, F])\n    H = TemporalAttention(gru_h, H, length)\n    \n    \n    H = Dense(fc_h, activation='relu')(H)\n    H = Dropout(0.1)(H)\n    H = Dense(fc_h, activation='relu')(H)\n    H = Dropout(0.1)(H)\n        \n    y = Dense(n_steps * n_districts)(H)\n    y = Reshape([n_steps, n_districts])(y)\n\n    # A_train, A_poi_train, A_demo_train, A_road_train, A_traffic_train, node_features_train\n    model = Model(inputs=[A_S, A_P, A_D, A_R, A_T, F], outputs=y)\n    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(delta=d))\n    model.fit(x_train, y_train, epochs=100, batch_size=32, validation_data=(x_val, y_val), callbacks=[es], verbose=0)\n    return model\n\ndef MG_TAR_A(x_train, y_train, x_val, y_val, configs, length=12, n_steps=6, attn='All'):\n    tf.keras.backend.clear_session()\n    \n    _, _, _, _, _, node_features = x_train\n    _, _, n_districts, n_features = node_features.shape\n    gru_h, gcn_f, fc_h, n_layers, bn, d = configs\n\n    A_S = Input(shape=[n_districts, n_districts]) # spatial closeness\n    A_P = Input(shape=[n_districts, n_districts]) # functional similarity (POI)\n    A_R = Input(shape=[n_districts, n_districts]) # road similarity\n    A_D = Input(shape=[n_districts, n_districts]) # demographic similarity\n    A_T = Input(shape=[length, n_districts, n_districts]) # traffic patterns\n\n    F = Input(shape=[length, n_districts, n_features]) # node features\n    \n    H = [] # H_1 to H_T\n\n    for t in range(length):\n        # slice for each time step t\n        Ft = Lambda(lambda f: f[:,t,:,:])(F)\n        A_Tt = Lambda(lambda a: a[:,t,:,:])(A_T)\n\n        X_S, X_P, X_R, X_D, X_Tt = Ft, Ft, Ft, Ft, Ft # input H_t0 time t layer 0\n        H_S, H_P, H_R, H_D, H_Tt = X_S, X_P, X_R, X_D, X_Tt\n\n        for i in range(n_layers): # using aggregation for each layer as in ST-MGCN ?\n            H_S = GCNConv(gcn_f)([H_S, A_S]) # GCN for Adjacency Matrix\n            if (i + 1) % 2 == 0:\n                H_S = BatchNormalization()(H_S) if bn else H_S\n            H_S = Activation('relu')(H_S)\n\n            H_P = GCNConv(gcn_f)([H_P, A_P]) # GCN for POI Graph\n            if (i + 1) % 2 == 0:\n                H_P = BatchNormalization()(H_P) if bn else H_P\n            H_P = Activation('relu')(H_P)\n\n            H_D = GCNConv(gcn_f)([H_D, A_D])  # GCN for Demographic Graph\n            if (i + 1) % 2 == 0:\n                H_D = BatchNormalization()(H_D) if bn else H_D\n            H_D = Activation('relu')(H_D)\n\n            H_R = GCNConv(gcn_f)([H_R, A_R]) # GCN for Road Graph\n            if (i + 1) % 2 == 0:\n                H_R = BatchNormalization()(H_R) if bn else H_R\n            H_R = Activation('relu')(H_R)   \n\n            H_Tt = GCNConv(gcn_f)([H_Tt, A_Tt]) # GCN for Traffic Patterns\n            if (i + 1) % 2 == 0:\n                H_Tt = BatchNormalization()(H_Tt) if bn else H_Tt\n            H_Tt = Activation('relu')(H_Tt) \n\n            \n        H_S = GCNConv(1, activation='relu')([H_S, A_S]) \n        H_P = GCNConv(1, activation='relu')([H_P, A_P])\n        H_D = GCNConv(1, activation='relu')([H_D, A_D])\n        H_R = GCNConv(1, activation='relu')([H_R, A_R])\n        H_Tt = GCNConv(1, activation='relu')([H_Tt, A_Tt])\n        \n        if attn == 'View':\n            z = Concatenate()([GlobalAvgPool()(H_S), GlobalAvgPool()(H_P), GlobalAvgPool()(H_D), GlobalAvgPool()(H_R), GlobalAvgPool()(H_Tt)]) # concatenate it into vector z\n            H.append(z)\n        else:\n            # summarize each channel (i.e., view) into a scalar \n            z = Concatenate()([GlobalAvgPool()(H_S), GlobalAvgPool()(H_P), GlobalAvgPool()(H_D), GlobalAvgPool()(H_R), GlobalAvgPool()(H_Tt)]) # concatenate it into vector z\n            Ht = Concatenate()([H_S, H_P, H_D, H_R, H_Tt]) # concatenate each view i to Ht\n            H.append(InterviewAttention(z, Ht)) # get scaled Ht\n\n    if attn == 'Temp':\n        H = Concatenate()(H)\n    elif attn == 'View':\n        H = Concatenate()(H)\n        H = Reshape(target_shape=[length, 5, 1])(H)\n        H = TemporalAttention(gru_h, H, length)\n    else:\n        H = Concatenate()(H)\n        H = Reshape(target_shape=[length, n_districts, 1])(H)\n        H = Concatenate()([H, F])\n        H = TemporalAttention(gru_h, H, length)\n    \n    \n    H = Dense(fc_h, activation='relu')(H)\n    H = Dropout(0.1)(H)\n    H = Dense(fc_h, activation='relu')(H)\n    H = Dropout(0.1)(H)\n        \n    y = Dense(n_steps * n_districts)(H)\n    y = Reshape([n_steps, n_districts])(y)\n\n    # A_train, A_poi_train, A_demo_train, A_road_train, A_traffic_train, node_features_train\n    model = Model(inputs=[A_S, A_P, A_D, A_R, A_T, F], outputs=y)\n    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(delta=d))\n    model.fit(x_train, y_train, epochs=100, batch_size=32, validation_data=(x_val, y_val), callbacks=[es], verbose=0)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T05:00:30.095299Z","iopub.execute_input":"2025-12-19T05:00:30.095623Z","iopub.status.idle":"2025-12-19T05:00:31.506050Z","shell.execute_reply.started":"2025-12-19T05:00:30.095596Z","shell.execute_reply":"2025-12-19T05:00:31.505195Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1766120431.460211      55 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1766120431.464098      55 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport warnings\nimport logging\nimport itertools\nimport json\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import pearsonr # Cần thêm thư viện này để tính PCC\n\n# Giả định các module này nằm cùng thư mục\nfrom tqdm.notebook import tqdm\n\nlogging.disable(logging.WARNING)\nwarnings.filterwarnings('ignore')\n\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n\n# --- CONFIGURATION ---\ncity = 'Seoul'\nyear = '2016' # '2016' or '2018'\nn_steps, length = 6, 12\nmetric = 'jaccard'\nmodel_configs = {\n    \"2018\": [\n        [256, 256, 512, 4, False, 11], \n        [256, 128, 1024, 4, True, 13], \n        [256, 128, 512, 6, True, 7], \n        [512, 128, 512, 8, True, 13], \n        [256, 128, 512, 8, True, 13]\n    ], \n    \"2016\": [\n        [256, 128, 512, 4, True, 9], \n        [256, 128, 1024, 6, True, 9], \n        [512, 128, 512, 6, True, 7], \n        [256, 128, 512, 8, True, 9], \n        [512, 128, 1024, 8, True, 9]\n    ]\n}\n# --- DATA LOADING ---\ndatasets = data_loader('/kaggle/input/mg-tar', city, year, length=length, n_steps=n_steps, is_scale=True, temporal_copy=True)\nn_districts = len(datasets['selected_areas'])\n\n# Extract Features to their corresponding variables\nrisk_train, risk_test = datasets['risk'][0], datasets['risk'][1]\ndemo_train, demo_test = datasets['demo'][0], datasets['demo'][1]\npoi_train, poi_test = datasets['poi'][0], datasets['poi'][1]\nroad_train, road_test = datasets['road'][0], datasets['road'][1]\nvolume_train, volume_test = datasets['volume'][0], datasets['volume'][1]\nspeed_train, speed_test = datasets['speed'][0], datasets['speed'][1]\nweather_train, weather_test = datasets['weather'][0], datasets['weather'][1]\ncalendar_train, calendar_test = datasets['calendar'][0], datasets['calendar'][1]\nc_train, c_test = datasets['dtg'][0], datasets['dtg'][1]\ny_train, y_test = datasets['y'][0], datasets['y'][1]\n\n# Train - Validation Split\nval_idx = round(risk_train.shape[0] * 0.10) # 10% of Train Set\nrisk_train, risk_val = risk_train[:-val_idx], risk_train[-val_idx:]\ndemo_train, demo_val = demo_train[:-val_idx], demo_train[-val_idx:]\npoi_train, poi_val = poi_train[:-val_idx], poi_train[-val_idx:]\nroad_train, road_val = road_train[:-val_idx], road_train[-val_idx:]\nvolume_train, volume_val = volume_train[:-val_idx], volume_train[-val_idx:]\nspeed_train, speed_val = speed_train[:-val_idx], speed_train[-val_idx:]\nweather_train, weather_val = weather_train[:-val_idx], weather_train[-val_idx:]\ncalendar_train, calendar_val = calendar_train[:-val_idx], calendar_train[-val_idx:]\nc_train, c_val = c_train[:-val_idx], c_train[-val_idx:]\ny_train, y_val = y_train[:-val_idx], y_train[-val_idx:]\n\n# Contextual & Adjacency Matrices\nbase_path = '/kaggle/input/mg-tar/graph_data'\nA = pd.read_csv(f'{base_path}/{city}-normalized-district.csv', engine='c', index_col=0).to_numpy()\nA_poi = pd.read_csv(f'{base_path}/{city}-{year}-poi-normalized-district-{metric}.csv', engine='c', index_col=0).to_numpy()\nA_demo = pd.read_csv(f'{base_path}/{city}-{year}-demo-normalized-district-{metric}.csv', engine='c', index_col=0).to_numpy()\nA_road = pd.read_csv(f'{base_path}/{city}-{year}-road-normalized-district-{metric}.csv', engine='c', index_col=0).to_numpy()\n\n# Tiling matrices for batch processing\nA_train, A_val = np.tile(A, (risk_train.shape[0], 1, 1)), np.tile(A, (risk_val.shape[0], 1, 1))\nA_poi_train, A_poi_val = np.tile(A_poi, (risk_train.shape[0], 1, 1)), np.tile(A_poi, (risk_val.shape[0], 1, 1))\nA_demo_train, A_demo_val = np.tile(A_demo, (risk_train.shape[0], 1, 1)), np.tile(A_demo, (risk_val.shape[0], 1, 1))\nA_road_train, A_road_val = np.tile(A_road, (risk_train.shape[0], 1, 1)), np.tile(A_road, (risk_val.shape[0], 1, 1))\n\nA_test = np.tile(A, (risk_test.shape[0], 1, 1))\nA_poi_test = np.tile(A_poi, (risk_test.shape[0], 1, 1))\nA_demo_test = np.tile(A_demo, (risk_test.shape[0], 1, 1))\nA_road_test = np.tile(A_road, (risk_test.shape[0], 1, 1))\n\nwith open(f'{base_path}/{city}-{year}-traffic-district-normalized-train-{metric}.npy', 'rb') as f:\n    A_traffic_train = np.load(f)\n    A_traffic_train, A_traffic_val = A_traffic_train[:-val_idx], A_traffic_train[-val_idx:]\n\nwith open(f'{base_path}/{city}-{year}-traffic-district-normalized-test-{metric}.npy', 'rb') as f:\n    A_traffic_test = np.load(f)\n\n# Features Aggregation\nnode_features_train = np.concatenate([risk_train, demo_train, poi_train, road_train, volume_train, speed_train, weather_train, calendar_train, c_train], axis=-1)\nnode_features_val = np.concatenate([risk_val, demo_val, poi_val, road_val, volume_val, speed_val, weather_val, calendar_val, c_val], axis=-1)\nnode_features_test = np.concatenate([risk_test, demo_test, poi_test, road_test, volume_test, speed_test, weather_test, calendar_test, c_test], axis=-1)\n\nx_train = [A_train, A_poi_train, A_demo_train, A_road_train, A_traffic_train, node_features_train]\nx_val = [A_val, A_poi_val, A_demo_val, A_road_val, A_traffic_val, node_features_val]\nx_test = [A_test, A_poi_test, A_demo_test, A_road_test, A_traffic_test, node_features_test]\nprint(1)\n# --- MODEL LOADING & PREDICTION ---\nbest_config = model_configs[year][0]\nbest_model = MG_TAR(x_train, y_train, x_val, y_val, best_config)\ny_pred = best_model.predict(x_test)\n\n# --- EVALUATION METRICS (MAE, RMSE, PCC) ---\n\n# Đảm bảo shape phù hợp để tính toán\ny_true_flat = y_test.flatten()\ny_pred_flat = y_pred.flatten()\n\n# 1. MAE (Mean Absolute Error) -> Đổi tên biến thành global_mae\nglobal_mae = np.mean(np.abs(y_true_flat - y_pred_flat))\n\n# 2. RMSE (Root Mean Squared Error) -> Đổi tên biến thành global_rmse\nglobal_rmse = np.sqrt(np.mean((y_true_flat - y_pred_flat) ** 2))\n\n# 3. PCC (Pearson Correlation Coefficient)\npcc_val, _ = pearsonr(y_true_flat, y_pred_flat)\n\nprint(f'\\n=== Model Performance Seoul ({year}) ===')\nprint(f\"MAE : {global_mae:.5f}\")\nprint(f\"RMSE: {global_rmse:.5f}\")\nprint(f\"PCC : {pcc_val:.5f}\")\n\n# Bây giờ chạy hàm stepwise_error sẽ không bị lỗi nữa\nsw_errors = stepwise_error(y_test, y_pred, n_steps)\nprint(f\"Original MSE: {np.average(sw_errors['MSE'])} - Original Acc@K: {np.average(sw_errors['ACC'])}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-19T05:00:31.507153Z","iopub.execute_input":"2025-12-19T05:00:31.507673Z","iopub.status.idle":"2025-12-19T05:05:38.243536Z","shell.execute_reply.started":"2025-12-19T05:00:31.507647Z","shell.execute_reply":"2025-12-19T05:05:38.242739Z"}},"outputs":[{"name":"stdout","text":"1\n","output_type":"stream"},{"name":"stderr","text":"E0000 00:00:1766120566.697039      55 meta_optimizer.cc:967] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inStatefulPartitionedCall/functional_1_1/transformer_block_1/dropout_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\nI0000 00:00:1766120585.582495     130 cuda_dnn.cc:529] Loaded cuDNN version 91002\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step\n\n=== Model Performance Seoul (2016) ===\nMAE : 0.87456\nRMSE: 1.61599\nPCC : 0.16956\nOriginal MSE: 2.611439267794291 - Original Acc@K: 0.32299498746867156\n","output_type":"stream"}],"execution_count":5}]}