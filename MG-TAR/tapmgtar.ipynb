{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14203902,"sourceType":"datasetVersion","datasetId":9059075}],"dockerImageVersionId":31240,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch_geometric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T02:18:44.397894Z","iopub.execute_input":"2025-12-18T02:18:44.398087Z","iopub.status.idle":"2025-12-18T02:18:47.714239Z","shell.execute_reply.started":"2025-12-18T02:18:44.398069Z","shell.execute_reply":"2025-12-18T02:18:47.713302Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch_geometric in /usr/local/lib/python3.11/dist-packages (2.7.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.13.2)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.10.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (7.1.3)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.0.9)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.5)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.6.0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.22.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.10.5)\nRequirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.15.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch_geometric) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch_geometric) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch_geometric) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torch_geometric) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torch_geometric) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torch_geometric) (2024.2.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport json\n\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef data_loader(data_path, city, year, level='district', length=12, n_steps=12, is_scale=False, temporal_copy=False, is_realtime=False, train_ratio=0.8):\n    \n    def normalize(train, test):\n        if is_scale:\n            scaler = MinMaxScaler()\n            train_shape, test_shape = train.shape, test.shape\n            train = scaler.fit_transform(train.reshape(-1, train_shape[-1]))\n            test = scaler.transform(test.reshape(-1, test_shape[-1]))\n            return train.reshape(train_shape), test.reshape(test_shape), scaler\n        else:\n            return train, test, None\n\n    risk_data = pd.read_csv(f'{data_path}/risk_scores/{city}-{year}-{level}-hour-risk.csv')\n    selected_areas = risk_data.drop(columns=['date', 'time']).columns\n    n_districts = len(selected_areas) # number of districts\n    n_outputs = len(selected_areas)\n    train_length = int(30 * train_ratio)\n\n    risk_train, y_train = [], []\n    risk_test, y_test = [], []\n    for i in range(length, 721-n_steps):\n        if i <= (train_length * 24): # before date 25th\n            y_train.append(risk_data.drop(columns=['date', 'time']).iloc[i:i+n_steps, :n_outputs].to_numpy())\n            risk_train.append(risk_data.drop(columns=['date', 'time']).iloc[i-length:i, :n_districts].to_numpy())\n        else:\n            y_test.append(risk_data.drop(columns=['date', 'time']).iloc[i:i+n_steps, :n_outputs].to_numpy())\n            risk_test.append(risk_data.drop(columns=['date', 'time']).iloc[i-length:i, :n_districts].to_numpy())\n        \n    risk_train, risk_test, risk_scaler = normalize(np.array(risk_train), np.array(risk_test))\n    y_train, y_test = np.array(y_train), np.array(y_test)\n    y_train_scaled, y_test_scaled, y_scaler = normalize(y_train, y_test)\n\n    # Weather & Air Quality  \n    weather_data = pd.read_csv(f'{data_path}/weather/{city}-{year}-count.csv').fillna(0)\n    if level == 'district':\n        weather_data['location'] = weather_data['location'].apply(lambda x: x.split('|')[0])\n        weather_data = weather_data.groupby(by=['date','time','location'], as_index=False).mean()                \n    weather_train, weather_test = [], []\n\n    location_weather = []\n    for location in selected_areas:\n        location_weather.append(weather_data[weather_data['location'] == location].iloc[:, 3:].to_numpy())\n\n    location_weather = np.concatenate(location_weather, axis=1)\n\n    for i in range(length, 721-n_steps):\n        if i <= (train_length * 24): # before date 25th\n            weather_train.append(location_weather[i-length:i])\n        else:\n            weather_test.append(location_weather[i-length:i])\n    \n    weather_train, weather_test, _ = normalize(np.array(weather_train).reshape(len(weather_train), length, n_districts, -1), np.array(weather_test).reshape(len(weather_test), length, n_districts, -1))\n\n\n    # Dangerous Driving Behavior\n    dtg_data = pd.read_csv(f'{data_path}/dangerous_cases/{city}-{year}-date-hour-{level}-new.csv')\n    dtg_train, dtg_test = [], []\n\n    location_dtg = []\n    for location in selected_areas:\n        if level == 'district':\n            district = location.split('|')[0]\n            location_dtg.append(dtg_data[dtg_data['district'] == district].iloc[:, 3:].to_numpy())\n        else:\n            district, subdistrict = location.split('|')[0], location.split('|')[1]\n            location_dtg.append(dtg_data[(dtg_data['district'] == district) & (dtg_data['subdistrict'] == subdistrict)].iloc[:, 3:].to_numpy())\n\n    location_dtg = np.concatenate(location_dtg, axis=1)\n\n    for i in range(length, 721-n_steps):\n        if i <= (train_length * 24): # before date 25th\n            dtg_train.append(location_dtg[i-length:i])\n        else:\n            dtg_test.append(location_dtg[i-length:i])\n\n    dtg_train, dtg_test, _ = normalize(np.array(dtg_train).reshape(len(dtg_train), length, n_districts, -1), np.array(dtg_test).reshape(len(dtg_test), length, n_districts, -1))\n\n\n    # Road data\n    road_data = pd.read_csv(f'{data_path}/roads/{city}-{year}-{level}-road-count.csv').drop(columns=['attribute'])\n    road_train, road_test = [], []\n\n    location_road = []\n    for location in selected_areas:\n        location_road.append(road_data[location].to_numpy())\n\n    for i in range(length, 721-n_steps):\n        if i <= (train_length * 24): # before date 25th\n            road_train.append(np.array([location_road]*length)) if temporal_copy else road_train.append(np.array(location_road))\n        else:\n            road_test.append(np.array([location_road]*length)) if temporal_copy else road_test.append(np.array(location_road))\n            \n    road_train, road_test, _ = normalize(np.array(road_train), np.array(road_test))\n\n\n    # demographics data\n    demo_data = pd.read_csv(f'{data_path}/demographic/{city}-{year}-{level}.csv').drop(columns=['index'])\n    demo_train, demo_test = [], []\n\n    location_demo = []\n    for location in selected_areas:\n        location_demo.append(demo_data[location].to_numpy())\n\n    for i in range(length, 721-n_steps):\n        if i <= (train_length * 24): # before date 25th\n            demo_train.append(np.array([location_demo]*length)) if temporal_copy else demo_train.append(np.array(location_demo))\n        else:\n            demo_test.append(np.array([location_demo]*length)) if temporal_copy else demo_test.append(np.array(location_demo))\n            \n    demo_train, demo_test, _ = normalize(np.array(demo_train), np.array(demo_test))\n\n\n    # POI data\n    poi_data = pd.read_csv(f'{data_path}/poi/{city}-{year}-{level}.csv').drop(columns=['location'])\n    poi_train, poi_test = [], []\n\n    location_poi = []\n    for location in selected_areas:\n        location_poi.append(poi_data[location].to_numpy())\n\n    for i in range(length, 721-n_steps):\n        if i <= (train_length * 24): # before date 25th\n            poi_train.append(np.array([location_poi]*length)) if temporal_copy else poi_train.append(np.array(location_poi))\n        else:\n            poi_test.append(np.array([location_poi]*length)) if temporal_copy else poi_test.append(np.array(location_poi))\n            \n    poi_train, poi_test, _ = normalize(np.array(poi_train), np.array(poi_test))\n\n\n    # traffic volumes\n    volume_data = pd.read_csv(f'{data_path}/traffic_volume/{city}-{year}.csv').drop(columns=['date', 'hour'])\n    volume_train, volume_test = [], []\n\n    for i in range(length, 721-n_steps):\n        if i <= (train_length * 24): # before date 25th\n            volume_train.append(volume_data.iloc[i-length:i, :n_districts].to_numpy())\n        else:\n            volume_test.append(volume_data.iloc[i-length:i, :n_districts].to_numpy())\n\n    volume_train, volume_test, _ = normalize(np.array(volume_train), np.array(volume_test))\n    \n\n    # traffic speed\n    speed_data = pd.read_csv(f'{data_path}/traffic_speed/{city}-{year}.csv').drop(columns=['date', 'hour'])\n    speed_train, speed_test = [], []\n\n    for i in range(length, 721-n_steps):\n        if i <= (train_length * 24): # before date 25th\n            speed_train.append(speed_data.iloc[i-length:i, :n_districts].to_numpy())\n        else:\n            speed_test.append(speed_data.iloc[i-length:i, :n_districts].to_numpy())\n\n    speed_train, speed_test, _ = normalize(np.array(speed_train), np.array(speed_test))\n    \n\n    # calendar\n    calendar_data = pd.read_csv(f'{data_path}/calendar/calendar-{city}-{year}-{level}.csv')\n    calendar_train, calendar_test = [], []\n    \n    location_calendar = []\n    for location in selected_areas:\n        location_calendar.append(calendar_data[calendar_data['location'] == location].iloc[:, 1:].to_numpy())\n\n    location_calendar = np.concatenate(location_calendar, axis=1)\n\n    for i in range(length, 721-n_steps):\n        if i <= (train_length * 24): # before date 25th\n            calendar_train.append(location_calendar[i:i+n_steps]) if is_realtime else calendar_train.append(location_calendar[i-length:i])\n        else:\n            calendar_test.append(location_calendar[i:i+n_steps]) if is_realtime else calendar_test.append(location_calendar[i-length:i])\n    calendar_train, calendar_test = np.array(calendar_train), np.array(calendar_test)        \n    calendar_train, calendar_test, _ = normalize(calendar_train.reshape(calendar_train.shape[0], calendar_train.shape[1], n_districts, -1), calendar_test.reshape(calendar_test.shape[0], calendar_test.shape[1], n_districts, -1))\n    \n    # Match Shape\n    risk_train = risk_train[:,:,:,None]\n    risk_test = risk_test[:,:,:,None]\n    volume_train = volume_train[:,:,:,None]\n    volume_test = volume_test[:,:,:,None]\n    speed_train = speed_train[:,:,:,None]\n    speed_test = speed_test[:,:,:,None]\n\n    return {\n        'risk': [risk_train, risk_test],\n        'road': [road_train, road_test],\n        'poi': [poi_train, poi_test],\n        'demo': [demo_train, demo_test],\n        'weather': [weather_train, weather_test],\n        'calendar': [calendar_train, calendar_test],\n        'volume': [volume_train, volume_test],\n        'speed': [speed_train, speed_test],\n        'dtg': [dtg_train, dtg_test],\n        'y': [y_train, y_test],\n        'y_scaled': [y_train_scaled, y_test_scaled],\n        'selected_areas': selected_areas,\n        'scaler': risk_scaler\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T02:18:47.715676Z","iopub.execute_input":"2025-12-18T02:18:47.715852Z","iopub.status.idle":"2025-12-18T02:18:47.745461Z","shell.execute_reply.started":"2025-12-18T02:18:47.715828Z","shell.execute_reply":"2025-12-18T02:18:47.744893Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import os\nimport warnings\nimport logging\nimport json\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch_geometric.data import Data, Batch\nfrom torch_geometric.nn import MessagePassing\nfrom torch.nn import Parameter\nfrom typing import Union, Tuple, Callable\nfrom torch import Tensor\nfrom torch_geometric.typing import OptPairTensor, Adj, OptTensor, Size\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom scipy.stats import pearsonr\n\n# --- SETUP MÔI TRƯỜNG ---\nlogging.disable(logging.WARNING)\nwarnings.filterwarnings('ignore')\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Chọn GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# ==========================================\n# 1. LOAD DATA\n# ==========================================\n# Import data_loader (Giả định file data_loader.py đã tồn tại)\n# try:\n#     from data_loader import data_loader\n# except ImportError:\n#     raise ImportError(\"Không tìm thấy file data_loader.py. Vui lòng đảm bảo file này nằm cùng thư mục.\")\n\ncity = 'Seoul'\nyear = '2016' \nn_steps, length = 6, 12\nmetric = 'jaccard'\nDATA_PATH = '/kaggle/input/mg-tar' # Đường dẫn dữ liệu của bạn\n\nprint(\"Loading Data...\")\n# Lưu ý: Đảm bảo đường dẫn DATA_PATH chính xác tới folder chứa datasets\ndatasets = data_loader(DATA_PATH, city, year, length=length, n_steps=n_steps, is_scale=True, temporal_copy=True)\nn_districts = len(datasets['selected_areas'])\n\n# Extract Features\nrisk_train, risk_test = datasets['risk'][0], datasets['risk'][1]\ndemo_train, demo_test = datasets['demo'][0], datasets['demo'][1]\npoi_train, poi_test = datasets['poi'][0], datasets['poi'][1]\nroad_train, road_test = datasets['road'][0], datasets['road'][1]\nvolume_train, volume_test = datasets['volume'][0], datasets['volume'][1]\nspeed_train, speed_test = datasets['speed'][0], datasets['speed'][1]\nweather_train, weather_test = datasets['weather'][0], datasets['weather'][1]\ncalendar_train, calendar_test = datasets['calendar'][0], datasets['calendar'][1]\nc_train, c_test = datasets['dtg'][0], datasets['dtg'][1]\ny_train, y_test = datasets['y'][0], datasets['y'][1]\n\n# Train - Validation Split\nval_idx = round(risk_train.shape[0] * 0.10)\nrisk_train, risk_val = risk_train[:-val_idx], risk_train[-val_idx:]\ndemo_train, demo_val = demo_train[:-val_idx], demo_train[-val_idx:]\npoi_train, poi_val = poi_train[:-val_idx], poi_train[-val_idx:]\nroad_train, road_val = road_train[:-val_idx], road_train[-val_idx:]\nvolume_train, volume_val = volume_train[:-val_idx], volume_train[-val_idx:]\nspeed_train, speed_val = speed_train[:-val_idx], speed_train[-val_idx:]\nweather_train, weather_val = weather_train[:-val_idx], weather_train[-val_idx:]\ncalendar_train, calendar_val = calendar_train[:-val_idx], calendar_train[-val_idx:]\nc_train, c_val = c_train[:-val_idx], c_train[-val_idx:]\ny_train, y_val = y_train[:-val_idx], y_train[-val_idx:]\n\n# Load Adjacency Matrix\nA = pd.read_csv(f'{DATA_PATH}/graph_data/{city}-normalized-district.csv', engine='c', index_col=0).to_numpy()\n\n# Concatenate Features\nnode_features_train = np.concatenate([risk_train, demo_train, poi_train, road_train, volume_train, speed_train, weather_train, calendar_train, c_train], axis=-1)\nnode_features_val = np.concatenate([risk_val, demo_val, poi_val, road_val, volume_val, speed_val, weather_val, calendar_val, c_val], axis=-1)\nnode_features_test = np.concatenate([risk_test, demo_test, poi_test, road_test, volume_test, speed_test, weather_test, calendar_test, c_test], axis=-1)\n\nprint(f\"Train Shape: {node_features_train.shape}\")\nprint(f\"Val Shape:   {node_features_val.shape}\")\nprint(f\"Test Shape:  {node_features_test.shape}\")\n\n# ==========================================\n# 2. XỬ LÝ GRAPH & DATASET\n# ==========================================\n\ndef get_static_graph_structure(adj_matrix, num_nodes):\n    # 1. Edge Index\n    rows, cols = np.where(adj_matrix > 0)\n    edge_index = torch.tensor(np.array([rows, cols]), dtype=torch.long)\n    num_edges = edge_index.shape[1]\n    \n    # 2. FIX QUAN TRỌNG: Dùng Zeros thay vì Random\n    # Để tránh việc model bị nhiễu bởi tín hiệu ngẫu nhiên\n    component_dir = torch.zeros((num_edges, 2), dtype=torch.float) \n    component_ang = torch.zeros((num_edges, 2), dtype=torch.float)\n    \n    return edge_index, component_dir, component_ang\n\n# Tạo cấu trúc graph tĩnh\nstatic_edge_index, static_dir, static_ang = get_static_graph_structure(A, n_districts)\nstatic_edge_index = static_edge_index.to(device)\nstatic_dir = static_dir.to(device)\nstatic_ang = static_ang.to(device)\n\nclass TrafficDataset(Dataset):\n    def __init__(self, x_data, y_data):\n        self.x = x_data\n        self.y = y_data\n        \n    def __len__(self):\n        return len(self.x)\n    \n    def __getitem__(self, idx):\n        # Flatten Time dimension vào Feature dimension\n        x_sample = self.x[idx] # (Time, Nodes, Feats)\n        x_sample = np.transpose(x_sample, (1, 0, 2)) # (Nodes, Time, Feats)\n        num_nodes = x_sample.shape[0]\n        x_flat = x_sample.reshape(num_nodes, -1) # (Nodes, Input_Dim)\n        \n        # Xử lý Label (Y)\n        y_sample = self.y[idx] # (Horizon, Nodes)\n        y_sample = y_sample.T  # (Nodes, Horizon)\n        \n        x_tensor = torch.tensor(x_flat, dtype=torch.float)\n        y_tensor = torch.tensor(y_sample, dtype=torch.float)\n        \n        return Data(x=x_tensor, y=y_tensor)\n\n# Tạo DataLoader\nbatch_size = 32\ntrain_dataset = TrafficDataset(node_features_train, y_train)\nval_dataset = TrafficDataset(node_features_val, y_val)\ntest_dataset = TrafficDataset(node_features_test, y_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: Batch.from_data_list(x))\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: Batch.from_data_list(x))\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: Batch.from_data_list(x))\n\n# Tính input dimension\ninput_dim = node_features_train.shape[1] * node_features_train.shape[3]\noutput_dim = n_steps # Dự báo 6 bước tương lai\n\nprint(f\"Input Feature Dim (per Node): {input_dim}\")\nprint(f\"Output Prediction Dim (per Node): {output_dim}\")\n\n# ==========================================\n# 3. MODEL DEFINITION\n# ==========================================\n\nclass TRAVELConv(MessagePassing):\n    def __init__(self, in_channels: Union[int, Tuple[int, int]],\n                 out_channels: int, nn: Callable, aggr: str = 'add',\n                 root_weight: bool = True, bias: bool = True, **kwargs):\n        super(TRAVELConv, self).__init__(aggr=aggr, **kwargs)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.nn = nn\n        self.aggr = aggr\n        if isinstance(in_channels, int):\n            in_channels = (in_channels, in_channels)\n        self.in_channels_l = in_channels[0]\n\n        if root_weight:\n            self.root = Parameter(torch.Tensor(in_channels[1], out_channels))\n        else:\n            self.register_parameter('root', None)\n        if bias:\n            self.bias = Parameter(torch.Tensor(out_channels))\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        for layer in self.nn:\n            if hasattr(layer, 'reset_parameters'):\n                layer.reset_parameters()\n        if self.root is not None:\n            nn.init.xavier_uniform_(self.root)\n        if self.bias is not None:\n            nn.init.zeros_(self.bias)\n\n    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,\n                edge_attr: OptTensor = None, size: Size = None) -> Tensor:\n        if isinstance(x, Tensor):\n            x: OptPairTensor = (x, x)\n        out = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=size)\n        x_r = x[1]\n        if x_r is not None and self.root is not None:\n            out += torch.matmul(x_r, self.root)\n        if self.bias is not None:\n            out += self.bias\n        return out\n\n    def message(self, x_i: Tensor, x_j: Tensor, edge_attr: Tensor) -> Tensor:\n        inputs = torch.cat([x_j, edge_attr], dim=1) if edge_attr is not None else x_j\n        return self.nn(inputs)\n\nclass TRAVELNetRegressor(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, num_edges, dropout_p=0.5):\n        super(TRAVELNetRegressor, self).__init__()\n        self.dropout_p = dropout_p\n        convdim = 32 # Tăng capacity\n        \n        # FIX: Node Encoder mạnh hơn có BatchNorm\n        self.node_encoder = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.LeakyReLU(),\n            nn.Linear(hidden_dim, hidden_dim)\n        )\n        \n        # FIX: Edge Encoder (Learnable)\n        # Vì đầu vào là Zeros (shape 2), ta dùng Linear để project nó thành vector ẩn\n        self.edge_encoder_dir = nn.Sequential(nn.Linear(2, hidden_dim), nn.LeakyReLU())\n        self.edge_encoder_ang = nn.Sequential(nn.Linear(2, hidden_dim), nn.LeakyReLU())\n        \n        # Conv Block 1\n        nn1 = nn.Sequential(nn.Linear(hidden_dim + hidden_dim, hidden_dim), nn.LeakyReLU(), nn.Linear(hidden_dim, convdim))\n        self.conv1 = TRAVELConv(hidden_dim, convdim, nn1)\n        \n        nn1_2 = nn.Sequential(nn.Linear(hidden_dim + hidden_dim, hidden_dim), nn.LeakyReLU(), nn.Linear(hidden_dim, convdim))\n        self.conv1_2 = TRAVELConv(hidden_dim, convdim, nn1_2)\n        \n        self.bn1 = nn.BatchNorm1d(convdim*2)\n        \n        # Conv Block 2\n        nn2 = nn.Sequential(nn.Linear(2*convdim + hidden_dim, hidden_dim), nn.LeakyReLU(), nn.Linear(hidden_dim, output_dim))\n        self.conv2 = TRAVELConv(2*convdim, output_dim, nn2)\n        \n        nn2_2 = nn.Sequential(nn.Linear(2*convdim + hidden_dim, hidden_dim), nn.LeakyReLU(), nn.Linear(hidden_dim, output_dim))\n        self.conv2_2 = TRAVELConv(2*convdim, output_dim, nn2_2)\n        \n        # Final Output\n        self.fc = nn.Linear(output_dim*2, output_dim)\n\n    def forward(self, x, edge_index, component_dir, component_ang):\n        # 1. Encode\n        x = self.node_encoder(x)\n        \n        # 2. Learn Edge features\n        edge_attr_dir = self.edge_encoder_dir(component_dir)\n        edge_attr_ang = self.edge_encoder_ang(component_ang)\n        \n        # 3. Layer 1\n        x1 = F.relu(self.conv1(x, edge_index, edge_attr_dir))\n        x2 = F.relu(self.conv1_2(x, edge_index, edge_attr_ang))\n        x_concat = torch.cat((x1, x2), axis=1)\n        \n        if x_concat.shape[0] > 1: # Batch norm requires > 1 sample\n            x_concat = self.bn1(x_concat)\n        x_concat = F.dropout(x_concat, p=self.dropout_p, training=self.training)\n        \n        # 4. Layer 2\n        x1_out = self.conv2(x_concat, edge_index, edge_attr_dir)\n        x2_out = self.conv2_2(x_concat, edge_index, edge_attr_ang)\n        \n        x_final = torch.cat((x1_out, x2_out), axis=1)\n        \n        # 5. Final Projection\n        out = self.fc(x_final)\n        return out\n\n# ==========================================\n# 4. TRAINING LOOP\n# ==========================================\n\n# Khởi tạo Model\nnum_edges = static_edge_index.shape[1]\nmodel = TRAVELNetRegressor(\n    input_dim=input_dim, \n    hidden_dim=64, \n    output_dim=output_dim,\n    num_edges=num_edges\n).to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-4)\ncriterion = nn.MSELoss()\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n\ndef train_epoch(loader):\n    model.train()\n    total_loss = 0\n    for batch in loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        \n        # Tái tạo batch graph structure\n        curr_batch_size = batch.num_graphs\n        edge_index_list = []\n        dir_list = []\n        ang_list = []\n        \n        for i in range(curr_batch_size):\n            edge_index_list.append(static_edge_index + i * n_districts)\n            dir_list.append(static_dir)\n            ang_list.append(static_ang)\n            \n        batch_edge_index = torch.cat(edge_index_list, dim=1)\n        batch_dir = torch.cat(dir_list, dim=0)\n        batch_ang = torch.cat(ang_list, dim=0)\n        \n        out = model(batch.x, batch_edge_index, batch_dir, batch_ang)\n        loss = criterion(out, batch.y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(loader)\n\ndef evaluate_debug(loader, epoch_idx):\n    model.eval()\n    preds = []\n    trues = []\n    with torch.no_grad():\n        for batch in loader:\n            batch = batch.to(device)\n            # Tạo batch edge attributes\n            curr_batch_size = batch.num_graphs\n            edge_index_list = [static_edge_index + i * n_districts for i in range(curr_batch_size)]\n            dir_list = [static_dir for _ in range(curr_batch_size)]\n            ang_list = [static_ang for _ in range(curr_batch_size)]\n            \n            batch_edge_index = torch.cat(edge_index_list, dim=1)\n            batch_dir = torch.cat(dir_list, dim=0)\n            batch_ang = torch.cat(ang_list, dim=0)\n            \n            out = model(batch.x, batch_edge_index, batch_dir, batch_ang)\n            preds.append(out.cpu().numpy())\n            trues.append(batch.y.cpu().numpy())\n            \n    preds = np.concatenate(preds, axis=0)\n    trues = np.concatenate(trues, axis=0)\n    \n    mse = mean_squared_error(trues, preds)\n    mae = mean_absolute_error(trues, preds)\n    rmse = np.sqrt(mse)\n    pcc, _ = pearsonr(trues.flatten(), preds.flatten())\n    \n    # DEBUG: In ra 5 giá trị đầu tiên để xem model đoán cái gì\n    if epoch_idx % 5 == 0:\n        print(f\"\\n[DEBUG Epoch {epoch_idx}]\")\n        print(f\"True (first 5): {trues.flatten()[:5]}\")\n        print(f\"Pred (first 5): {preds.flatten()[:5]}\")\n        print(f\"Pred Mean: {preds.mean():.4f}, True Mean: {trues.mean():.4f}\\n\")\n\n    return mse, mae, rmse, pcc\n\nprint(\"Start Training FIXED TRAVELNet...\")\nbest_val_pcc = -1 \n\nfor epoch in range(1, 51):\n    train_loss = train_epoch(train_loader)\n    val_mse, val_mae, val_rmse, val_pcc = evaluate_debug(val_loader, epoch)\n    \n    # Step LR theo MSE\n    scheduler.step(val_mse)\n    \n    print(f'Epoch {epoch:02d} | Loss: {train_loss:.4f} | MSE: {val_mse:.4f} | MAE: {val_mae:.4f} | RMSE: {val_rmse:.4f} | PCC: {val_pcc:.4f}')\n    \n    # Save model if PCC improves\n    if val_pcc > best_val_pcc:\n        best_val_pcc = val_pcc\n        torch.save(model.state_dict(), 'best_travel_model_fixed.pth')\n\n# ==========================================\n# 5. TEST & METRICS\n# ==========================================\nprint(\"\\nEvaluating on Test Set...\")\ntry:\n    model.load_state_dict(torch.load('best_travel_model_fixed.pth'))\nexcept FileNotFoundError:\n    print(\"Warning: Không tìm thấy file 'best_travel_model_fixed.pth'. Model có thể chưa được lưu nếu PCC không cải thiện.\")\n\nmodel.eval()\n\nall_preds = []\nall_trues = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = batch.to(device)\n        curr_batch_size = batch.num_graphs\n        edge_index_list = [static_edge_index + i * n_districts for i in range(curr_batch_size)]\n        dir_list = [static_dir for _ in range(curr_batch_size)]\n        ang_list = [static_ang for _ in range(curr_batch_size)]\n        \n        batch_edge_index = torch.cat(edge_index_list, dim=1)\n        batch_dir = torch.cat(dir_list, dim=0)\n        batch_ang = torch.cat(ang_list, dim=0)\n        \n        out = model(batch.x, batch_edge_index, batch_dir, batch_ang)\n        all_preds.append(out.cpu().numpy())\n        all_trues.append(batch.y.cpu().numpy())\n\ny_pred_flat = np.concatenate(all_preds, axis=0) \ny_true_flat = np.concatenate(all_trues, axis=0)\n\n# Reshape về (Samples, Horizon, Nodes)\nfinal_preds = y_pred_flat.reshape(len(node_features_test), n_districts, n_steps).transpose(0, 2, 1)\nfinal_trues = y_true_flat.reshape(len(node_features_test), n_districts, n_steps).transpose(0, 2, 1)\n\n# --- TÍNH TOÁN STEP-WISE ERROR ---\nprint(f\"\\nModel Performance Seoul ({year}) - TRAVELNet:\")\nprint(\"-\" * 60)\nprint(f\"{'Horizon':<10} | {'MSE':<10} | {'MAE':<10} | {'RMSE':<10} | {'PCC':<10}\")\nprint(\"-\" * 60)\n\nmetric_lists = {'MSE': [], 'MAE': [], 'RMSE': [], 'PCC': []}\n\nfor i in range(n_steps):\n    y_true_step = final_trues[:, i, :]\n    y_pred_step = final_preds[:, i, :]\n    \n    step_mse = mean_squared_error(y_true_step, y_pred_step)\n    step_mae = mean_absolute_error(y_true_step, y_pred_step)\n    step_rmse = np.sqrt(step_mse)\n    step_pcc, _ = pearsonr(y_true_step.flatten(), y_pred_step.flatten())\n    \n    metric_lists['MSE'].append(step_mse)\n    metric_lists['MAE'].append(step_mae)\n    metric_lists['RMSE'].append(step_rmse)\n    metric_lists['PCC'].append(step_pcc)\n    \n    print(f\"Step {i+1:<5} | {step_mse:.4f}     | {step_mae:.4f}     | {step_rmse:.4f}     | {step_pcc:.4f}\")\n\nprint(\"-\" * 60)\nprint(f\"AVERAGE    | {np.mean(metric_lists['MSE']):.4f}     | {np.mean(metric_lists['MAE']):.4f}     | {np.mean(metric_lists['RMSE']):.4f}     | {np.mean(metric_lists['PCC']):.4f}\")\nprint(\"-\" * 60)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T02:32:49.754839Z","iopub.execute_input":"2025-12-18T02:32:49.755374Z","iopub.status.idle":"2025-12-18T02:33:03.213297Z","shell.execute_reply.started":"2025-12-18T02:32:49.755350Z","shell.execute_reply":"2025-12-18T02:33:03.212578Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading Data...\nTrain Shape: (509, 12, 25, 133)\nVal Shape:   (56, 12, 25, 133)\nTest Shape:  (138, 12, 25, 133)\nInput Feature Dim (per Node): 1596\nOutput Prediction Dim (per Node): 6\nStart Training FIXED TRAVELNet...\nEpoch 01 | Loss: 2.7266 | MSE: 2.8319 | MAE: 0.9237 | RMSE: 1.6828 | PCC: 0.0634\nEpoch 02 | Loss: 2.4128 | MSE: 2.6667 | MAE: 0.8061 | RMSE: 1.6330 | PCC: 0.1177\nEpoch 03 | Loss: 2.3776 | MSE: 2.6561 | MAE: 0.8072 | RMSE: 1.6297 | PCC: 0.1287\nEpoch 04 | Loss: 2.3610 | MSE: 2.6667 | MAE: 0.7909 | RMSE: 1.6330 | PCC: 0.1304\n\n[DEBUG Epoch 5]\nTrue (first 5): [2. 0. 0. 0. 2.]\nPred (first 5): [1.1539905  0.88912976 1.0600554  0.97054684 1.1605597 ]\nPred Mean: 0.4110, True Mean: 0.5899\n\nEpoch 05 | Loss: 2.3557 | MSE: 2.6267 | MAE: 0.8431 | RMSE: 1.6207 | PCC: 0.1340\nEpoch 06 | Loss: 2.3487 | MSE: 2.6776 | MAE: 0.7781 | RMSE: 1.6364 | PCC: 0.1347\nEpoch 07 | Loss: 2.3452 | MSE: 2.6286 | MAE: 0.8352 | RMSE: 1.6213 | PCC: 0.1367\nEpoch 08 | Loss: 2.3427 | MSE: 2.5965 | MAE: 0.9088 | RMSE: 1.6114 | PCC: 0.1398\nEpoch 09 | Loss: 2.3373 | MSE: 2.5989 | MAE: 0.9595 | RMSE: 1.6121 | PCC: 0.1313\n\n[DEBUG Epoch 10]\nTrue (first 5): [2. 0. 0. 0. 2.]\nPred (first 5): [1.5866398 1.5131993 1.476311  1.5446479 1.5726161]\nPred Mean: 0.7442, True Mean: 0.5899\n\nEpoch 10 | Loss: 2.3353 | MSE: 2.6540 | MAE: 1.0538 | RMSE: 1.6291 | PCC: 0.1193\nEpoch 11 | Loss: 2.3324 | MSE: 2.5952 | MAE: 0.8936 | RMSE: 1.6110 | PCC: 0.1453\nEpoch 12 | Loss: 2.3346 | MSE: 2.5995 | MAE: 0.9382 | RMSE: 1.6123 | PCC: 0.1397\nEpoch 13 | Loss: 2.3276 | MSE: 2.6091 | MAE: 0.9926 | RMSE: 1.6153 | PCC: 0.1306\nEpoch 14 | Loss: 2.3266 | MSE: 2.7190 | MAE: 1.1237 | RMSE: 1.6489 | PCC: 0.1286\n\n[DEBUG Epoch 15]\nTrue (first 5): [2. 0. 0. 0. 2.]\nPred (first 5): [1.5906923 1.3925321 1.4845238 1.6998287 1.6272014]\nPred Mean: 0.7091, True Mean: 0.5899\n\nEpoch 15 | Loss: 2.3241 | MSE: 2.6569 | MAE: 1.0188 | RMSE: 1.6300 | PCC: 0.1385\nEpoch 16 | Loss: 2.3159 | MSE: 2.6281 | MAE: 1.0116 | RMSE: 1.6211 | PCC: 0.1329\nEpoch 17 | Loss: 2.3097 | MSE: 2.6328 | MAE: 1.0187 | RMSE: 1.6226 | PCC: 0.1283\nEpoch 18 | Loss: 2.3214 | MSE: 2.6921 | MAE: 1.0752 | RMSE: 1.6407 | PCC: 0.1256\nEpoch 19 | Loss: 2.3211 | MSE: 2.6096 | MAE: 0.9060 | RMSE: 1.6154 | PCC: 0.1401\n\n[DEBUG Epoch 20]\nTrue (first 5): [2. 0. 0. 0. 2.]\nPred (first 5): [1.0654113 1.1403854 1.110748  1.1103191 1.0897007]\nPred Mean: 0.5924, True Mean: 0.5899\n\nEpoch 20 | Loss: 2.3052 | MSE: 2.6064 | MAE: 0.9477 | RMSE: 1.6144 | PCC: 0.1406\nEpoch 21 | Loss: 2.3010 | MSE: 2.6516 | MAE: 1.0189 | RMSE: 1.6284 | PCC: 0.1329\nEpoch 22 | Loss: 2.2990 | MSE: 2.6216 | MAE: 0.9420 | RMSE: 1.6191 | PCC: 0.1320\nEpoch 23 | Loss: 2.2985 | MSE: 2.6480 | MAE: 0.9779 | RMSE: 1.6273 | PCC: 0.1231\nEpoch 24 | Loss: 2.2915 | MSE: 2.6893 | MAE: 1.0589 | RMSE: 1.6399 | PCC: 0.1332\n\n[DEBUG Epoch 25]\nTrue (first 5): [2. 0. 0. 0. 2.]\nPred (first 5): [1.1942343 1.2469572 1.2472178 1.2955561 1.2619224]\nPred Mean: 0.6721, True Mean: 0.5899\n\nEpoch 25 | Loss: 2.2917 | MSE: 2.6431 | MAE: 0.9940 | RMSE: 1.6258 | PCC: 0.1389\nEpoch 26 | Loss: 2.2878 | MSE: 2.6532 | MAE: 0.9960 | RMSE: 1.6289 | PCC: 0.1336\nEpoch 27 | Loss: 2.2906 | MSE: 2.6839 | MAE: 1.0017 | RMSE: 1.6383 | PCC: 0.1214\nEpoch 28 | Loss: 2.2897 | MSE: 2.7369 | MAE: 1.0622 | RMSE: 1.6543 | PCC: 0.1266\nEpoch 29 | Loss: 2.2789 | MSE: 2.6833 | MAE: 1.0112 | RMSE: 1.6381 | PCC: 0.1279\n\n[DEBUG Epoch 30]\nTrue (first 5): [2. 0. 0. 0. 2.]\nPred (first 5): [1.31389   1.4026734 1.392777  1.4159856 1.3642253]\nPred Mean: 0.7174, True Mean: 0.5899\n\nEpoch 30 | Loss: 2.2816 | MSE: 2.6840 | MAE: 1.0228 | RMSE: 1.6383 | PCC: 0.1310\nEpoch 31 | Loss: 2.2861 | MSE: 2.7077 | MAE: 1.0497 | RMSE: 1.6455 | PCC: 0.1309\nEpoch 32 | Loss: 2.2814 | MSE: 2.6953 | MAE: 1.0304 | RMSE: 1.6417 | PCC: 0.1325\nEpoch 33 | Loss: 2.2791 | MSE: 2.6976 | MAE: 1.0227 | RMSE: 1.6425 | PCC: 0.1332\nEpoch 34 | Loss: 2.2782 | MSE: 2.6895 | MAE: 1.0123 | RMSE: 1.6400 | PCC: 0.1304\n\n[DEBUG Epoch 35]\nTrue (first 5): [2. 0. 0. 0. 2.]\nPred (first 5): [1.1984237 1.2692758 1.2524781 1.3006835 1.2747478]\nPred Mean: 0.7351, True Mean: 0.5899\n\nEpoch 35 | Loss: 2.2816 | MSE: 2.7021 | MAE: 1.0335 | RMSE: 1.6438 | PCC: 0.1287\nEpoch 36 | Loss: 2.2742 | MSE: 2.7279 | MAE: 1.0506 | RMSE: 1.6516 | PCC: 0.1320\nEpoch 37 | Loss: 2.2762 | MSE: 2.7175 | MAE: 1.0314 | RMSE: 1.6485 | PCC: 0.1298\nEpoch 38 | Loss: 2.2750 | MSE: 2.7065 | MAE: 1.0233 | RMSE: 1.6451 | PCC: 0.1282\nEpoch 39 | Loss: 2.2769 | MSE: 2.7354 | MAE: 1.0486 | RMSE: 1.6539 | PCC: 0.1302\n\n[DEBUG Epoch 40]\nTrue (first 5): [2. 0. 0. 0. 2.]\nPred (first 5): [1.2591482 1.3665812 1.3530585 1.3684162 1.3241206]\nPred Mean: 0.7424, True Mean: 0.5899\n\nEpoch 40 | Loss: 2.2751 | MSE: 2.7187 | MAE: 1.0367 | RMSE: 1.6488 | PCC: 0.1297\nEpoch 41 | Loss: 2.2754 | MSE: 2.7337 | MAE: 1.0485 | RMSE: 1.6534 | PCC: 0.1303\nEpoch 42 | Loss: 2.2769 | MSE: 2.7150 | MAE: 1.0338 | RMSE: 1.6477 | PCC: 0.1305\nEpoch 43 | Loss: 2.2721 | MSE: 2.7251 | MAE: 1.0391 | RMSE: 1.6508 | PCC: 0.1288\nEpoch 44 | Loss: 2.2703 | MSE: 2.7312 | MAE: 1.0427 | RMSE: 1.6526 | PCC: 0.1300\n\n[DEBUG Epoch 45]\nTrue (first 5): [2. 0. 0. 0. 2.]\nPred (first 5): [1.312577  1.433272  1.4243765 1.4270139 1.3718076]\nPred Mean: 0.7525, True Mean: 0.5899\n\nEpoch 45 | Loss: 2.2745 | MSE: 2.7314 | MAE: 1.0427 | RMSE: 1.6527 | PCC: 0.1298\nEpoch 46 | Loss: 2.2694 | MSE: 2.7346 | MAE: 1.0465 | RMSE: 1.6537 | PCC: 0.1303\nEpoch 47 | Loss: 2.2734 | MSE: 2.7311 | MAE: 1.0436 | RMSE: 1.6526 | PCC: 0.1294\nEpoch 48 | Loss: 2.2738 | MSE: 2.7247 | MAE: 1.0393 | RMSE: 1.6507 | PCC: 0.1300\nEpoch 49 | Loss: 2.2702 | MSE: 2.7218 | MAE: 1.0382 | RMSE: 1.6498 | PCC: 0.1303\n\n[DEBUG Epoch 50]\nTrue (first 5): [2. 0. 0. 0. 2.]\nPred (first 5): [1.2826848 1.3825223 1.3790263 1.3925151 1.3426034]\nPred Mean: 0.7460, True Mean: 0.5899\n\nEpoch 50 | Loss: 2.2741 | MSE: 2.7250 | MAE: 1.0386 | RMSE: 1.6508 | PCC: 0.1303\n\nEvaluating on Test Set...\n\nModel Performance Seoul (2016) - TRAVELNet:\n------------------------------------------------------------\nHorizon    | MSE        | MAE        | RMSE       | PCC       \n------------------------------------------------------------\nStep 1     | 2.6082     | 0.8335     | 1.6150     | 0.1713\nStep 2     | 2.6088     | 0.8281     | 1.6152     | 0.1699\nStep 3     | 2.6298     | 0.8205     | 1.6217     | 0.1716\nStep 4     | 2.6308     | 0.8400     | 1.6220     | 0.1682\nStep 5     | 2.6627     | 0.8123     | 1.6318     | 0.1658\nStep 6     | 2.6790     | 0.8098     | 1.6368     | 0.1761\n------------------------------------------------------------\nAVERAGE    | 2.6365     | 0.8240     | 1.6237     | 0.1705\n------------------------------------------------------------\n","output_type":"stream"}],"execution_count":14}]}