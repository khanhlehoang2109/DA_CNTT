{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14203902,"sourceType":"datasetVersion","datasetId":9059075}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport json\n\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef data_loader(data_path, city, year, level='district', length=12, n_steps=12, is_scale=False, temporal_copy=False, is_realtime=False, train_ratio=0.8):\n    \n    def normalize(train, test):\n        if is_scale:\n            scaler = MinMaxScaler()\n            train_shape, test_shape = train.shape, test.shape\n            train = scaler.fit_transform(train.reshape(-1, train_shape[-1]))\n            test = scaler.transform(test.reshape(-1, test_shape[-1]))\n            return train.reshape(train_shape), test.reshape(test_shape), scaler\n        else:\n            return train, test, None\n\n    risk_data = pd.read_csv(f'{data_path}/risk_scores/{city}-{year}-{level}-hour-risk.csv')\n    selected_areas = risk_data.drop(columns=['date', 'time']).columns\n    n_districts = len(selected_areas) # number of districts\n    n_outputs = len(selected_areas)\n    train_length = int(30 * train_ratio)\n\n    risk_train, y_train = [], []\n    risk_test, y_test = [], []\n    for i in range(length, 721-n_steps):\n        if i <= (train_length * 24): # before date 25th\n            y_train.append(risk_data.drop(columns=['date', 'time']).iloc[i:i+n_steps, :n_outputs].to_numpy())\n            risk_train.append(risk_data.drop(columns=['date', 'time']).iloc[i-length:i, :n_districts].to_numpy())\n        else:\n            y_test.append(risk_data.drop(columns=['date', 'time']).iloc[i:i+n_steps, :n_outputs].to_numpy())\n            risk_test.append(risk_data.drop(columns=['date', 'time']).iloc[i-length:i, :n_districts].to_numpy())\n        \n    risk_train, risk_test, risk_scaler = normalize(np.array(risk_train), np.array(risk_test))\n    y_train, y_test = np.array(y_train), np.array(y_test)\n    y_train_scaled, y_test_scaled, y_scaler = normalize(y_train, y_test)\n\n    # Weather & Air Quality  \n    weather_data = pd.read_csv(f'{data_path}/weather/{city}-{year}-count.csv').fillna(0)\n    if level == 'district':\n        weather_data['location'] = weather_data['location'].apply(lambda x: x.split('|')[0])\n        weather_data = weather_data.groupby(by=['date','time','location'], as_index=False).mean()                \n    weather_train, weather_test = [], []\n\n    location_weather = []\n    for location in selected_areas:\n        location_weather.append(weather_data[weather_data['location'] == location].iloc[:, 3:].to_numpy())\n\n    location_weather = np.concatenate(location_weather, axis=1)\n\n    for i in range(length, 721-n_steps):\n        if i <= (train_length * 24): # before date 25th\n            weather_train.append(location_weather[i-length:i])\n        else:\n            weather_test.append(location_weather[i-length:i])\n    \n    weather_train, weather_test, _ = normalize(np.array(weather_train).reshape(len(weather_train), length, n_districts, -1), np.array(weather_test).reshape(len(weather_test), length, n_districts, -1))\n\n\n    # Dangerous Driving Behavior\n    dtg_data = pd.read_csv(f'{data_path}/dangerous_cases/{city}-{year}-date-hour-{level}-new.csv')\n    dtg_train, dtg_test = [], []\n\n    location_dtg = []\n    for location in selected_areas:\n        if level == 'district':\n            district = location.split('|')[0]\n            location_dtg.append(dtg_data[dtg_data['district'] == district].iloc[:, 3:].to_numpy())\n        else:\n            district, subdistrict = location.split('|')[0], location.split('|')[1]\n            location_dtg.append(dtg_data[(dtg_data['district'] == district) & (dtg_data['subdistrict'] == subdistrict)].iloc[:, 3:].to_numpy())\n\n    location_dtg = np.concatenate(location_dtg, axis=1)\n\n    for i in range(length, 721-n_steps):\n        if i <= (train_length * 24): # before date 25th\n            dtg_train.append(location_dtg[i-length:i])\n        else:\n            dtg_test.append(location_dtg[i-length:i])\n\n    dtg_train, dtg_test, _ = normalize(np.array(dtg_train).reshape(len(dtg_train), length, n_districts, -1), np.array(dtg_test).reshape(len(dtg_test), length, n_districts, -1))\n\n\n    # Road data\n    road_data = pd.read_csv(f'{data_path}/roads/{city}-{year}-{level}-road-count.csv').drop(columns=['attribute'])\n    road_train, road_test = [], []\n\n    location_road = []\n    for location in selected_areas:\n        location_road.append(road_data[location].to_numpy())\n\n    for i in range(length, 721-n_steps):\n        if i <= (train_length * 24): # before date 25th\n            road_train.append(np.array([location_road]*length)) if temporal_copy else road_train.append(np.array(location_road))\n        else:\n            road_test.append(np.array([location_road]*length)) if temporal_copy else road_test.append(np.array(location_road))\n            \n    road_train, road_test, _ = normalize(np.array(road_train), np.array(road_test))\n\n\n    # demographics data\n    demo_data = pd.read_csv(f'{data_path}/demographic/{city}-{year}-{level}.csv').drop(columns=['index'])\n    demo_train, demo_test = [], []\n\n    location_demo = []\n    for location in selected_areas:\n        location_demo.append(demo_data[location].to_numpy())\n\n    for i in range(length, 721-n_steps):\n        if i <= (train_length * 24): # before date 25th\n            demo_train.append(np.array([location_demo]*length)) if temporal_copy else demo_train.append(np.array(location_demo))\n        else:\n            demo_test.append(np.array([location_demo]*length)) if temporal_copy else demo_test.append(np.array(location_demo))\n            \n    demo_train, demo_test, _ = normalize(np.array(demo_train), np.array(demo_test))\n\n\n    # POI data\n    poi_data = pd.read_csv(f'{data_path}/poi/{city}-{year}-{level}.csv').drop(columns=['location'])\n    poi_train, poi_test = [], []\n\n    location_poi = []\n    for location in selected_areas:\n        location_poi.append(poi_data[location].to_numpy())\n\n    for i in range(length, 721-n_steps):\n        if i <= (train_length * 24): # before date 25th\n            poi_train.append(np.array([location_poi]*length)) if temporal_copy else poi_train.append(np.array(location_poi))\n        else:\n            poi_test.append(np.array([location_poi]*length)) if temporal_copy else poi_test.append(np.array(location_poi))\n            \n    poi_train, poi_test, _ = normalize(np.array(poi_train), np.array(poi_test))\n\n\n    # traffic volumes\n    volume_data = pd.read_csv(f'{data_path}/traffic_volume/{city}-{year}.csv').drop(columns=['date', 'hour'])\n    volume_train, volume_test = [], []\n\n    for i in range(length, 721-n_steps):\n        if i <= (train_length * 24): # before date 25th\n            volume_train.append(volume_data.iloc[i-length:i, :n_districts].to_numpy())\n        else:\n            volume_test.append(volume_data.iloc[i-length:i, :n_districts].to_numpy())\n\n    volume_train, volume_test, _ = normalize(np.array(volume_train), np.array(volume_test))\n    \n\n    # traffic speed\n    speed_data = pd.read_csv(f'{data_path}/traffic_speed/{city}-{year}.csv').drop(columns=['date', 'hour'])\n    speed_train, speed_test = [], []\n\n    for i in range(length, 721-n_steps):\n        if i <= (train_length * 24): # before date 25th\n            speed_train.append(speed_data.iloc[i-length:i, :n_districts].to_numpy())\n        else:\n            speed_test.append(speed_data.iloc[i-length:i, :n_districts].to_numpy())\n\n    speed_train, speed_test, _ = normalize(np.array(speed_train), np.array(speed_test))\n    \n\n    # calendar\n    calendar_data = pd.read_csv(f'{data_path}/calendar/calendar-{city}-{year}-{level}.csv')\n    calendar_train, calendar_test = [], []\n    \n    location_calendar = []\n    for location in selected_areas:\n        location_calendar.append(calendar_data[calendar_data['location'] == location].iloc[:, 1:].to_numpy())\n\n    location_calendar = np.concatenate(location_calendar, axis=1)\n\n    for i in range(length, 721-n_steps):\n        if i <= (train_length * 24): # before date 25th\n            calendar_train.append(location_calendar[i:i+n_steps]) if is_realtime else calendar_train.append(location_calendar[i-length:i])\n        else:\n            calendar_test.append(location_calendar[i:i+n_steps]) if is_realtime else calendar_test.append(location_calendar[i-length:i])\n    calendar_train, calendar_test = np.array(calendar_train), np.array(calendar_test)        \n    calendar_train, calendar_test, _ = normalize(calendar_train.reshape(calendar_train.shape[0], calendar_train.shape[1], n_districts, -1), calendar_test.reshape(calendar_test.shape[0], calendar_test.shape[1], n_districts, -1))\n    \n    # Match Shape\n    risk_train = risk_train[:,:,:,None]\n    risk_test = risk_test[:,:,:,None]\n    volume_train = volume_train[:,:,:,None]\n    volume_test = volume_test[:,:,:,None]\n    speed_train = speed_train[:,:,:,None]\n    speed_test = speed_test[:,:,:,None]\n\n    return {\n        'risk': [risk_train, risk_test],\n        'road': [road_train, road_test],\n        'poi': [poi_train, poi_test],\n        'demo': [demo_train, demo_test],\n        'weather': [weather_train, weather_test],\n        'calendar': [calendar_train, calendar_test],\n        'volume': [volume_train, volume_test],\n        'speed': [speed_train, speed_test],\n        'dtg': [dtg_train, dtg_test],\n        'y': [y_train, y_test],\n        'y_scaled': [y_train_scaled, y_test_scaled],\n        'selected_areas': selected_areas,\n        'scaler': risk_scaler\n    }","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-19T18:56:42.180173Z","iopub.execute_input":"2025-12-19T18:56:42.180854Z","iopub.status.idle":"2025-12-19T18:56:42.211131Z","shell.execute_reply.started":"2025-12-19T18:56:42.180821Z","shell.execute_reply":"2025-12-19T18:56:42.210300Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\n\ndef metric(pred, real):\n    mae = np.mean(np.abs(pred - real))\n    rmse = np.sqrt(np.mean(np.square(pred - real)))\n    \n    # PCC Calculation\n    pred_flat = pred.flatten()\n    real_flat = real.flatten()\n    if len(pred_flat) > 0 and len(real_flat) > 0:\n        pcc = np.corrcoef(pred_flat, real_flat)[0, 1]\n    else:\n        pcc = 0\n    return mae, rmse, pcc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T18:56:42.212312Z","iopub.execute_input":"2025-12-19T18:56:42.212568Z","iopub.status.idle":"2025-12-19T18:56:42.224724Z","shell.execute_reply.started":"2025-12-19T18:56:42.212536Z","shell.execute_reply":"2025-12-19T18:56:42.223921Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, Model, backend as K\n\nclass STE_Layer(layers.Layer):\n    \"\"\"\n    3. Lớp nhúng không gian - thời gian (STE Module)\n    Kết hợp đặc trưng nút (Spatial) và nhúng thời gian (Temporal).\n    \"\"\"\n    def __init__(self, output_dim, n_nodes, input_steps):\n        super(STE_Layer, self).__init__()\n        self.output_dim = output_dim\n        self.n_nodes = n_nodes\n        self.input_steps = input_steps\n        # Temporal Embedding: Mã hóa thời gian (Time of day / Day of week)\n        # Giả sử đầu vào time encoding nằm ở các feature cuối của input\n        self.time_embedding = layers.Dense(output_dim, activation='relu') \n        self.feature_embedding = layers.Dense(output_dim, activation='relu')\n\n    def call(self, inputs):\n        # inputs shape: (Batch, Steps, Nodes, Features)\n        # Tách đặc trưng thời gian (giả định nằm ở cuối) và đặc trưng không gian\n        features = self.feature_embedding(inputs)\n        \n        # Thêm positional encoding đơn giản để mô phỏng Temporal Embedding\n        # Trong thực tế, STE sẽ phức tạp hơn với Node2Vec, ở đây ta dùng Dense layers\n        return features\n\nclass SpatialAttention(layers.Layer):\n    \"\"\"\n    4.1 Spatial Attention: Tính toán tương quan giữa các nút vi và vj\n    s_vi,v = <h_vi|e_vi, h_v|e_v> / sqrt(2D)\n    \"\"\"\n    def __init__(self, n_nodes):\n        super(SpatialAttention, self).__init__()\n        self.W1 = layers.Dense(1)\n        self.W2 = layers.Dense(1)\n        self.W3 = layers.Dense(1)\n        self.V = layers.Dense(n_nodes) \n\n    def call(self, inputs):\n        # inputs: (Batch, Steps, Nodes, Features)\n        # Quan tâm đến chiều Nodes\n        # Shape: (Batch, Nodes, Features) (Lấy trung bình theo steps hoặc xét step cuối)\n        x = tf.reduce_mean(inputs, axis=1) \n        \n        # Cơ chế Attention cơ bản\n        # Logic: S = V * sigmoid(W1*x + W2*x + b)\n        lhs = self.W1(x) # (B, N, 1)\n        rhs = self.W2(x) # (B, N, 1)\n        \n        # Broadcast để tạo ma trận N x N\n        logits = lhs + tf.transpose(rhs, [0, 2, 1]) # (B, N, N)\n        attention = tf.nn.softmax(tf.nn.sigmoid(logits), axis=-1)\n        return attention # Matrix S\n\nclass TemporalAttention(layers.Layer):\n    \"\"\"\n    4.2 Temporal Attention: Bắt giữ phụ thuộc giữa các bước thời gian\n    M_tx,t\n    \"\"\"\n    def __init__(self, n_steps):\n        super(TemporalAttention, self).__init__()\n        self.W1 = layers.Dense(1)\n        self.W2 = layers.Dense(1)\n        self.W3 = layers.Dense(1)\n        self.V = layers.Dense(n_steps)\n\n    def call(self, inputs):\n        # inputs: (Batch, Steps, Nodes, Features)\n        # Quan tâm đến chiều Steps, gộp Nodes\n        x = tf.transpose(inputs, (0, 2, 1, 3)) # (B, N, T, F)\n        x = tf.reduce_mean(x, axis=1) # (B, T, F)\n        \n        lhs = self.W1(x) \n        rhs = self.W2(x)\n        \n        logits = lhs + tf.transpose(rhs, [0, 2, 1]) # (B, T, T)\n        attention = tf.nn.softmax(tf.nn.sigmoid(logits), axis=-1)\n        return attention # Matrix M\n\nclass GatedFusion(layers.Layer):\n    \"\"\"\n    4.3 Gated Fusion Module: Hợp nhất cổng\n    z = sigmoid(W*XS + U*XT)\n    output = z * XS + (1-z) * XT\n    \"\"\"\n    def __init__(self, output_dim):\n        super(GatedFusion, self).__init__()\n        self.W = layers.Dense(output_dim)\n        self.U = layers.Dense(output_dim)\n\n    def call(self, spatial_feat, temporal_feat):\n        z = tf.nn.sigmoid(self.W(spatial_feat) + self.U(temporal_feat))\n        return z * spatial_feat + (1 - z) * temporal_feat\n\nclass AI_GFACN(Model):\n    def __init__(self, n_nodes, input_steps, output_steps, feature_dim, hidden_dim=64):\n        super(AI_GFACN, self).__init__()\n        self.n_nodes = n_nodes\n        self.input_steps = input_steps\n        self.output_steps = output_steps\n        \n        # 3. STE Module\n        self.ste = STE_Layer(hidden_dim, n_nodes, input_steps)\n        \n        # 4. Core Architecture\n        # Attention\n        self.spatial_attn = SpatialAttention(n_nodes)\n        self.temporal_attn = TemporalAttention(input_steps)\n        \n        # GCN Encoder Layers (Giả lập GCN đơn giản với Dense + Adjacency)\n        self.gcn_weight = layers.Dense(hidden_dim, activation='relu')\n        \n        # Gated Fusion\n        self.fusion = GatedFusion(hidden_dim)\n        \n        # 5. Output FCs\n        self.flatten = layers.Flatten()\n        # Output shape: (Batch, Output_Steps, Nodes) -> ta reshape lại sau\n        self.fc_out = layers.Dense(output_steps * n_nodes, activation=None) # Linear activation cho hồi quy\n\n    def call(self, inputs, training=False):\n        # inputs list: [A1, A2, Features]\n        # A1: (Batch, Nodes, Nodes) -> (32, 25, 25)\n        # A2: (Batch, Time, Nodes, Nodes) -> (32, 12, 25, 25)\n        # x:  (Batch, Time, Nodes, Feats) -> (32, 12, 25, 133)\n        A1, A2, x = inputs[0], inputs[1], inputs[2]\n        \n        # --- B1: STE Embedding ---\n        x_embed = self.ste(x) # (32, 12, 25, 64)\n        \n        # --- B2: Attention Mechanism ---\n        # 1. Spatial Attention\n        S = self.spatial_attn(x_embed) # (32, 25, 25)\n        S_expanded = tf.expand_dims(S, axis=1) # (32, 1, 25, 25)\n        x_spatial = tf.matmul(S_expanded, x_embed, transpose_a=True) # (32, 12, 25, 64)\n        \n        # 2. Temporal Attention\n        M = self.temporal_attn(x_embed) # (32, 12, 12)\n        x_temp_trans = tf.transpose(x_embed, (0, 2, 1, 3)) # (32, 25, 12, 64)\n        M_expanded = tf.expand_dims(M, axis=1) # (32, 1, 12, 12)\n        x_temporal = tf.matmul(M_expanded, x_temp_trans, transpose_a=True) # (32, 25, 12, 64)\n        x_temporal = tf.transpose(x_temporal, (0, 2, 1, 3)) # (32, 12, 25, 64)\n        \n        # --- B3: Gated Fusion ---\n        x_fused = self.fusion(x_spatial, x_temporal)\n        \n        # --- B4: Graph Convolution (GCN) ---\n        \n        # SỬA LỖI TẠI ĐÂY: Hợp nhất A1 và A2\n        # A1 đang là (Batch, Nodes, Nodes), cần thêm dimension Time để cộng với A2\n        A1_expanded = tf.expand_dims(A1, axis=1) # (32, 1, 25, 25)\n        \n        # Bây giờ broadcast sẽ hoạt động: (32, 1, 25, 25) + (32, 12, 25, 25) \n        A_combined = A1_expanded + A2  # Kết quả: (32, 12, 25, 25)\n        \n        # Chuẩn bị cho GCN (Flatten Batch và Time)\n        # Input features: (Batch * Time, Nodes, Hidden)\n        batch_size = tf.shape(x_fused)[0]\n        x_reshaped = tf.reshape(x_fused, (-1, self.n_nodes, x_fused.shape[-1])) # (384, 25, 64)\n        \n        # Input Adjacency: (Batch * Time, Nodes, Nodes)\n        A_reshaped = tf.reshape(A_combined, (-1, self.n_nodes, self.n_nodes)) # (384, 25, 25)\n        \n        # Tính toán GCN: ReLU(A * X * W)\n        # Bước 1: X * W\n        x_gcn = self.gcn_weight(x_reshaped) # (384, 25, 64)\n        \n        # Bước 2: A * (XW)\n        out_gcn = tf.matmul(A_reshaped, x_gcn) # (384, 25, 64)\n        out_gcn = tf.nn.relu(out_gcn)\n        \n        # --- B5: Output Prediction ---\n        # Flatten về (Batch, ...) để đưa vào lớp Dense cuối\n        out_flat = layers.Flatten()(tf.reshape(out_gcn, (batch_size, -1)))\n        \n        prediction = self.fc_out(out_flat) \n        prediction = tf.reshape(prediction, (batch_size, self.output_steps, self.n_nodes))\n        \n        return prediction","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T18:56:42.351652Z","iopub.execute_input":"2025-12-19T18:56:42.352257Z","iopub.status.idle":"2025-12-19T18:56:42.369785Z","shell.execute_reply.started":"2025-12-19T18:56:42.352226Z","shell.execute_reply":"2025-12-19T18:56:42.369112Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\n\n# --- Config ---\ncity = 'Seoul'\nyear = '2016'\nn_steps_in = 12   # Input history (H=12 in description)\nn_steps_out = 3   # Output prediction (J=3 in description)\nbatch_size = 32\nepochs = 50\nlearning_rate = 0.001\n\n# --- 1. Load Data (MG_TAR Pipeline) ---\nprint(\"Loading MG_TAR Data...\")\n# Lưu ý: n_steps trong data_loader của bạn là output steps, length là input steps\ndatasets = data_loader('/kaggle/input/mg-tar', city, year, length=n_steps_in, n_steps=n_steps_out, is_scale=True, temporal_copy=True)\n\n# Lấy các biến cần thiết\nrisk_train, risk_test = datasets['risk'][0], datasets['risk'][1]\n# ... (Load các feature khác tương tự như code mẫu của bạn: road, poi, speed, volume...)\n# Để gọn, ta giả định 'node_features' đã được nối như trong code mẫu\n# Tái tạo lại node_features (Feature Engineering)\ndef create_node_features(d, split_idx=0): # 0 for train, 1 for test\n    return np.concatenate([\n        d['risk'][split_idx], d['demo'][split_idx], d['poi'][split_idx], \n        d['road'][split_idx], d['volume'][split_idx], d['speed'][split_idx], \n        d['weather'][split_idx], d['calendar'][split_idx], d['dtg'][split_idx]\n    ], axis=-1)\n\nnode_feat_train = create_node_features(datasets, 0)\nnode_feat_test = create_node_features(datasets, 1)\ny_train, y_test = datasets['y'][0], datasets['y'][1] # Target\n\n# Load Ma trận kề (Giả lập A1 và A2 từ dữ liệu có sẵn)\n# AI-GFACN cần 2 ma trận: A1 (Distance), A2 (Accident Risk)\n# Ta dùng A_road làm A1, A_traffic (hoặc A_risk nếu có) làm A2\nimport pandas as pd\nA_road = pd.read_csv(f'/kaggle/input/mg-tar/graph_data/{city}-{year}-road-normalized-district-jaccard.csv', engine='c', index_col=0).to_numpy()\nwith open(f'/kaggle/input/mg-tar/graph_data/{city}-{year}-traffic-district-normalized-train-jaccard.npy', 'rb') as f:\n    A_traffic_full = np.load(f)\n\n# Chuẩn bị dữ liệu cho Model (Tile ma trận kề theo batch size để khớp input model)\n# Lưu ý: Trong thực tế nên dùng tf.data.Dataset để tránh memory leak khi tile quá lớn\n# Ở đây ta demo logic đơn giản\nnum_train = node_feat_train.shape[0]\nnum_test = node_feat_test.shape[0]\nn_nodes = node_feat_train.shape[2]\nn_features = node_feat_train.shape[3]\n\n# A1: Static (Road) -> Tile\nA1_train = np.tile(A_road, (num_train, 1, 1)).astype('float32')\nA1_test = np.tile(A_road, (num_test, 1, 1)).astype('float32')\n\n# A2: Dynamic (Traffic/Accident) -> Cắt từ dữ liệu traffic đã load\n# Giả sử A_traffic_full tương ứng với train set\nA2_train = A_traffic_full[:num_train].astype('float32')\n# Với test, ta lấy phần cuối (lưu ý check size kỹ trong thực tế)\nA2_test = np.tile(A_road, (num_test, 1, 1)).astype('float32') # Placeholder nếu thiếu dữ liệu test động\n\n# --- 2. Initialize Model ---\nprint(\"Building AI-GFACN Model...\")\nmodel = AI_GFACN(n_nodes=n_nodes, \n                 input_steps=n_steps_in, \n                 output_steps=n_steps_out, \n                 feature_dim=n_features,\n                 hidden_dim=64)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\nloss_fn = tf.keras.losses.MeanSquaredError()\n\n# --- 3. Custom Training Loop (để kiểm soát input list) ---\n# Chuyển đổi sang tf.data\ntrain_dataset = tf.data.Dataset.from_tensor_slices(((A1_train, A2_train, node_feat_train), y_train))\ntrain_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n\ntest_dataset = tf.data.Dataset.from_tensor_slices(((A1_test, A2_test, node_feat_test), y_test))\ntest_dataset = test_dataset.batch(batch_size)\n\nprint(\"Start Training...\")\nfor epoch in range(epochs):\n    # Training\n    epoch_loss_avg = tf.keras.metrics.Mean()\n    for inputs, targets in train_dataset: # inputs là tuple (A1, A2, Feats)\n        with tf.GradientTape() as tape:\n            predictions = model(inputs, training=True)\n            loss = loss_fn(targets, predictions)\n        gradients = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n        epoch_loss_avg.update_state(loss)\n    \n    print(f\"Epoch {epoch+1}/{epochs} - Loss (MSE): {epoch_loss_avg.result():.4f}\")\n\n# --- 4. Evaluation (MAE, RMSE, PCC) ---\nprint(\"\\nEvaluating...\")\nall_preds = []\nall_targets = []\n\nfor inputs, targets in test_dataset:\n    preds = model(inputs, training=False)\n    all_preds.append(preds.numpy())\n    all_targets.append(targets.numpy())\n\nall_preds = np.concatenate(all_preds, axis=0)\nall_targets = np.concatenate(all_targets, axis=0)\n\n# Tính toán Metrics\nmae_score, rmse_score, pcc_score = metric(all_preds, all_targets)\n\nprint(\"=\"*30)\nprint(f\"AI-GFACN Model Results on {city} {year}\")\nprint(f\"MAE : {mae_score:.4f}\")\nprint(f\"RMSE: {rmse_score:.4f}\")\nprint(f\"PCC : {pcc_score:.4f}\")\nprint(\"=\"*30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T18:56:42.371454Z","iopub.execute_input":"2025-12-19T18:56:42.371765Z","iopub.status.idle":"2025-12-19T18:58:47.882023Z","shell.execute_reply.started":"2025-12-19T18:56:42.371742Z","shell.execute_reply":"2025-12-19T18:58:47.880907Z"}},"outputs":[{"name":"stdout","text":"Loading MG_TAR Data...\nBuilding AI-GFACN Model...\nStart Training...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'ste__layer_3', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'spatial_attention_3', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'temporal_attention_3', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'ai_gfacn_3', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50 - Loss (MSE): 3.2616\nEpoch 2/50 - Loss (MSE): 2.3831\nEpoch 3/50 - Loss (MSE): 2.3581\nEpoch 4/50 - Loss (MSE): 2.3508\nEpoch 5/50 - Loss (MSE): 2.3111\nEpoch 6/50 - Loss (MSE): 2.2909\nEpoch 7/50 - Loss (MSE): 2.2884\nEpoch 8/50 - Loss (MSE): 2.2481\nEpoch 9/50 - Loss (MSE): 2.2546\nEpoch 10/50 - Loss (MSE): 2.2175\nEpoch 11/50 - Loss (MSE): 2.2027\nEpoch 12/50 - Loss (MSE): 2.1927\nEpoch 13/50 - Loss (MSE): 2.1635\nEpoch 14/50 - Loss (MSE): 2.1461\nEpoch 15/50 - Loss (MSE): 2.1439\nEpoch 16/50 - Loss (MSE): 2.1172\nEpoch 17/50 - Loss (MSE): 2.0935\nEpoch 18/50 - Loss (MSE): 2.0866\nEpoch 19/50 - Loss (MSE): 2.0585\nEpoch 20/50 - Loss (MSE): 2.0356\nEpoch 21/50 - Loss (MSE): 2.0072\nEpoch 22/50 - Loss (MSE): 1.9891\nEpoch 23/50 - Loss (MSE): 1.9717\nEpoch 24/50 - Loss (MSE): 1.9463\nEpoch 25/50 - Loss (MSE): 1.9348\nEpoch 26/50 - Loss (MSE): 1.9162\nEpoch 27/50 - Loss (MSE): 1.8915\nEpoch 28/50 - Loss (MSE): 1.8804\nEpoch 29/50 - Loss (MSE): 1.8648\nEpoch 30/50 - Loss (MSE): 1.8312\nEpoch 31/50 - Loss (MSE): 1.8146\nEpoch 32/50 - Loss (MSE): 1.7916\nEpoch 33/50 - Loss (MSE): 1.7729\nEpoch 34/50 - Loss (MSE): 1.7543\nEpoch 35/50 - Loss (MSE): 1.7330\nEpoch 36/50 - Loss (MSE): 1.6990\nEpoch 37/50 - Loss (MSE): 1.6888\nEpoch 38/50 - Loss (MSE): 1.6719\nEpoch 39/50 - Loss (MSE): 1.6599\nEpoch 40/50 - Loss (MSE): 1.6217\nEpoch 41/50 - Loss (MSE): 1.5880\nEpoch 42/50 - Loss (MSE): 1.5847\nEpoch 43/50 - Loss (MSE): 1.5496\nEpoch 44/50 - Loss (MSE): 1.5430\nEpoch 45/50 - Loss (MSE): 1.5059\nEpoch 46/50 - Loss (MSE): 1.4922\nEpoch 47/50 - Loss (MSE): 1.4733\nEpoch 48/50 - Loss (MSE): 1.4475\nEpoch 49/50 - Loss (MSE): 1.4242\nEpoch 50/50 - Loss (MSE): 1.3843\n\nEvaluating...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/3073904853.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0mall_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mall_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/898703802.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# Bước 2: A * (XW)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mout_gcn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_reshaped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_gcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (384, 25, 64)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mout_gcn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_gcn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling AI_GFACN.call().\n\n\u001b[1m{{function_node __wrapped__BatchMatMulV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} In[0] and In[1] must have compatible batch dimensions: [1024,25,25] vs. [384,25,64] [Op:BatchMatMulV2] name: \u001b[0m\n\nArguments received by AI_GFACN.call():\n  • inputs=('tf.Tensor(shape=(32, 25, 25), dtype=float32)', 'tf.Tensor(shape=(32, 25, 25), dtype=float32)', 'tf.Tensor(shape=(32, 12, 25, 133), dtype=float32)')\n  • training=False"],"ename":"InvalidArgumentError","evalue":"Exception encountered when calling AI_GFACN.call().\n\n\u001b[1m{{function_node __wrapped__BatchMatMulV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} In[0] and In[1] must have compatible batch dimensions: [1024,25,25] vs. [384,25,64] [Op:BatchMatMulV2] name: \u001b[0m\n\nArguments received by AI_GFACN.call():\n  • inputs=('tf.Tensor(shape=(32, 25, 25), dtype=float32)', 'tf.Tensor(shape=(32, 25, 25), dtype=float32)', 'tf.Tensor(shape=(32, 12, 25, 133), dtype=float32)')\n  • training=False","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"# --- SỬA LỖI DATA PREPARATION CHO A2_TEST ---\n\n# 1. Định nghĩa lại số bước thời gian input (quan trọng)\nn_steps_in = 12 \n\n# 2. Sửa lại cách tạo A2_test (Ma trận động cho tập test)\n# Lỗi cũ: A2_test = np.tile(A_road, (num_test, 1, 1)) -> Thiếu chiều Time\n# Sửa mới: Tile thêm dimension n_steps_in\nprint(\"Fixing A2_test shape...\")\n\nif 'A_traffic_test' in locals() and len(A_traffic_test) == num_test:\n    # Nếu bạn có dữ liệu traffic thật cho test\n    A2_test = A_traffic_test.astype('float32')\nelse:\n    # Nếu không có (fallback), lặp lại A_road 12 lần cho mỗi mẫu\n    # Shape mong muốn: (num_test, 12, 25, 25)\n    # A_road shape gốc: (25, 25)\n    \n    # Bước 1: Expand A_road thành (1, 1, 25, 25)\n    A_road_expanded = np.expand_dims(np.expand_dims(A_road, axis=0), axis=0)\n    \n    # Bước 2: Tile thành (num_test, 12, 25, 25)\n    A2_test = np.tile(A_road_expanded, (num_test, n_steps_in, 1, 1)).astype('float32')\n\nprint(f\"Corrected A2_test shape: {A2_test.shape}\") # Nên là (..., 12, 25, 25)\n\n# --- TẠO LẠI DATASET VỚI A2_TEST MỚI ---\ntest_dataset = tf.data.Dataset.from_tensor_slices(((A1_test, A2_test, node_feat_test), y_test))\ntest_dataset = test_dataset.batch(batch_size)\n\n# --- CHẠY LẠI ĐÁNH GIÁ ---\nprint(\"\\nRe-evaluating...\")\nall_preds = []\nall_targets = []\n\nfor inputs, targets in test_dataset:\n    preds = model(inputs, training=False)\n    all_preds.append(preds.numpy())\n    all_targets.append(targets.numpy())\n\nall_preds = np.concatenate(all_preds, axis=0)\nall_targets = np.concatenate(all_targets, axis=0)\n\n# Tính toán Metrics\nmae_score, rmse_score, pcc_score = metric(all_preds, all_targets)\n\nprint(\"=\"*30)\nprint(f\"AI-GFACN Model Results on {city} {year}\")\nprint(f\"MAE : {mae_score:.4f}\")\nprint(f\"RMSE: {rmse_score:.4f}\")\nprint(f\"PCC : {pcc_score:.4f}\")\nprint(\"=\"*30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T19:00:10.779634Z","iopub.execute_input":"2025-12-19T19:00:10.780328Z","iopub.status.idle":"2025-12-19T19:00:11.071694Z","shell.execute_reply.started":"2025-12-19T19:00:10.780294Z","shell.execute_reply":"2025-12-19T19:00:11.070934Z"}},"outputs":[{"name":"stdout","text":"Fixing A2_test shape...\nCorrected A2_test shape: (141, 12, 25, 25)\n\nRe-evaluating...\n==============================\nAI-GFACN Model Results on Seoul 2016\nMAE : 1.1496\nRMSE: 1.8325\nPCC : 0.0554\n==============================\n","output_type":"stream"}],"execution_count":17}]}