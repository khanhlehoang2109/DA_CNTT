{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31240,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch_geometric","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-20T02:03:41.623441Z","iopub.execute_input":"2025-12-20T02:03:41.624382Z","iopub.status.idle":"2025-12-20T02:03:46.054586Z","shell.execute_reply.started":"2025-12-20T02:03:41.624328Z","shell.execute_reply":"2025-12-20T02:03:46.053754Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch_geometric in /usr/local/lib/python3.11/dist-packages (2.7.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.13.2)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.10.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (7.1.3)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.0.9)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.5)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.6.0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.22.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.10.5)\nRequirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.15.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch_geometric) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch_geometric) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch_geometric) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torch_geometric) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torch_geometric) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torch_geometric) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport math\nimport time\nimport shutil\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nimport scipy.sparse as sp\nfrom scipy.sparse.linalg import eigsh\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.nn import Parameter\nfrom torch_geometric.data import Data, InMemoryDataset, download_url\nfrom torch_geometric.utils import to_scipy_sparse_matrix\nfrom sklearn.metrics import accuracy_score, f1_score, average_precision_score, roc_auc_score, mean_absolute_error, mean_squared_error\nfrom scipy.stats import pearsonr\nfrom sklearn.preprocessing import MinMaxScaler\n\n# --- Helper Functions for Spectral GCN (Required by your model code) ---\nimport scipy.sparse as sp\nfrom scipy.sparse.linalg import eigsh\nimport torch\n\ndef scaled_Laplacian(W):\n    '''\n    Compute \\tilde{L} using Scipy Sparse Matrices to save RAM\n    '''\n    # W is expected to be a scipy.sparse matrix (CSR/COO)\n    assert W.shape[0] == W.shape[1]\n    \n    # Tính Degree Matrix (D) dạng thưa\n    D_values = np.array(W.sum(1)).flatten()\n    D = sp.diags(D_values)\n    \n    L = D - W\n    \n    # Tính giá trị riêng lớn nhất (lambda_max) trên ma trận thưa\n    # k=1: chỉ lấy 1 giá trị riêng lớn nhất\n    lambda_max = eigsh(L, k=1, which='LM', return_eigenvectors=False)[0]\n    \n    # Công thức: (2 * L / lambda_max) - I\n    L_tilde = (2 * L / lambda_max) - sp.eye(W.shape[0])\n    return L_tilde\n\ndef cheb_polynomial(L_tilde, K):\n    '''\n    Compute Chebyshev polynomials but keep them as Sparse Tensors\n    '''\n    N = L_tilde.shape[0]\n    \n    # Chuyển đổi Scipy Sparse -> PyTorch Sparse Tensor\n    def scipy_to_torch_sparse(probs):\n        coo = probs.tocoo()\n        values = coo.data\n        indices = np.vstack((coo.row, coo.col))\n        \n        i = torch.LongTensor(indices)\n        v = torch.FloatTensor(values)\n        shape = coo.shape\n        \n        # Trả về sparse tensor\n        return torch.sparse_coo_tensor(i, v, torch.Size(shape))\n\n    L_tilde_torch = scipy_to_torch_sparse(L_tilde)\n    \n    # T0 = I, T1 = L_tilde\n    indices_eye = torch.arange(N)\n    indices_eye = torch.stack((indices_eye, indices_eye))\n    values_eye = torch.ones(N)\n    T0 = torch.sparse_coo_tensor(indices_eye, values_eye, torch.Size((N, N)))\n    \n    cheb_polynomials = [T0, L_tilde_torch]\n    \n    # Đệ quy: Tk = 2 * L * Tk-1 - Tk-2\n    # Lưu ý: Nhân 2 ma trận sparse trong PyTorch đôi khi tốn kém, \n    # nhưng ở đây ta tính trước (pre-compute) nên chấp nhận được.\n    for i in range(2, K):\n        # Sparse MM: L_tilde (NxN) @ T_{i-1} (NxN)\n        # PyTorch hỗ trợ sparse @ sparse -> sparse (trong các version mới)\n        # Hoặc dùng torch.sparse.mm (cho sparse @ dense). \n        # Để an toàn và tiết kiệm RAM, ta giữ list dạng sparse tensor.\n        \n        # Với K nhỏ (2, 3), ta có thể xấp xỉ hoặc tính toán cẩn thận.\n        # Để đơn giản và tránh lỗi version PyTorch cũ với sparse@sparse, \n        # ta sẽ thực hiện phép nhân này ngay trong model hoặc convert sang dense CHỈ KHI cần thiết.\n        # Tuy nhiên, cách tốt nhất là giữ nguyên L_tilde và thực hiện phép nhân đệ quy trong forward pass\n        # Nhưng để tuân thủ cấu trúc code cũ, ta sẽ lưu list này.\n        \n        # *Lưu ý*: PyTorch `spmm` (sparse x sparse) chưa ổn định ở mọi version.\n        # Giải pháp tối ưu RAM cho người dùng: \n        # Chỉ lưu L_tilde và thực hiện phép tính Chebyshev \"on-the-fly\" hoặc \n        # chỉ dùng K=2 (thường đủ tốt).\n        pass \n        \n    # *FIX*: Để tránh lỗi tính toán phức tạp sparse-sparse, \n    # code này sẽ trả về list chứa các tham số để tính trong GCN layer\n    # hoặc trả về list sparse tensor đã convert.\n    # Ở đây tôi trả về list sparse tensor cho K=2 (phổ biến nhất).\n    # Nếu K>2, cần dùng thư viện `torch_sparse` chuyên dụng, nhưng để đơn giản:\n    return cheb_polynomials","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T02:03:46.056460Z","iopub.execute_input":"2025-12-20T02:03:46.056728Z","iopub.status.idle":"2025-12-20T02:03:54.806184Z","shell.execute_reply.started":"2025-12-20T02:03:46.056701Z","shell.execute_reply":"2025-12-20T02:03:54.805510Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class GCN(nn.Module):\n    def __init__(self, L_tilde, dim_in, dim_out, order_K, device, in_drop=0.0, gcn_drop=0.0, residual=False):\n        super(GCN, self).__init__()\n        self.DEVICE = device\n        self.order_K = order_K\n        \n        # L_tilde bây giờ là Sparse Tensor, không lưu dense\n        self.L_tilde = L_tilde.to(device) \n        \n        self.dim_in = dim_in\n        self.dim_out = dim_out\n        self.Theta = nn.ParameterList([nn.Parameter(torch.FloatTensor(dim_in, dim_out)) for _ in range(order_K)])\n        self.weights = nn.Parameter(torch.FloatTensor(size=(dim_out, dim_out)))\n        self.biases = nn.Parameter(torch.FloatTensor(size=(dim_out,)))\n        self._in_drop = in_drop\n        self._gcn_drop = gcn_drop\n        self._residual = residual\n        self.linear = nn.Linear(dim_in, dim_out)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        for param in self.parameters():\n            if param.dim() > 1:\n                nn.init.xavier_uniform_(param)\n\n    def forward(self, x, state=None, M=None):\n        # x shape: (Batch, Nodes, Features) -> (B, N, F_in)\n        batch_size = x.shape[0]\n        num_nodes = x.shape[1]\n        \n        output = torch.zeros(batch_size, num_nodes, self.dim_out).to(self.DEVICE)\n        \n        # Chuẩn bị dữ liệu để nhân sparse: (N, N) sparse @ (N, Batch*F) dense\n        # Reshape x: (B, N, F) -> (N, B, F) -> (N, B*F)\n        x_reshaped = x.permute(1, 0, 2).reshape(num_nodes, -1)\n        \n        x0 = x\n        if self._in_drop != 0:\n            x = torch.dropout(x, 1.0 - self._in_drop, train=True)\n            \n        # Tính Chebyshev on-the-fly hoặc dùng pre-computed\n        # Ở đây ta dùng đệ quy trực tiếp để tiết kiệm RAM lưu trữ các T_k\n        \n        # T_0(L) * x = I * x = x\n        # Term 0\n        support = x # (B, N, F)\n        # support @ Theta_0\n        out_k = torch.matmul(support, self.Theta[0])\n        output = output + out_k\n        \n        if self.order_K > 1:\n            # T_1(L) * x = L * x\n            # Sparse MM: (N, N) @ (N, B*F) -> (N, B*F)\n            Lx = torch.sparse.mm(self.L_tilde, x_reshaped)\n            # Reshape lại về (B, N, F)\n            Lx_out = Lx.reshape(num_nodes, batch_size, -1).permute(1, 0, 2)\n            \n            # Term 1\n            out_k = torch.matmul(Lx_out, self.Theta[1])\n            output = output + out_k\n            \n            # Lưu lại T_{k-1} và T_{k-2} để đệ quy nếu K > 2\n            T_prev = Lx_out\n            T_prev2 = x\n            \n            for k in range(2, self.order_K):\n                # T_k = 2 * L * T_{k-1} - T_{k-2}\n                # Tính L * T_{k-1}\n                T_prev_reshaped = T_prev.permute(1, 0, 2).reshape(num_nodes, -1)\n                L_Tprev = torch.sparse.mm(self.L_tilde, T_prev_reshaped)\n                L_Tprev = L_Tprev.reshape(num_nodes, batch_size, -1).permute(1, 0, 2)\n                \n                T_k = 2 * L_Tprev - T_prev2\n                \n                # Term k\n                out_k = torch.matmul(T_k, self.Theta[k])\n                output = output + out_k\n                \n                # Update đệ quy\n                T_prev2 = T_prev\n                T_prev = T_k\n            \n        output = torch.matmul(output, self.weights)\n        output = output + self.biases\n        res = F.relu(output)\n        \n        if self._gcn_drop != 0.0:\n            res = torch.dropout(res, 1.0 - self._gcn_drop, train=True)\n        if self._residual:\n            x0 = self.linear(x0)\n            res = res + x0\n        return res\n\nclass MGCN_Standard(nn.Module):\n    def __init__(self, L_tilde, dim_in, dim_out, range_K, device, in_drop=0.0, gcn_drop=0.0, residual=False):\n        super(MGCN_Standard, self).__init__()\n        self.DEVICE = device\n        self.K = range_K\n        self.GCN_khops_node = nn.ModuleList([GCN(L_tilde, dim_in, dim_out, k + 1, device, in_drop=in_drop, gcn_drop=gcn_drop, residual=residual) for k in range(self.K)])\n        self.linear = nn.Linear(dim_out, dim_in)\n        self.W = nn.Parameter(torch.FloatTensor(dim_in, dim_out))\n        self.b = nn.Parameter(torch.FloatTensor(dim_out, ))\n        nn.init.xavier_uniform_(self.W)\n\n    def forward(self, X):\n        Xs = []\n        for k in range(self.K):\n            X_out = self.GCN_khops_node[k](X)\n            # Attention mechanism part 1\n            # X_out: (B, N, F_out)\n            # linear maps back to dim_in? The code logic seems to try to combine inputs.\n            # Let's follow the code strictly.\n            # Note: In the provided code, self.linear maps dim_out -> dim_in.\n            X_linear = self.linear(X_out) \n            X1 = torch.sigmoid(X_linear.matmul(self.W) + self.b)\n            Xs.append(X1)\n        Xs = torch.stack(Xs)\n        return Xs\n\nclass MRA_GCN_Standard(nn.Module):\n    def __init__(self, L_tilde, dim_in, dim_out, range_K, device, in_drop=0.0, gcn_drop=0.0, residual=False):\n        super(MRA_GCN_Standard, self).__init__()\n        self.DEVICE = device\n        self.dim_out = dim_out\n        self.W_a = nn.Parameter(torch.FloatTensor(self.dim_out, self.dim_out))\n        self.U = nn.Parameter(torch.FloatTensor(self.dim_out))\n        self.MGCN = MGCN_Standard(L_tilde, dim_in, dim_out, range_K, device, in_drop=in_drop, gcn_drop=gcn_drop, residual=residual)\n        \n        # Output layer for classification\n        self.fc_final = nn.Linear(dim_out, 2) # Assuming binary classification for TRAVEL\n\n        nn.init.xavier_uniform_(self.W_a)\n        nn.init.uniform_(self.U)\n\n    def forward(self, X):\n        # X shape: (Batch, N, F) or (N, F)\n        if X.dim() == 2:\n            X = X.unsqueeze(0) # Add batch dimension if missing\n            \n        input = self.MGCN(X) # Shape: (K, B, N, F_out) - wait, MGCN returns stack of Xs\n        \n        # Attention pooling\n        # input shape expected: (K, B, N, dim_out) based on MGCN return\n        # Code: torch.einsum('ijkl,lm->ijkm', input, self.W_a)\n        # Assuming input is (K, B, N, F)\n        \n        tmp = torch.einsum('kbnf,fm->kbnm', input, self.W_a)\n        e = torch.einsum('kbnm,m->kbn', tmp, self.U)\n        \n        alpha = F.softmax(e, dim=0).unsqueeze(-1) # Softmax over K\n        \n        # Weighted sum\n        h = torch.einsum('kbnf,kbnl->bnf', input, alpha) # (B, N, F)\n        \n        return h","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T02:03:54.806996Z","iopub.execute_input":"2025-12-20T02:03:54.807495Z","iopub.status.idle":"2025-12-20T02:03:54.826599Z","shell.execute_reply.started":"2025-12-20T02:03:54.807473Z","shell.execute_reply":"2025-12-20T02:03:54.825890Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# --- Dataset Loading from Task 1 ---\ndef read_npz(path):\n    with np.load(path, allow_pickle=True) as f:\n        return parse_npz(f)\n\ndef parse_npz(f):\n    crash_time = f['crash_time']\n    x = torch.from_numpy(f['x']).to(torch.float)\n    coords = torch.from_numpy(f['coordinates']).to(torch.float)\n    edge_attr = torch.from_numpy(f['edge_attr']).to(torch.float)\n    cnt_labels = torch.from_numpy(f['cnt_labels']).to(torch.long)\n    occur_labels = torch.from_numpy(f['occur_labels']).to(torch.long)\n    # Some files might have different keys, handling basic ones\n    edge_index = torch.from_numpy(f['edge_index']).to(torch.long).t().contiguous()\n    return Data(x=x, y=occur_labels, edge_index=edge_index,\n                edge_attr=edge_attr, coords=coords, cnt_labels=cnt_labels)\n\nclass TRAVELDataset(InMemoryDataset):\n    url = 'https://github.com/baixianghuang/travel/raw/main/TAP-city/{}.npz'\n    def __init__(self, root: str, name: str, transform=None, pre_transform=None):\n        self.name = name.lower()\n        super().__init__(root, transform, pre_transform)\n        self.data, self.slices = torch.load(self.processed_paths[0], weights_only=False)\n\n    @property\n    def raw_dir(self) -> str: return os.path.join(self.root, self.name, 'raw')\n    @property\n    def processed_dir(self) -> str: return os.path.join(self.root, self.name, 'processed')\n    @property\n    def raw_file_names(self) -> str: return f'{self.name}.npz'\n    @property\n    def processed_file_names(self) -> str: return 'data.pt'\n\n    def download(self):\n        download_url(self.url.format(self.name), self.raw_dir)\n\n    def process(self):\n        data = read_npz(self.raw_paths[0])\n        data = data if self.pre_transform is None else self.pre_transform(data)\n        data, slices = self.collate([data])\n        torch.save((data, slices), self.processed_paths[0])\n\ndef train_test_split_stratify(dataset, train_ratio, val_ratio, class_num):\n    labels = dataset[0].y\n    train_mask = torch.zeros(size=labels.shape, dtype=bool)\n    val_mask = torch.zeros(size=labels.shape, dtype=bool)\n    test_mask = torch.zeros(size=labels.shape, dtype=bool)\n    for i in range(class_num):\n        stratify_idx = np.argwhere(labels.numpy() == i).flatten()\n        np.random.shuffle(stratify_idx)\n        split1 = int(len(stratify_idx) * train_ratio)\n        split2 = split1 + int(len(stratify_idx) * val_ratio)\n        train_mask[stratify_idx[:split1]] = True\n        val_mask[stratify_idx[split1:split2]] = True\n        test_mask[stratify_idx[split2:]] = True\n    return train_mask, val_mask, test_mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T02:03:54.827347Z","iopub.execute_input":"2025-12-20T02:03:54.827630Z","iopub.status.idle":"2025-12-20T02:03:54.844014Z","shell.execute_reply.started":"2025-12-20T02:03:54.827606Z","shell.execute_reply":"2025-12-20T02:03:54.843416Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class MADGCN_Wrapper(nn.Module):\n    def __init__(self, dataset, device, hidden_dim=64, K=2):\n        super(MADGCN_Wrapper, self).__init__()\n        self.device = device\n        data = dataset[0]\n        \n        # --- FIX: Xử lý Sparse hoàn toàn ---\n        print(\"Converting to Sparse Adjacency...\")\n        # 1. Lấy Edge Index\n        edge_index = data.edge_index\n        num_nodes = data.num_nodes\n        \n        # 2. Tạo Scipy Sparse Matrix (COO) trực tiếp, KHÔNG dùng toarray()\n        # Tạo vector trọng số là 1 cho các cạnh\n        values = np.ones(edge_index.shape[1])\n        # Chuyển edge_index sang numpy\n        indices = edge_index.cpu().numpy()\n        \n        adj_sparse = sp.coo_matrix((values, (indices[0], indices[1])), \n                                   shape=(num_nodes, num_nodes))\n        \n        # 3. Tính Laplacian dạng Sparse\n        print(\"Computing Scaled Laplacian (Sparse)...\")\n        L_tilde_sparse = scaled_Laplacian(adj_sparse)\n        \n        # 4. Chuyển sang PyTorch Sparse Tensor\n        def scipy_to_torch_sparse(probs):\n            coo = probs.tocoo()\n            values = coo.data\n            indices = np.vstack((coo.row, coo.col))\n            i = torch.LongTensor(indices)\n            v = torch.FloatTensor(values)\n            return torch.sparse_coo_tensor(i, v, torch.Size(coo.shape))\n\n        self.L_tilde = scipy_to_torch_sparse(L_tilde_sparse).to(device)\n        print(\"Laplacian computed and loaded to device.\")\n        \n        # 5. Khởi tạo Model\n        # Lưu ý: Ta truyền trực tiếp L_tilde sparse vào, và GCN class mới sẽ xử lý nó\n        self.mra_gcn = MRA_GCN_Standard(\n            L_tilde=self.L_tilde,\n            dim_in=dataset.num_features,\n            dim_out=hidden_dim,\n            range_K=K,\n            device=device,\n            in_drop=0.5,\n            gcn_drop=0.5\n        )\n        \n        self.fc = nn.Linear(hidden_dim, dataset.num_classes)\n\n    def forward(self, x):\n        h = self.mra_gcn(x)\n        if h.dim() == 3:\n            h = h.squeeze(0)\n        out = self.fc(h)\n        return F.log_softmax(out, dim=1)\n        \n# --- Training & Evaluation Function ---\ndef train_eval_pipeline(city_name='Miami', state_name='Florida'):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Paths\n    file_path = 'exp/'\n    city_formatted = city_name.lower().replace(\" \", \"_\") + '_' + state_name\n    \n    # Prepare Data\n    if os.path.exists(file_path+city_formatted+'/processed'):\n        shutil.rmtree(file_path+city_formatted+'/processed')\n    \n    print(f\"Loading dataset for {city_name}...\")\n    dataset = TRAVELDataset(file_path, city_formatted)\n    data = dataset[0]\n    \n    # Split & Normalize (Logic from Task 1)\n    class_num = dataset.num_classes\n    data.train_mask, data.val_mask, data.test_mask = train_test_split_stratify(\n        dataset, 0.6, 0.2, class_num)\n    \n    sc = MinMaxScaler()\n    data.x[data.train_mask] = torch.tensor(sc.fit_transform(data.x[data.train_mask]), dtype=torch.float)\n    data.x[data.val_mask] = torch.tensor(sc.transform(data.x[data.val_mask]), dtype=torch.float)\n    data.x[data.test_mask] = torch.tensor(sc.transform(data.x[data.test_mask]), dtype=torch.float)\n    \n    data = data.to(device)\n    \n    # Initialize Model (The MADGCN integration)\n    model = MADGCN_Wrapper(dataset, device, hidden_dim=32, K=2).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n    \n    # Training Loop\n    epochs = 200\n    best_val_f1 = 0\n    test_metrics = {}\n    \n    print(\"Start Training MADGCN (MRA_GCN variant)...\")\n    for epoch in range(epochs):\n        model.train()\n        optimizer.zero_grad()\n        out = model(data.x)\n        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n        loss.backward()\n        optimizer.step()\n        \n        # Validation\n        model.eval()\n        with torch.no_grad():\n            logits = model(data.x)\n            pred = logits.max(1)[1]\n            \n            # Val metrics\n            val_f1 = f1_score(data.y[data.val_mask].cpu(), pred[data.val_mask].cpu(), average='binary')\n            \n            if val_f1 > best_val_f1:\n                best_val_f1 = val_f1\n                \n                # Calculate Test Metrics (MAE, RMSE, PCC)\n                mask = data.test_mask\n                y_true = data.y[mask].cpu().numpy()\n                y_pred = pred[mask].cpu().numpy()\n                y_prob = torch.exp(logits[mask])[:, 1].cpu().numpy() # Probability of class 1\n                \n                acc = accuracy_score(y_true, y_pred)\n                auc = roc_auc_score(y_true, y_prob)\n                \n                # Regression metrics on binary labels (as requested)\n                mae = mean_absolute_error(y_true, y_pred)\n                rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n                pcc, _ = pearsonr(y_true, y_pred)\n                \n                test_metrics = {\n                    'F1': val_f1,\n                    'ACC': acc,\n                    'AUC': auc,\n                    'MAE': mae,\n                    'RMSE': rmse,\n                    'PCC': pcc\n                }\n        \n        if epoch % 20 == 0:\n            print(f'Epoch {epoch:03d} | Loss: {loss:.4f} | Val F1: {val_f1:.4f}')\n\n    print(\"\\nFinal Results for MADGCN:\")\n    print(f\"City: {city_name}\")\n    print(f\"Accuracy: {test_metrics['ACC']:.4f}\")\n    print(f\"MAE: {test_metrics['MAE']:.4f}\")\n    print(f\"RMSE: {test_metrics['RMSE']:.4f}\")\n    print(f\"PCC: {test_metrics['PCC']:.4f}\")\n    \n    return test_metrics\n\n# --- Execute ---\n# Chạy thử với Dallas, TX (thường có dữ liệu ổn định trong bộ TRAVEL)\n# Bạn có thể đổi thành ('Miami', 'Florida') hoặc ('Los Angeles', 'California')\nus_state_to_abbrev = {\n    \"Alabama\": \"AL\", \"Alaska\": \"AK\", \"Arizona\": \"AZ\", \"Arkansas\": \"AR\", \"California\": \"CA\", \n    \"Colorado\": \"CO\", \"Connecticut\": \"CT\", \"Florida\": \"FL\", \"Georgia\": \"GA\", \"Texas\": \"TX\",\n    \"New York\": \"NY\" \n} # (Thêm đầy đủ nếu cần từ code cũ)\n\nif __name__ == \"__main__\":\n    results = train_eval_pipeline('Los Angeles', 'ca')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T02:03:54.845725Z","iopub.execute_input":"2025-12-20T02:03:54.845967Z","iopub.status.idle":"2025-12-20T02:04:02.060643Z","shell.execute_reply.started":"2025-12-20T02:03:54.845941Z","shell.execute_reply":"2025-12-20T02:04:02.059699Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading dataset for Los Angeles...\n","output_type":"stream"},{"name":"stderr","text":"Processing...\nDone!\n","output_type":"stream"},{"name":"stdout","text":"Converting to Sparse Adjacency...\nComputing Scaled Laplacian (Sparse)...\nLaplacian computed and loaded to device.\nStart Training MADGCN (MRA_GCN variant)...\nEpoch 000 | Loss: 0.6710 | Val F1: 0.0000\nEpoch 020 | Loss: 0.3889 | Val F1: 0.0000\nEpoch 040 | Loss: 0.3738 | Val F1: 0.0000\nEpoch 060 | Loss: 0.3409 | Val F1: 0.0000\nEpoch 080 | Loss: 0.3378 | Val F1: 0.0951\nEpoch 100 | Loss: 0.3328 | Val F1: 0.2291\nEpoch 120 | Loss: 0.3298 | Val F1: 0.2602\nEpoch 140 | Loss: 0.3294 | Val F1: 0.2358\nEpoch 160 | Loss: 0.3275 | Val F1: 0.2533\nEpoch 180 | Loss: 0.3272 | Val F1: 0.2386\n\nFinal Results for MADGCN:\nCity: Los Angeles\nAccuracy: 0.8853\nMAE: 0.1147\nRMSE: 0.3387\nPCC: 0.3348\n","output_type":"stream"}],"execution_count":5}]}